


[Music] with the demand for ID professionals
with the devops background being at its highest there's never been a better time to choose this career path not only it
is one of the most amazing careers in Cloud but it is also incredibly well paid
AWS devops professionals are some of the most sought after specialists in the cloud which is reflected in their
fantastic salaries too hello everyone and welcome to this session you are currently watching an edureka AWS Star
Wars full course video but I'm certain by the end of this video you will have a thorough understanding
about AWS devops all the way from Theory to practical applications now if you love watching videos like
these then subscribe to eduraca's YouTube channel and click the Bell button to never miss out any updates
from us also if you want to learn more about AWS devops after watching this
session or wish to obtain edureka's AWS devop certification course then please
see the link in the description below now it's time to begin with our agenda where we'll have a brief overview of
what we will cover in this AWS devops full course video well we'll start with
the AWS fundamentals where we will learn what AWS is and why should we learn it and we'll also cover the very Basics
that you must and should know about AWS after which we will head over and look
on to some devops fundamentals once we understand both the concepts individually we will then see what is
AWS devops now it's time to delve deep into the concepts of AWS demo well we will start
with AWS code commit then we will move ahead and learn about AWS code pipeline
well we will also learn about AWS code star after which we will learn about AWS
Ops Works followed by Amazon elastic container service after all this we will then see how
Docker works on AWS once this is done we will then compare AWS devops with azure
devops well we hope that the session assist you in getting jobs in the industry in order to accomplish this we
will look at some of the best practices in AWS devops before heading over to AWS
interview questions with answers so stick till the end now let's begin
with our session with our first topic
[Music] so what exactly is AWS well AWS is Amazon web services and it is one of the
best cloud service providers in the market well it is a complete software suit or a cloud service provider which
is highly secure it provides with various compute storage database and N
number of other services which we would be discussing in further slides as well and when we talk about the market it is
the best and it has various reasons to be the best in the market one being its flexibility its scalability and its
pricing other reasons being its compute capacity now why is it so important the
compute capacity well if you talk about the compute capacity you need to understand one thing if you take all the
other cloud service providers in the market and you combine the compute capacity that is you leave out AWS and
you take all others into consideration the space would be somewhere equal to say x and if you compare it with AWS it
is 6X so AWS has more compute capacity which is six times more then all the
other service providers that are there in the market so that is a huge amount so these are the reasons that make AWS
one of the best in the market and let's try to find out what are the other reasons about AWS that make it so good
what are the services features and its users basically so I would be discussing some use cases now if you are talking
about a manufacturing organization now the main focus is to manufacture Goods
but most of the businesses they focus so much on various other services or practices that need to be taken care of
that they cannot focus on the manufacturing goal now this is where AWS steps in it takes care of all the IIT
infrastructure and management that means businesses are free to focus on manufacturing and they can actually go
ahead and expand a lot architecture Consulting now the main concern is prototyping and rendering AWS takes care
of both the issues it lets you have automated or speed up rendering as far as prototyping is concerned and that is
why architectural business benefit it a lot when you talk about using AWS or any cloud provider but AWS being the best in
the market again their services are the best media company now as far as a media company goes the main concern is
generating content and the place to dump it or to store it again AWS takes care of all these situations or both these
situations large Enterprises when you talk about large Enterprises their reach is worldwide so they have to reach their
customers and their employees globally or across different places so AWS gives
you that option because it has a global architecture and your reach can be very wide as far as these points are
concerned now advantages of AWS as I've mentioned I won't say advantages exactly I would say features as well flexibility
now as far as AWS is concerned it is highly flexible now there are various reasons to support it and one of the
major reasons is it's very cost effective let us try to understand these two points together rather now when you
talk about flexibility the first concern you should have is you are dealing with big organizations they have a lot of
data that needs to be managed deployed and taken care of now when you talk about a cloud provider if it is flexible
all these things are taken care of the second thing is it is highly cost effective now when I say cost effective
AWS takes care of almost every aspect if you are a beginner or a learner they
have something called as a free tier that means you have sufficient resources to use for free and that took for one
long year so you would have sufficient Hands-On without paying anything plus it has something called as pay as you go
model now when I say pay as you go model what it does is it charges you only for the services which you are using and
only for the time being you are using them again that lets you scale up nice Lee and hence you end up paying very
less since you are paying very less and since you have so many options when you are actually buying its services what
that does is that gives you a lot of flexibility scalability again the first two points are related to this point now
how is that now when I say scalability what happens is as I've mentioned it is
very affordable so you're paying on Ali basis if you're using a particular service for one hour you'll be paying it
only for one hour that is how flexible it is and what that does is that gives
you a freedom to scale up and even scale down since it is easy to scale up it is always advisable that you start with
less and then scale as for your needs plus there are quite a few services that are there which can be automatically
scheduled now what that means is you'd be using them only when there is an uptime and in downtime you can miss
those get automatically shut down so you do not have to worry about that as well so when you talk about scalability
scaling up and down is very easy as far as AWS goes security again now security has been a
topic of debate when you talk about cloud services especially but AWS puts all those questions to rest it has great
security mechanism plus it provides with various compliance programs that again help you take care of security and when
you talk about real-time Security even that is taken care of you can take care of all the suspicious activities that
are there and not you AWS takes care of all those things and you're let free to focus on your business rather so these
are the advantages which I feel that AWS adds value to and apart from that there are quite a few other points like we
have automated scheduling which I just mentioned you have various integrated apis now these apis they're available in
different programming languages and that makes it architecturally very strong to switch from one programming language to
another so these are some of the features I feel that make AWS a wonderful wonderful service provider in
the market so let's move further and try to understand other things as far as AWS is concerned it's Global architecture
when you talk about AWS have mentioned it is the best service provider in the market so what makes AWS this popular
one of the reasons is its architecture now when I talk about its architecture it is very widely spread and it covers
almost every area that needs to be covered so let's try to understand how it works exactly well if you talk about
AWS architecture now the architecture is divided into two major parts that is Regions and availability zones now when
you talk about the regions and availability zones regions are nothing but different locations across the world
where they have their various data centers put up now as far as one region goes it might have more than one Data
Center and these data centers are known as availability Zone you being a consumer or an individual you can
actually access or access these Services by sitting anywhere in the world to give
you an example if I am sitting in some part of the world say for example I'm in Japan right now I can actually have
access to the services or data centers that are there in US right now so that is how it works you can choose your
region and accordingly you can pick your availability zones and use those so you do not have to worry about anything
domains of AWS now when you talk about its domains the first domain that we are
going to discuss is compute and when you talk about compute the first thing that should come to your mind is ec2 now when
I say ec2 it is elastic Cloud compute and what it does is it lets you have a
resizable compute capacity it's more of a raw server where you can host a website and it is a clean slate now what
do I mean by this say for example you go ahead and buy a laptop it is a clean device where you can have your own OS
you can choose which OS you want and all those things accordingly your ec2 is
again a clean slate and you can do so many things with it now next you have elastic Beanstalk which lets you deploy
your various applications on AWS and the only thing you need to know about this
thing is you do not have to worry about the underlying architecture now it is very Sim simpler to your ec2 and the
only difference between the two is as far as your elastic bean stock is concerned you can think of it as
something that has predefined libraries whereas your ec2 is a clean slate now
when I say predefined libraries say for example you want to use Java as far as
easy to goes now this is just an example don't take it literally you'll have to say for example install everything from
the beginning and start fresh but as far as your elastic bean stock is concerned
it has this predefined libraries and you can just go ahead and use those because there's an underlying architecture which
is defined let me say it again I just gave you an example don't take these sentences literally so next we have
migration when you talk about migration you need to understand one thing AWS has
a global architecture and there would be a requirement for migration and what AWS
does is it lets you have physical migration as well that means you can physically move your data to the data
center which you desire now why do we need to do that say for example I am sending an email to somebody I can do
that through internet but imagine if I have to give somebody a movie so instead of sending it online I can actually go
ahead and give it to someone if that person is means reachable for me and that way it would be more better for me
my data remains secure and so many other things so same is with data migration as well and when you talk about AWS it has
something called as snowball which actually lets you move this data physically now it's a storage service
and it actually helps you in migration a lot security and compliance now when you
talk about security we have various services like I have IIM we have KMS now
when I say im it is nothing but your identification and authentication management tool we have KMS which let's
say actually go ahead and create your own public and private keys and that helps you keep your system secure then
we have storage now when I talk about storage again AWS has quite a few services to offer to you we have
something called as your S3 now S3 it works as a bucket object kind of a thing your storage place is called as a bucket
and your object which you store in are nothing but your files now these objects have to be stored in their root files
which act as the buckets basically and then we have something called as your Cloud front which is nothing but your
content delivery Network we have something called as Glacier now when you talk about Glacier you can think of it
as a place where you can store archives because it is highly affordable next we have networking now when you talk about
networking we have services like VPC Direct Connect at Route 53 which is a
DNS now when I say VPC it is a virtual Network which actually lets you move or
launch your resources that is your AWS resources basically when you talk about Direct Connect you can think of it as a
least internet connection which can be used within AWS next on this list we have something called as messaging yes
AWS assures secured messaging and there are quite a few applications to take care of that as well now we have
something called as Cloud trial we have Ops works all these things they help you in messaging or communicating with other
parties basically databases now storage and databases are similar but you have to understand one difference when you
talk about your storage that is where you store your executable files so that is the difference between the two and
when you talk about databases we have something called as your Arora with is something which is very SQL like and it
lets you perform various SQL options at a very faster rate and what Amazon
claims is it is five times faster than what is scale is so yes when you talk
about Aurora again a great service to have we also have something called as dynamodb which is a non-relational dbms
now when you talk about non-relational dbms I won't be discussing that but this helps you in dealing with various
unstructured data sources as well next on this list we have the last domain
that is the management tools now when you talk about management tools we have something called as Cloud watch which is
a monitoring tool and it lets you set alarms and all those things so this is about AWS and its Basics as in the
points which we just discussed that is what it is its uses its advantages its
domain its Global architecture so yes guys what I've done is I've gone ahead and I've switched into my AWS
account the first thing you need to understand is what AWS does is it offers you a free tier now while I was talking
about these things I just rushed through it because I knew that I was going to give you a demo on these things so and I
wanted to discuss this thing in detail now when you talk about AWS if you are a beginner this is where you start now
what AWS does is it provides you with its free tier which is accessible to you for 12 months and there are quite a few
Services which we just discussed which are available to you for free and when I say free the certain limitations on it
as in these many hours is what you can use it for and this is the amount of memory or storage you can use in total
and all those things and its capacity and everything based on that you have different instances which you can create
and all those things now what AWS does is it gives you these services for free and as long as you stay in the limits
that AWS has set you won't be charged anything and just ask me when it is for
learning purposes that is more than enough and let's quickly go ahead and take a look at these Services first and
then there are few other points which I would like to discuss as well but firstly the free tier Services now say
this is what it has to offer to you 12 months of free and always free products when you talk about ec2 which is one of
its most popular compute Services 750 hours and that is per month next you
have Amazon quick site which gives you one GB of spice capacity now I won't get into the details of these things as in
what spice capacities and all those things when you have time I would suggest that you go ahead and explore
these things as in what do these things do today we are going to focus more on the ec2 part so for now let's quickly
take a look at these one by one first Amazon RDS which is again which gives you 750 hours of your T2 micro instance
Amazon S3 which is a storage which again gives you 5gb of standard storage and
AWS Lambda 1 million free requests per month so there are some of the videos
here actually which would introduce you to these things that would help you get started with how to creating an account
and all those things and this is the other important point which I would like to mention when you do create an AWS
account the first thing you need to consider is they'll be asking you for your credit card details so how does the
login process work firstly you go there you're given your email ID and your basic details as in why do you want to
use it and all those things next what it would do is just to verify your account it would ask you for your credit card
details even the debit card details work I've actually tried those so you can go ahead and give your credit card or debit
card details and when you do that what it does is it subtracts a very small amount from your account I did this in
India and I know that I was charged two rupees which is fairly less and that was again refunded back to me in two to
three working days the only reason they cut those two rupees was just for the verification purpose that my account is
up and running and I am a legitimate user now I as long as you stay in the limits you won't be charging anything
but if you do cross those limits you'll be charged now you might be worried as in what if I do cross the limit would I
be charged yes you would be but the fact is you actually won't go beyond it and even if you do you'll be notified saying
that you are going about the limit or about the limit even when your free subscription ends you are notified
saying that do you want to enter your billing details and do you want to start billing and if you say yes only then
you'd be charged for the subsequent months and that is a very stringent process you don't have to worry about it
that is you won't be losing out on any money as long as you follow these rules so if you do not have an account my
suggestion would be you go ahead you log into AWS and create your free tier account which is a very easy and two to
three step process once you've done that these are the services that are available to you and you can go ahead and use all of these services on your
own next we have something called as your simple monthly calculator now this
is a very useful app to have what it does is whatever service which you are going to use you can enter in the detail
and accordingly you'll be given the price as well now this is something for use after your free tier ends so that
you know that if I have to use these Services how much would I be charged and this thing will tell you in total now if
you can take a look at here you have these Services say for example I want to add an ec2 service
an instance gets added here I can put in the description now it is Linux T1 dot micro instance and this is what my
monthly cost is I can go ahead and I can change the number of instances here to suit my need and accordingly everything
would get verified as in okay um this is the cost and all those things so if I enter these values my AWS calculates all
the values accordingly for my storage for my compute and they're the other details which you can go ahead and fill
in here and once you've done that you can actually come here and you can check your estimate monthly bill because once
you save those details there or once you calculate those details here would be your monthly expense because you would
be dealing with more than one instances so yeah different costs are added together and a total is given to you here and you can actually go ahead and
save it and share it with others because yes you might be required to exchange these details with some of your
colleagues or whatever it is and to do that you have an option here as well which is provided by AWS there was a
query on stack Overflow once as in can I do that and AWS quickly responded by
going ahead and solving this issue and giving them with this save and share button so yeah this is your calculator
your monthly AWS calculator which lets you take care of your usage and gives
you the bill accordingly so if you are worried about as in how much you're going to spend by using AWS or its
services you can always come here and calculate that as well now that being said let's quickly move to the demo part
and see what all we can do with the AWS now I have my account here and I've logged in actually so these are the
services that are there at my disposal rather so I'm just going to go ahead and open one of these say for example I open
my ec2 service my internet is little slow today so yep
as you can see I have one running instance I have one key pair I have three security groups as we move further
we would be dealing with these and we would be implementing or creating new instances and all those things so
firstly what I want to do is I want to go ahead and create a key pair now why do I need to do that the first thing you
need to understand is when you create a key pair what you are doing is you're actually going ahead and you're creating
your SSH key now when I say SSH key that is something but a secure Shell Key now
this key is very useful when you talk about certain OSS because that helps you establish your connection when you talk
about Windows you do not need to do that so but still you would be needing an SSH key now you might wonder why the reason
you'd be needing it is because given on the longer run even if you're using Windows you'd be required to put in an
admin login and for that you would be requiring this key anyways if you are on using this application on your Linux OS
in that case it is a must that you generate the key first one more thing you need to understand is once you generate this key you need to take care
that you do not lose it because once it's gone it's gone forever so that is one precaution you need to take so let's
first start by creating a key pair here and as we move further you'd understand why did I create one I say create
my second key because I believe I've created a key in this account prior to this second key I say create
and there you go I have a key now my second key is something which I basically have it here I'll just cut it
from here and I'll paste it in one of my folders basically so let's say
I'm here and I say I need a new folder I come here I say AWS demo and I put my
key in here I would be needing this that is why I'm doing it now you can see that the extension here is dot pem you might get
other extensions depending upon the system which you're using and the OSS that it's running or using so depending
on that this extension might vary it's preferable that it is dot pem so yep we
have this key now and let's see what we can do with it
I'm gonna go ahead and select this key and select this for now and I have all the information as far as this key is
concerned now as I've already mentioned that security is very important when you talk about the cloud services so it is
very important that you define a proper Security Group to take care of all these things so before we create an instance
let's quickly go ahead and create a security group first I come here I've actually gone ahead and
created a security group so I would want to go ahead and create one more so I would say create
now when I do that I need to given a name to it say this security for today's
demo is a security demo description for reference sake is what I would say
there you go and we'll be going ahead and adding in rules here and where do I
want certain rules because you can go ahead and Define certain rules how do you want to give access to your system and all those things so I say as far as
my accesses are concerned I'd be wanting an SSH rule see the port range is given
to you because you can communicate with the system through the Esports basically and then I can go ahead and add more I
can say http there you go I say I would be needing HTTP S as well
and since I'm using Windows I would go for RDP these are the different port
names that are there now as far as these groups are concerned so you can see that this has zero zero zero zero zero
security because you would be wanting these ports to be available to others as well because why you're dealing with
data exchange and all those things so these are the ports that need to be active now you have your sshs and your
RDP yes for now they are secured here I can actually go ahead and change it here for the Simplicity sake if I have it
something like this or maybe like this this is how it would look like and for
the sake of being easy to use I'd be having it like this but as far as you
are concerned you need to consider better practices for this you have different sources here why because you
have an option to customize it you can go ahead and customize those because there are certain Services which you
would be wanting to use differently and it is very important that your system is secured because since certain Services
would be accessing your data it is important that access is denied to them in certain cases where it's not required
and that is why you need to consider your security practices now as far as this demo goes this is a basic thing
which we are doing and we can ignore our security policies here this is just for the demo second I would be creating a
simple Security Group so this sources bear with it for now now I say create
and there you go my security group is created it is security demo here that is the name now let us move further and go
ahead and create one of the instances that we can do so I say to I go to Services I say ec2
and I said launch and instance or create an instance now there are quite a few instances here
we have Linux and all those things I would be going for Microsoft Windows
let me just go ahead and pick one we can actually go ahead and pick any the only thing we need to consider is whether
it's eligible for you now when I say eligible I mean eligible for your free
tier basically so let us go ahead and pick one of these I would say base 2012
server let us see whether it's doable for us yep now this is selected by default
because it's free tier eligible and we can we're actually going to go ahead and use it now when you are creating an
instance you need to understand one thing these are the details that are there about the instance and these are some of the points you need to consider
when you say review and launch you can just click on it you would directly be taken to the last step and all the by
default admits are given to you but now that we are doing it for the first time here I would want to do it step by step
there's nothing we are doing we are just selecting the by default thing but I just wanted to see what all it has to offer to you say for example when you
pick a general purpose T2 instance number of CPUs one your memory 1GB
basically and these are the other details are the network performance is low to moderate and all those things so
I say next the other details which I can enter in as in the number of instances purchasing options Network subnet and
all these things is there an IIM Rule and all those things so if for now I'm
not going to make any changes again here and I'm just going to say next as you can see the size I have 30 GBS of
total available with me and these are the details about it iops and all those things so you can just go ahead and say
next now you have an option here whether you want to go ahead and add a tag and all those things and you can do that or you
can just go ahead and say next have your wizard here and there you go I have my seventh step
where I can go ahead and launch my instance you can see I have a notification here saying that improve its security as I've mentioned that we
are doing it for the demo sake and I've chosen few basic ones so I do not have to worry about anything my ports are
open and those are accessible so the thing is my AWS is warning me as you should not have it that way so when you
do go ahead and implement it on larger scales you need to take care that your security groups are well defined and
properly defined for now let's go ahead and say launch now it would ask you to pick a key pair as I've mentioned that
keypad is important whether you're using it on Windows or not but you need to go ahead and Define a key pair so I would
go ahead and select an existing one because we created one proceed with the key pair okay
I say I acknowledge and I say launch and it says that your instance is now
launching so what I'm going to do is I'm going to go ahead and view the launch log and it
gives me details as in whatever inbound rules and all those things are mentioned here how do I connect to my instances
and all those things there's an information here I can just go ahead and save you instances
and there you go I have both of my instances here and the details are given to me as in what are the details now
I've gone ahead and I'm stuck with this availability Zone as far as I'm concerned which is Mumbai for my case
you can go ahead and pick an availability zone for your sake but make sure you stay consistent with it because
that would be more convenient for you that is for your different instances status checks as far as this goes
everything is okay here this was the instance which I launched yesterday this is the one which I've launched today
which is still initializing so there you go you have one here yeah so this was
about going ahead and creating an instance as far as AWS is concerned now
what we can do is we can go ahead and try and do other things as far as your AWS is concerned now the other points
which we need to discuss are can we connect this instance to a server and the answer to this question is yes we
can do that meanwhile what I want is I want my instance to refresh first and everything needs to be in place before
we do that yeah so there you go now as you can see the status checks as far as
the checks are concerned it is complete and we can go ahead and we can connect to a server now so I was talking about
the different service servers that you can go ahead and connect to now if you're using a Linux machine you need to
understand one thing that you have to go ahead into your terminal and then you have to connect to your server but when
you talk about Windows the process is a little different let us see how we can do that using Windows
say for example I select this instance here the second one I say connect so what I'll have to do is I'll have to go
ahead and download a remote desktop file for that and how do I do that
I do this I say connect it would be asking me for my admin
password Here
it would give me an error because I need the password here and I guess I missed out on the password just bear with me
let me just generate the password and I'll get back to you on this okay so how do I do that what I'm going
to do is I'm going to get into my previous instance this one and I'm just going to connect or try to connect it
because I have one here already so as you can see I have this option here which says get me a password as far as
my administrator or username is concerned it is administrator and when I say get password I'll be clicking here
and it would be asking me to choose a file say for example I go ahead and I choose
one of the files now I had downloaded the password last time or the key rather now that is why you
need the key when you store it at some place you'd be asked for this key and you have to enter it here and when you
say decrypt password there you go you'll be getting your password here which is this now I would be copying this
password for my reference sake Ctrl C just to give you a demo again I'll
download it I'll close it and I would say connect it would ask for the admin password and
I say take this do you want to connect despite the certification errors yes
and there you go I've connected to the server here and this is the file which I
created the last time there you have it so this is how you go ahead and actually create your instance and connect to the
server you can go ahead and create another files here you can say new you can have your options it's just like
your nice operating system where you can do so many things yes it's more of a
clean slate and you are full it's fully available to you to go ahead and edit it the way you want to so this is what we
did right now we've actually gone ahead and we've created an SSH key the reason we did was because we wanted to log in
here second thing we went ahead and we created a security group yes it wasn't great but you can actually set security
rules as per your needs then you can go ahead and launch your instance and then you can connect to a server you want to
now this is what I have here I can actually go ahead and do other things as well what are the other things that I
can do well I can go ahead and I can set alarm for my systems and how do I do
that now I have something called as my Cloud watch here under my management
tools when I come here I can go ahead and set alarms as well what kind of alarms say for example I want to monitor
my system and I want to make sure that the usage as far as my system is concerned it does not go above maybe say
70 percent and if it does go above 70 percent and that happens for five
continuous minutes I want my system to throw in an alarm for me so can I do that yes I can actually go ahead and do
that I can create an alarm and I can do that accordingly I click here on alarm I say my ec2 metrics basically or I just
type ec2 here all the services are here and since we are creating an alarm for utilization
let's select that you get all the details here as in how much is the usage and all those things
you can come here and Define the alarm name say CPU usage
greater than 70 percent say and you say through an alarm
when usage exceeds 70 percent
there you go 70 and I moved on and I say notify me how
do I do that send notification to new list suppose I enter in some email ID now
take this for example I suppose I go ahead and use this account here
and I say create an alarm there has to be a name as well say
vpt for example and congratulations alarm has been
created it says insufficient data because the spending information here you need to
understand one thing when you do create an alarm the email ID you enter in AWS would go ahead and it would given a
message to the email ID or in the email ID saying that this is something that is trying to happen here do you want to be
notified when the usage goes this far and if the user says yes or he or she
confirms that that can be done in that case AWS would configure the status saying that yes it is no longer pending
and the status would be displayed here and this would turn to Green so this is how you actually go ahead and create
your own alarm as far as AWS is concerned foreign
[Music] console is a web interface for managing AWS resources it provides access to all
the various services offered by AWS it also provides information related to our
account like billing the AWS Management console provides easy access to services and resources and does not require prior
expertise in apis and sdks firstly let us know how to set up an AWS account
since I have already signed up for an AWS account you can see that I only need
to sign in but if you're a first time user you need to sign up for the AWS account first to set up an AWS account
follow these steps first navigate to the given URL in Step 1 enter your email ID
and password in Step 2 enter your debit card detail now AWS does not charge for
services and resources used up to the free limit we enter the debit card details as it wants to ensure that
you're a legit user with the legit bank account next enter your phone number to receive the verification code next enter
the verification code you have received AWS provides four support plans basic
developer business and Enterprise since we are just beginning I have chosen the
basic support plan you can choose any support plan and complete the sign up process this is how we set up our AWS
account now you must be thinking what if we exceed the free usage limit in that
case you also have the option to set up alarms before free usage it feeds you
can keep a check on the charges of AWS Services the charges for AWS services are estimated consent several times
daily to Cloud watch The Alarm can be set for user defined threshold limit when the charges exceed this limit the
alarm alerts the user before we set up alarms we need to enable the billing alerts to enable billing alerts you must
be signed into the AWS Management console for Consolidated accounts the user can view the billing information
for each of the linked accounts if he is logged in as the paying user you cannot view those members account of billing
information which are part of the Amazon partner Network as billing Matrix for APN accounts are not published to
cloudboard it is important to note that once enabled the billing alerts cannot be disabled to enable billing alerts
follow the steps are shown navigate to the cloudwatch console in the navigation pane select alarms since I've already
enabled alarms but you need to check the checkbox for receive billing alert and click on Save reference the billing
alerts will be enabled to create billing allowance follow the steps are shown navigate to the cloud watch console and
change the region to North Virginia that is U.S east in the navigation pane select Matrix then all Matrix gen select
billing and total estimated charge check the check box next to US Dollars
go to the graph Matrix Tab and click on the create alarm icon the create alarm
page is displayed choose the threshold type and whenever estimated charges is as what you want then Define the
threshold limit I have defined it as 200 US dollars then click next and we reach
the step 2. select the alarm State trigger and select an SMS topic enter
the email ID where you want to receive your notification then click next enter
the name and description for your alarm and click next in the preview and create page verify the information and
conditions and click the oncreate alarm the alarm will be created as you can see
to delete a billing alarm follow the steps are shown open the cloud watch console and ensure that the region is Us
East that is North Virginia in the navigation pane choose on alarm and
check the check box next to the alarm you want to delete in the actions drop down click on delete and in the prompt
box click delete again the alarm is deleted now let us talk about the services offered by AWS and ways to
access those there are three ways to access the AWS Services the first one is through the search box in the search box
type the service that you need the next is through the recently visited section the services access recently appear in
the recently visited section on the home page you can access the service from there the third is through the services
option click on the services option on the screen and see all services there
are range of services offered which are classified into different categories as you can see you can access any service
from here next on the navigation pane we have the search box as we have already
seen in the search box you can type for the service or resource that you need next on the navigation pane we have the
AWS cloud ship it is a browser-based shell that enables the user to manage the AWS resources Some Cloud shell
features include direct access to command line pre-install development tools and sharing of file now let us
look at some advantages of cloud shell they include that it works well with AWS management concern functions it is
automatically updated there is no cost for up to 1 GB of storage and the environment can be customized and
retained AWS also provides us with some tutorials for example you can learn how
to launch a virtual machine as you can see there are some tutorials of small durations that help you perform
different tasks through the AWS Management console all these are available under the Builder solution
title on the home page some common tutorials include launch a virtual machine build a web app build using
virtual server register a domain connect to an iot service and start migrating to
AWS next on the navigation pane we have the Bell icon any issue should your
changes or other notifications appear when we click the Bell icon it has the following feature open issues schedule
changes other notifications and event log in the account option on the navigation frame the information related
to your account is available my account displays the details of the user if the
user is connected to several other users they are arranged in an organization and the organization's details are displayed
you can also view service quota building dashboard and security credentials next
you have the option to choose your country and region certain services are Global which means they are available
globally from anywhere while there are a few other services that are exclusive to
a particular region only AWS Management console also provides you help and support for your problems and queries
support center is available where we can resolve our doubt we can seek expert help there are forums where we can
discuss our queries there is documentation training and getting started Resource Center you can also
view details related to your account's billing information you can view bills orders and invoices credit purchase
orders cost and usage report cost categories Etc
[Music] say whenever you run multiple
application it is certain that you require a server so depending on the application that you
are running you might need only a few server or sometimes you might need a hundred of servers the number of servers
are all dependent as per your requirements Amazon elastic compute Cloud actually provide you those virtual
servers along with secure and resizable compute capacity in the cloud virtual in
the sense that they are not physically existing but you can use them anywhere in the world without any limitation now
with Amazon ec2 it makes it easier for you to obtain these virtual server you simply choose the instance types you
want the template you will like to use which could be based on window Linux or Ubuntu
and launch the quantity you need you can do this with a few clicks from the AWS Management console or automate the
process via the API using SDK in your choice of languages now within minutes your instance will be
up and running with full administrative control now once you're done using your instances you can simply terminate them
and stop paying for them now let's go on right ahead and see why ec2 is one of the most popular AWS
offering now with ecs2 you can scale your instances up or down as per your
requirement secondly they can integrate with other services like S3 EBS VPC RDS
Etc and the best part is you pay only for the time your instance are running so when you stop using your instances
you stop paying for them fourth part is you can launch your instances in one or more region as you already know the
global infrastructure is divided in two regions and inside the region you can have more than one availability Zone
which is nothing but your data center fifth part is you have the freedom to choose different operating system like
window Linux Ubuntu Etc and lastly ec2 comes with a tight security network as
easy to work with virtual private cloud or VPC to provide you that secure network to all the resources which you
are going to utilize now moving on to the next topic Amazon easy to provide a
range of instances which are designed for different use cases Amazon ec2 provides instances optimized for compute
memory storage and GPU processing now they range from cluster compute instances designed for high performance
Computing workload to small economical instances for low volume applications you can see here the different type of
instance size and its use cases for different purposes we shall also discuss these instances
pricing the first type of pricing is on-demand pricing now with on-demand
pricing you only have to pay for what you use now when you stop and instances you simply stop paying for them you can
use your instance aided to launch stop hibernate start reboot and terminate
your instances now coming to the next pricing we have reserved pricing with
Reserve pricing you obtain a significant discount by bidding over the on-demand price in return for a low one-time
payment the third pricing is spot instances now with spot instances it
lets you name the price you want to pay for your instances you can do that by using market-based pricing and it can
allow you to obtain compute capacity at a significant discount to that on-demand pricing
and lastly we have a new type of pricing that is saving plans saving plans allow
you to easily reduce your bill by making a commitment to compute usage say for a
period of one or three years term now there are two types of saving plans one is compute saving plan and another is
ec2 instance Savings Plan now compute saving plans provide the
most flexibility and help reduce your costs up to 66 regardless of its
instance family its size availability zone region operating system or Tennessee now whereas on the other hand
we have ec2 instance saving plans that provide the lowest price offering saving you up to 72 percent in exchange for a
committed period of compute usage I hope they'll hear everyone is familiar with what ec2 is its types and pricing so
let's quickly do a quick Hands-On to see how simple it is to launch an instance after that I'll even show you guys how
to host a server let's get started with the Hands-On and the first thing you need to do is go to
AWS Management console from there you can click on Amazon ec2
click on Services click on S2 and open that in the new tab from there on
you can click on launch instance click on instant then launch instance so
you simply click on launch instance we give it a name for your instance in
this case I'll be naming it as Ubuntu demo and scroll down and click on Ubuntu
which is a free tier by the way and simply click a new keeper scroll down
and set your keeper if you guys don't know what is a key pair you need not worry I'll be explaining you guys what
is a key pair and why do we need one in the later part of the video so simply click on dot PPK and click on
create new keypad and we'll name that as Ubuntu key
for use and click on create keeper so we simply
keep everything to default for now and click on launch instance
once you click on that simply wait for a few minutes until then let me give you a
brief walkthrough on ec2 dashboard
once you click on that simply wait for a few minutes until then let me give you a brief walkthrough on the ec2 dashboard
you can see here in the dashboard the instance that is running in your AWS Management console
say it's instances then like security groups IP address Etc and to the right
here we have a region where you'll be able to select any region based on your location nearer to you
if you have an instance running you will be charged as long as your instance are running so how will you be able to check
which region your instances are running now for that ec2 provided us with the function is to global view the way you
will be able to see how many instances are running in which region without having to select each and every region
now as you can see here eight instances are running in one region only
now coming back to our instance we can see that it is running so the next thing that we need to do is host a website
server in order to do that you simply copy the IP address and simply open
putty and you simply paste your IP address
click on SSH and click on and then paste your key
pair which you have generated a few minutes ago and click here as Ubuntu for use
click on OK and click on open click on accept and simply click here
Ubuntu will be logged into an Ubuntu terminal
simply minimize the screen a bit so you guys can see so before we host a server
the first thing you need to do is type this command sudo apt
get update
so once the message prom has done so the next thing you need to do is install apache2 so in order to do that type on
the code sudo repeat the get install
apache2
click on yes
and it's done so in order for our server to be hosted the next thing you need to do is go back to AWS Management console
go to description where in this part we need to change our security groups since
we've been hosting a server we will be needing an HTTP request so in order to
do that you simply go on action and indeed inbound rules and simply add on rule
click on HTTP and for custom anyway ipv4
simply click on Save once that is done you can go back to your instance
into demo click on open to demo and simply click on the IP address go back
to a new tab and simply paste your IP address as you can see here we have been
prompted with an Apache to default page and if you guys wanted to replace this Apache default Pages you can do this is
this file is located at slash VAR www HTML index.html so we can do that by
simply go back to putty see comment directory
this is the direction of your Apache
as you can see here this file is located at slash VAR slash ww HTML slash
index.html so we can do that by simply go back here and right here HTML
and we said LS and you can see here the file name is index.html now in order for
us to make some changes under file we simply have to remove it first so to remove
index.html which is the name of the file when that is done we can simply edit it
in the HTML template by typing the code sudo Nano index
dot HTML we are provided with an HTML template
foreign
welcome to edureka website
so that will be a heading and simply provide a body and say put a new line
and right here is hello guys this is
Damon from edureka
and simply close the body and we closed on the HTML
exit once that is done simply reload this page
and you can see here you have already added that Apache to file hello guys this is Damon from metal Recca with that
website name as welcome to ederica website so that is it guys for the
Hands-On and how to host a server now getting back to our presentation we
shall understand what is Amazon machine image so what is an Amazon machine image so
just as it name suggests Amazon machine image is basically a template or
information of your operating system to launch instances so it can either be an Ubuntu or window and then Java python
Etc where these files or this template will be converted to an Ami template and
create an N numbers of Ami template now when you launch an instance you must
first specify that Ami so whenever you launch an instance you can use multiple instances for a single Ami but the same
configuration in this case we can use multiple instances for the same Mis say V2
or you can use different emis to launch different instances for different configuration like V2 V3 V4 Etc now in
the next slide we shall also see an Emi lifecycle the first thing you need to do is create a template
in Amazon EPS snapshot then you need to register your Emi
after you have registered your Emi you can then use it to launch new instances or you can use it to copy an Emi with
the same region or different region now when you no longer require an Ami you simply deregister your Emi now let's go
back to the AWS Management console where I'll show you guys how to create your own Emi
so guys let me show you how to create your own Emi in order to do that you
simply go to your instance click on your instance see that you have just created and click on action
click on image and click on create image so we'll provide that a name it'll be as
an Ami image or demo the size will be GB set
everything to default and simply click on create image as you can see here your image has request has been created
simply close that go back to Ami and you can see here your Ami image has
been created and your status is still pending so this is how you create your own Emi
now coming back to the slide we have Amazon elastic block storage so Amazon block storage is nothing but a
level block storage in today's session we will be discussing about Amazon EBS volume Amazon EBS snapshot
and EPS lifecycle manager so coming to the first type we have EBS volume which
is a durable Block Level storage device that you can attach your instances now once you have attached a volume to an
instance the EBS volume can be used as a physical hard drive now EBS volume are
flexible as you can dynamically increase size modify the provisional iops capacity which is nothing but your input
output operational per second now that we have understood what Amazon
EBS volume is let us understand what Amazon EBS snapshot is now EBS snapshot
are used for backing up data for your EBS volume to an Amazon S3 bucket
snapshots when I say they are incremental backup it means that only the block on the device that have
changed after your most recent snapshots are safe now by doing this it minimizes
the time to create snapshot thirdly we have the lifecycle manager
which we can use to automate the creation retention and deletion of EBS snapshot and EBS pack Amis now secondly
it automatically copies snapshot that are created by a lifecycle policy to up
to three AWS region with lifecycle manager fast snapshot restores enable
you to restore volume that are fully initialized at the creation and deliver all the provision performance and the
fourth point is automated cross account snapshot copy which means it use cross
account sharing in conjunction with a cross account copy even policy to automatically share and copy snapshot
created by a policy across accounts now coming back to the Hands-On demo I will
show you guys how we can modify the EPS volume in order to do that go back to
your instances click on Ubuntu demo click on the IP address
open putty and simply paste your IP address click
on SSH click on auth and simply browse your
key they'll be Ubuntu key for use click on open
here you can write as Ubuntu once the authentication has been succeeded
we will minimize the screen so you guys will be able to see we'll type lsblk
and you can see here that of eps volume is 8 gigabytes so in
order to do that click on launch instant click on the instance you have just created
go down to description and you can see here an EPS optimize click on
the root device click on volume as you can see here size 8 click on
action and click on modify volume and in this
case I'll be changing in volume to 100 GB
and simply click on modify and click on yes click on close and go back to putty and
type on lsblk and you can see here our volume has
increased from 8 gigabytes to 100 gigabytes of this so that is how you modify your EBS
volume now coming back to the slide we have networking and Security in Amazon
ec2 so the first networking that we will be looking into is host multiple website
or multiple IP addresses now what this does is it hosts multiple website on a
single server by using an SSL certificate on a single server with a
specific IP address now it operates Network appliances such as firewall or load balancer they have multiple IP
addresses for each network interface now it redirects internal traffic to a
standby instance in case your instance failed by reassigning the secondary IP addresses to the standby instances and
the second point we have is elastic IP addresses now what is an elastic IP address is elastic IP addresses is
nothing but a static IP address which is designed for a dynamic cloud computing an elastic IP address is allocated to
your AWS account and it's yours until you release it alternate natively you can specify the elastic IP address in a
DNS record for your domain so that your domain point to your instance so guys let me show you what an elastic
IP address does so in order to understand what an elastic IP address does some first thing you need to do is
click on ec2 and go to your instances
as you can see here I've already stopped or terminate the instant States so in this case I'll be using an Ubuntu demo
as you can see here in the description there is no IP address for this
following instance so what if you require to add an IP address to this instance which have been stopped so in
order to do that you simply go to an elastic IP and allocate an elastic IP address
simply allocate once that is done simply click on the IP address so we'll be allocating this IP
address to the stop instances that is Ubuntu demo so in order to do that simply click on associate elastic IP
address and choose your instant this is an Ubuntu demo
and click here on associate as you can see here is
3.7.138.126 right so in order to check that you can you can copy that
and paste it in the next Tab and go back there and simply click
on your instance that you have stopped that is Ubuntu demo and you can see here that it's an IP
address has been attached to your Ubuntu demo which have been stopped as you can see here
3.7.138 0.126 which is nothing but your static IP elastic IP address
so that is how you associate your IP address to any instance which has been stop or terminated
the next thing we have is security groups a security group acts as a firewall for your ec2 instances you can
add rules to your each security groups that allow traffic or from its Associated instances they control the
incoming and outgoing traffic inbound rules control the incoming traffic to your instance and outbound rules
controls the outgoing traffic from your instance when you launch an instance you can specify one or more Security Group
if you don't specify a security group Amazon ec2 uses the default Security Group the next thing that we're gonna
look upon is key pair so what is a key pair a key pair is a set of security
credentials that you can use to prove your identity when connecting to an Amazon ec2 instance now a key pair
consists of a public key and a private key Amazon ec2 stores the public key on
your instance whereas you saw the private key the private key allows you to securely SSH
into your instance we shall see ec2 use cases in the real life application the
first use cases is running Cloud native and Enterprise application now Amazon
ec2 delivers secure reliable high performance and cost effective compute
infrastructure to meet demanding business needs and the Second Use cases is scalp for HPC application now it
access the on-demand infrastructure and capacity you need to run HPC application faster and cost effectively and the
third use cases is developed for Apple platform as the name suggests it is used for building testing and sign on demand
Macos workload access environment in minutes dynamically scale capacity as
needed and benefit from AWS pay as you go pricing and the fourth use cases is
train and deploy ml applications now Amazon an easy to deliver the broadest
choice of compute networking and storage Services purpose built to optimize price
performance for machine learning projects now coming to the next part is how do we provision our ec2 instance now
Amazon machine image or Ami is supported and maintained image provided by AWS it
is basically a template for your instance to be launched firstly we need to create an Amazon machine image like I
explained a while ago which is nothing but a template for your software operating system secondly we choose our
instance type we have only looked at the table of different types of instance in the previous slide and basically the
type is saying how powerful you want your machine to be how many CPUs memory storage or GPU you want to have thirdly
we will configure the instances how many instances you want which subnet you want it to be assigning your IP address then
its shutdown Behavior where you can stop pause or terminate your instances now
when you start an instances it has to have an operating system and that's a
disk which is basically a storage and that storage is an EBS volume when we
launch an instances we can add a tag they are value pairs which allow you to Simply identify your instance and
classify it and lastly you review your instance launch detail you can go back
and edit changes for each section or you can click launch to complete your process this is just an overview on how
to launch an ec2 instance which I have already showed you guys with a Hands-On demo
so in the next slide we shall understand what is load balancing in this video I
will only give you guys an introduction as to what is load balancing since the topic itself is vast so getting back to
the topic uh what does alert balancing does elastic load balancing automatically
distribute your incoming traffic across multiple targets such as ec2
instances containers and IP addresses in one or more
availability Zone what it mean is that say when you are streaming Netflix in India around
daytime or evening time the traffic will be huge since a lot of people will be streaming at the moment but a load
balancer does is it manages those traffic and distribute the load to another server which is available in
different region or Zone and increase the number of ec2 instances so the load is balanced without disrupting the
overall flow and it increases the availability and fault tolerance of your
applications now the next thing that we are going to discuss is auto scaling
just as the name suggests it monitors your application and automatically adjusts capacity to maintain steady
predictable performance at the lowest possible course so in order to understand what that mean is let me give
you an example so in the daytime a lot of people will be using the server when
that happens Auto scaling simply monitors and add more ec2 instances so the server can perform its tasks without
failing and when it's around midnight the amount of people using the server will decrease so the requirement of
server is also decreased so Auto scaling scales out the extra server which is not
required and scale up again if a lot of user is using it again so that is basically what an auto
scaling and a load balancer does now coming to the architecture we will understand how we launch an ec2
instances the first thing you need to do is select the region whichever it's closer to you then we have a VPC so we
are simply using the default VPC in this case and we have the availability Zone and each VPC has a number of public
Subnet in this case we can use two we can also have just one subnet
now if you guys are wondering what is the difference between a public subnet and a private Subnet in a public subnet
we attach an internet gateway and we have a routing table which point to that internet gateway and our ec2 instances
have public IP addresses those are the three things which you need first the ec2 instance must have a public IP
addresses we need to have an internet gateway which is attached to a VPC but you need a route table entry which
points to the internet [Music]
foreign
development and operations so what do we get when we combine these these two
simply speaking devops is a culture that implements technology in order to
promote collaboration between development and operations team to deploy code to production faster in an
automated and repeatable way the goal of devops is to increase an organization's
speed when it comes to delivering applications and services many companies
have successfully implemented devops to enhance their user experience let's take for example Facebook's mobile app which
is updated every two weeks effectively telling users you can have what you want and you can have it now ever wondered
how Facebook is able to do this so smoothly it's divorce philosophy that helps Facebook ensure that its apps
aren't outdated and that users get the best experience Facebook accomplishes this through a code ownership model that
makes its developers responsible that includes testing and supporting through production and Delivery for each kernel
of code they write an update extra policies like this that Facebook has developed a divorce culture and has
successfully accelerated its development life cycle Industries have started to cure up for the digital transformation
by shifting their needs to weeks and months instead of years while maintaining high quality as a result we
will soon see that devops Engineers have more access and control of the end user than any other person in the Enterprise
so what are you waiting for don't be in the sidelines when that happens to master your skills enroll in edureka's
devops certification program and become a leader [Music]
why do we need devops the birth of devops occurred when there
were some drawbacks with the existing software development models for application development the first model
we'll be addressing today is the waterfall model so the waterfall model is a model of software development which
is pretty straightforward and linear so this model follows a top-down approach as you can see in the image it has
various phases starting with requirements Gathering and Analysis so the first phase is where you get the
requirements from the client for developing an application so after this you try to analyze these requirements
that you've acquired next comes the design phase where you prepare a blueprint of the software so in this
phase you think about how the software is actually going to look like once the design is ready you proceed further with
the implementation phase where you begin with the coding for the application so here the team of developers work
together on various components of the application and once the application is developed it is tested in the
verification phase that is the fourth phase here there are various tests conducted on the application such as
unit testing integration testing performance testing Etc after all of these tests on the applications are done
it is finally deployed on the production servers at last that is the fifth phase
is the maintenance phase so in this phase the application is monitored for performance so any issues related to the
performance of the application are resolved in this phase but unfortunately this model that is the waterfall model
has some major drawbacks so the first drawback is gathering and documenting your requirements each step of the way
can be extremely time consuming not to also mention difficult it is hard to
assume things about your products so early into the project and because of the same reason your assumptions might
be flawed and different from what the customer expects the second drawback is that now if the above case is true your
customers are basically dissatisfied with your delivered product adding changes to the product can be expensive
and most of all difficult to implement so the third drawback is that in general
the risk is extremely high with the waterfall approach because the scope for mistakes is extremely high if things go
wrong fixing them can be hard as you have to go a couple of steps back the next model is the agile model so agile
is an iterative based software development approach where the software project is broken down into various
iterations or Sprints so every iteration has phases like the waterfall models
such as requirements Gathering design development testing and maintenance you can see each of these iteration in the
image so each of this iteration generally lasts for about two to eight weeks so the difference between agile
and waterfall model is that you release the application with some high priority features in the first iteration after
its release the end users or the customers can give you feedback about the performance of the application
so the necessary changes are made to the application along with some new features and once these features are added to
this application it is again released which is in the second iteration this procedure is repeated until the desired
software quality is finally achieved now in spite of being used often the agile method has a few disadvantages the first
one is that for the approach to work all the members of the team must be completely dedicated to the project that
is everyone must be involved equally if you want the whole team to learn and do better on the next run and this is
mainly because agile focuses on quick delivery there might be an issue with hitting deadline the second drawback is
that the approach may seem extremely simple but it is hard to execute so it requires commitment and for everyone to
be on the same page ideally in the same physical space the third drawback is that documentation
can sometimes be ignored because agile methodology focuses on working software
over comprehensive documentation things might sometimes get lost through each stage and iteration as a result the
final product can feel different from what it was first planned so these are the few drawbacks that the agile model
or the agile methodology gives us so in delivering valuable software to
customers development and operation teams are always in conflict with each other while development wants to deliver its
changes to customers quickly operations want stability which means not changing the production systems too often so the
gap between the development and operations team occurs on three different levels the first one is the
incentives Gap this is because of different goals of development and operations the second one is the process
gap which is the result from different approaches of development and operations and how to manage changes bring them to
production and maintain them there the third one is the tools gap which results from the fact that development and
operations often use their own tools to do their own work and this is resulting from the fact that development and
operations often use their own tools to do their work as a result from all of these reasons development and operations
often act like silos as they are two distinct teams so the conflict between development and operations is because of
two reasons the first one is Need For Change this is because development results in change that is they're always
working on new features bug fixes Etc it wants the change to quickly roll out to
production and the second one is fear of change once the software is finally delivered the operations Department
avoids making changes to the software to ensure stability this is the major conflict between both of these teams so
this is where devops comes into picture devops links software development to operations I hope that is clear that is
devops Bridges the gap between agile software development and operations experiences it is important to know that
devops is a set of principles to break down silos specifically devops is all
about culture automation measurement and sharing so the the first one is culture
so in culture people and processes comes first if you don't have a culture all automation attempts will be fruitless
the relationship is important in culture because its functions include engage early engage often destroy silos be open
to options and stop blaming and the second one is automation so once you understand your culture you can simply
start with automation right now you can finalize various tools to achieve automation for devops that is tools for
release management provisioning configuration Management Systems integration monitoring and control and
orchestration becomes extremely important pieces for devops the third is measurement so if you cannot measure you
cannot improve make sense right a successful devops implementation will always measure everything it can as
often as it can performance metrics process metrics people metrics capacity
planning Trend analysis fault finding Etc all of these have to always be
measured so the last one is devops is about sharing sharing is a loopback in
the cycle so creating a culture where people share ideas and problems is extremely critical so exposing ideas can
create great open feedback that in the end it helps to improve shared ideas share metrics give devops shell access
see what technology can be leveraged Etc so devops is all about these four
particular features I hope this is clear so now let's move on to the next part of
today's session that is Introduction to devops so finally we will address what is devops devops is simply a combination
of two words you can see that one is the software development and the second one is operations so this allows a single
team to handle the entire application life cycle that is from development to testing deployment and finally
operations so devops really helps you to reduce the disconnection between software developers quality assurance
engineers and system administrators that is it basically promotes collaboration between the development and operations
team to deploy code to production faster in an automated and repeatable way so
devops basically helps to increase organization speed to deliver applications and services it also allows
organizations to serve their customers better and compete more strongly in the market it can be also defined in another
way that is it is also a sequence of development in it operations with better communication and collaboration so in
the mark Market it has become one of the most valuable business disciplines for Enterprises or organizations with the
help of devops the quality and speed of the application delivery has improved to an extremely great extent organizations
that have adopted devops notice a 22 percent Improvement in software quality and a 17 Improvement in application
deployment frequency they have also achieved a 22 percent hike in customer satisfaction overall a 19 of Revenue
hikes as a result of the successful devops implementation has been achieved
moving on devops workflow is another important concept so what is a devops
workflow so it basically provides a visual overview of the sequence in which input is provided also it tells about
which one action is performed and output is generated for an operations process
so basically devops workflow allows the ability to separate and arrange the jobs which a top requested by the users also
this workflow gives the ability to mirror that ideal process in the configuration jobs moving on let's see some of the devops
principles so the main principles of devops are continuous delivery Automation and fast reaction to the
feedback but there are six other principles that we'll be discussing today the first one is the end-to-end
responsibility so devops teams need to provide performance support until they become the end of life it enhances the
responsibility and the quality of the products engineered the second one is continuous Improvement devops culture
focuses on continuous Improvement to minimize waste that is it continuously speeds up the growth of the products or
the services that has been offered the third one is automate everything automation is an essential part of the
devops process we'll be discussing about this later on in this session this is for the software development and also
for the entire infrastructure landscape the fourth one is custom Centric action
so the devops team must be customer Centric so that they should continuously invest in products and services the
fifth one is Monitor and test everything the devops team needs to have robust monitoring and testing procedures the
last one is work as one team in the devops culture role of the designers developers and testers are already
defined so all they need to do is work as one team with complete collaboration these principles are achieved through
several devops practices which include frequent deployments quality assurance automation continuous delivery
validating ideas as early as possible and in team collaboration now let's move
on to the third topic of today's session companies using devops by now you must have figured out that
devops helps to increase an organization's speed to deliver applications and services this is
exactly why many organizations such as Amazon Netflix Etsy HP Adobe and many other
organizations adopting devops and its practices we will now discuss how Amazon
makes use of devops to increase their work efficiency you must be aware that Amazon is one of the biggest e-commerce
companies in the world but way back in time their website followed a traditional monolithic architecture so
in this type of architecture all the processes are coupled together and run as a single service
over time as the code and the source files grew it became hard to scale maintain and upgrade their applications
on physical servers so what did Amazon do Amazon simply solved the problem of
the monolithic architecture by moving from physical servers to cloud-based Amazon web services you all must have
heard of AWS currently AWS follows a microservice architecture in Amazon
there basically three types of microservices they are users threads and posts developers here apply frequent but
small changes over the code via Version Control tools like get and GitHub practices like code deployment help fix
bugs and adds new features to improve the underlying software application AWS
code deploy is once a service that keeps track of deployments and simplifies the
entire software release process Amazon also uses Apollo Apollo is a simple
one-click internal deployment tool so Apollo's job is to deploy a specified set of software across a group of hosts
on the other hand practices like configuration management and infrastructure s code help to Monitor
and make changes in the software it keeps track of the system's performance and resources used by developers so in
this way the testing team can identify problems way before in hand and fix them immediately so this is how Amazon
Implement devops and its practices now let's move on and understand one major
concept of devops that is the devops lifecycle so if you are into devops from
quite some time now you must have heard at least one of these devops faces that is continuous development continuous
integration continuous testing continuous deployment and continuous monitoring so all of these phases that I
just mentioned make up the devops life cycle as you can see on the screen and as I've already mentioned all of these
phases make up the devops life cycle the first phase is the continuous development phase so this is the phase
that involves planning and coding of the software so the vision of the project is decided during the planning phase and
the developers begin developing the code for the application there are no devops tools that are required for planning but
there are a number of tools for maintaining the code the code can be written in any language but it is mainly
maintained by using Version Control tools so here maintaining the code is referred to a source code management so
some of the most popular version control tools are git SVN mercurials CVS X
cetera also tools like ant Maven Gradle can be used in this phase for building
or packaging the code into an executable file that can eventually be forwarded to any of the next phases
the next phase in the devops life cycle is continuous testing so this is the stage where the develop software is
continuously tested for bugs for continuous testing automation testing tools like selenium test NG junit Etc
are used so basically the tools that I just mentioned right now allow quality assurances to test multiple code bases
thoroughly in parallel to ensure there are no flaws in the functionality so what automation testing does is it saves
a lot of time effort and labor for executing the test instead of doing this manually right also in this phase Docker
containers can be used for simulating the test environment selenium heat does the automation testing and the reports
are generated by test NG so this entire testing phase can be automated with the
help of a continuous integration tool called Jenkins we'll talk about Jenkins later on in the session so now let me
explain you this with an example suppose you've written a selenium code in Java to test your application now what you
can do is you can build this code using and or Maven and once the code is built it is tested for user acceptance testing
so this entire process can be automated using Jenkins besides that the report
generation is a very big plus point the task of evaluating the test cases that
failed in a test Suite gets simpler we can also schedule the execution of test cases at predefined times once the
entire testing is done the code is continuously integrated with the existing code the third phase of this
life cycle is continuous integration so now this stage is the heart of the entire devops life cycle it is a
software development practice in which the developers require to commit changes to the source code more frequently this
may be on a daily or a weekly basis so every comment is then built and this
allows early detection of problems if they are present building code does not involve compilation but it also includes
code review unit testing integration testing and packaging the code that is supporting new functionality is
continuously integrated with the existing code since there is continuous development of software the updated code
needs to be integrated continuously as well as smoothly with the systems to reflect changes to the end users
as I've already said in the previous slide Jenkins is a very popular tool used in this phase so whenever there is
a change in the git repository Jenkins simply fetches the updated code and It prepares a build of that code which is
in an executable file so once this entire build is performed it is then forwarded to the test server or the
production server now let's move on to the fourth phase of the cycle that is the continuous deployment phase so this
is the stage where the code is deployed to the production servers it is very important to ensure that the code is
correctly deployed on all the servers now before moving on let us understand a few things about configuration
management and containerization tools now these set of tools help in achieving
continuous deployment firstly I will talk about configuration management it is basically the act of establishing and
maintaining consistency in an application's functional requirements and performance so let me put this in
simple words it is the act of releasing deployments to servers scheduling updates and most importantly keeping the
configurations consistent across all the servers since the new code is deployed on a continuous basis configuration
management tools play an extremely important role in executing tasks quickly and frequently now some popular
tools that are used here are puppet Chef Soul stack and ansible containerization tools also play an
equally important role in the deployment stage Docker is one of the most popular tool used for this purpose basically
these tools help produce consistency across development test staging and production environments besides this
they also help in scaling up and scaling down of instances very swiftly
basically containerization tools help in maintaining consistency across the environments where the application is
deployed developed and tested using these tools there is no scope of errors
or failures in the production environment as they package and replicate the same dependencies and
packages used in the development or testing or staging environment basically it makes your application easy to run on
different computers the next stage is the continuous monitoring stage this is the last stage
in the devops life cycle this is a very crucial stage where you continuously monitor the performance of your
application so here Vital Information about the use of the software is recorded this information is processed
to recognize the proper functionality of the application the system errors such as low memory server not reachable Etc
are resolved in this phase the root cause of any issue is determined in this phase it also maintains security and
availability of the services also if there are any network issues they are resolved in this phase that is
it helps us automatically fix the problem as soon as they're detected so now this practice involves the
participation of the operations team who will monitor the user activity for bugs or any improper behavior of the system
the most popular tools used for this phase is Splunk El key stack negios New
Relic and sensual so what these tools do is they help you monitor the application's performance and the
servers very closely and they also eventually enable you to check the health of the system proactively now
using these tools they can also improve productivity and increase the reliability of the systems which in turn
reduces it support costs any major issues if found are reported to the
development team so that it can be fixed in the continuous development phase this also leads to a faster resolution of the
problems the stages are carried out on Loop continuously till you achieve the desired product quality therefore almost
all of the major it companies have shifted to devops for building their products
moving on if you want to become a devops engineer or be a part of this ideology it is extremely important for you to be
familiar with some devops related tools today I will be talking about seven important devops tools now before we
understand the tools devops Engineers use it is important to understand containers so what are containers
containers allow you to package your application and its dependencies together into one manifest that can be
version controlled allowing for easy replication of your application across developers on your team and machines in
your cluster this is exactly why container-based microservices architectures have profoundly changed
the way development and operation teams test and deploy software containers
really help companies modernize by making it easier to scale and deploy applications but they have also
introduced new challenges in complexity by creating an entirely new infrastructure ecosystem
it companies are now deploying thousands of container instances daily and that's
a complexity of scale they have to manage how do they do it here is where
kubernetes comes to the rescue kubernetes is a container management technology developed in Google to manage
containerized applications in different kinds of environments like physical virtual Cloud infrastructure Etc so it
really is just an open source system which helps in creating and managing containerization of applications its
container management responsibilities include container deployment scaling and descaling of containers and container
load balancing it is extremely important to remember that kubernetes is not a
containerization platform it is a multi-container management solution now let's move on and discuss some of the
features of kubernetes the first one is automated scheduling so kubernetes
provides an advanced scheduler to launch containers on cluster nodes based on the
resource requirements and other constraints while not sacrificing availability the second feature is
self-healing capabilities so kubernetes allows you to replace and reschedule
containers when nodes die it also kills containers that do not respond to user-defined health checks and does not
advertise them to clients until they are ready to serve the third feature is
automated rollouts and rollback so what kubernetes does is it rolls out changes
to the application or its configuration while monitoring your application Health to ensure it does not kill all your
instances at the same time so if something goes wrong with kubernetes you can simply roll back the change the
fourth feature is horizontal scaling and load balancing kubernetes can scale up
and scale down the application as per the requirements with a simple command using a user interface or automatically
based on CPU usage now let us understand the architecture the kubernetes
architecture has two main components the first one is the masternode the second one is the worker node so the master
node is responsible for the management of kubernetes cluster it is mainly the
entry point for all administrative tasks they can always be more than one Master
node in the cluster to check for fault tolerance so there are few components in
the master node that I'd like to discuss the first one is the API server so here the API server is the entry point for
all the resd commands used to control the cluster the second one is the controller manager it is a Daemon that
regulates the kubernetes cluster and manages different non-terminating control loops the third one is the
scheduler so the scheduler schedules the task to worker nodes so what it really does is it stores the resource usage
information for each worker node the fourth one is the etcd so the etcd is
simple distributed consistent key value store so what it mainly is used for is
to share configuration and service discovery now that we know how the masternodes
work let's move on to the worker nodes so these worker nodes contain all the necessary services to manage the
networking between the containers communicate with the masternode and assign resources to the scheduled
containers so the first component that we'll be talking about in this worker node is the
docker container so Docker runs on each of the worker nodes and runs the configured pods the second one is the
cubelet the cubelet gets the configuration of a pod from the API server and ensures that the Discraft
containers are up and running the third one is Cube proxy so Cube proxy acts as
a network proxy and a load balancer for a service on a single Walker node the
fourth one is pods so a pod is one or more containers that logically run together on nodes
with this we come to the end of the First Tool in devops tool let's move on to the second tool that is Docker so
what is a Docker Docker is simply a containerization platform that packages
your application and all its dependencies together in the form of containers to ensure that your
application works seamlessly in any environment so each application here will run on a separate container and
will have its own set of libraries and dependencies it must be really convenient to move around these
containers right so containers also ensures that there is a process level isolation meaning each
application is independent of other applications giving developers surety that they can build applications that
will not interfere with one another now the quality assurance team need not
install all the dependent software and applications to test the code and this helps them save lots and lots of time
and energy this also ensures that the working environment is consistent across all the individuals involved in the
process starting from development to deployment so here the number of systems can be scaled up easily and the code can
be deployed on them effortlessly there are some basic concepts of Docker you must be aware of that is Doc file Docker
image and Docker container so a Docker image is created by the sequence of commands written in a file called Docker
file when this Docker file is executed using a Docker command it results into a
Docker image with the name so finally when this image is executed by Docker run command it will by itself
start whatever application or service it must start on its execution so I hope Docker file Docker images and
Docker containers are clear we'll understand what is Docker Hub now so Docker Hub is basically a cloud registry
where you can find Docker images uploaded by different communities also you can develop your own image and
upload on Docker Hub But first you need to create an accountant Docker hub so let's move on and understand what a
Docker engine is so the docker engine is the heart of the system it is simply the
application that is installed on your host machine basically it works like a client server application it uses a
server which is a type of a long running program called a Daemon process it also uses a command line interface client and
finally it uses an rest API which is used for communication between the CLI
client and Docker daemin as per the image in a Linux operating system there
is a client which can be accessed from the terminal and a host which runs the daemin we build our images and run
containers by passing commands from the CLI client to the Daemon however in the
case of Windows or Mac there is an additional toolbox component inside the docker host so this Docker toolbox is an
installer to quickly and easily install and set up a Docker environment on your windows or your MacBook
so this toolbox installs Docker client machine compose which is only present in
mac and virtualbox with this we come to the end of Docker we'll move on to the
third tool of devops tools that is git so git is an extremely important tool for software developers there is a free
open source distributed Version Control System designed to handle everything from small to very large projects with
speed and efficiency it was created by lennis travels in 2005 to develop Linux
kernel git has the functionality performance security and flexibility that most teams and individual
developers need it also serves as an important distributed Version Control devops tool so get this primarily used
to manage your project comprising a set of code or text files that may change over time
now before we go further let us take a step back to learn about Version Control Systems and how git came into existence
so Version Control is the management of changes to documents computer programs large websites and other collection of
information there are basically two types of Version Control Systems the first one being centralized Version
Control System the second one is distributed version control system so git is a distributed Version Control
System where every contributor that is the user has a local copy or clone of
the main repository what is the repository a repository is simply a storage space for your project files
here everyone maintains a local repository of their own which contains all the files and metadata present in
the main repository so the local repository is usually on the local machine of the user so every programmer
maintains a local repository which is actually the copy or clone of the central repository on the hard drive the
central repository may be present on a web hosting platform like GitHub they can commit and update the local
repository without any interference they can also update their local repositories with new data from the central server by
an operation called pull and effect changes to the main repository by an operation called push from the local
repository so I hope everyone understood how git works now moving on let us
understand the role of git in devops so git plays a very vital role when it comes to managing the code that the
collaborators contribute to the shared repository so the score is then extracted for performing continuous
integration to create a build and test it on the test server and eventually deploy it to the production tools like
get enable communication between the development and the operations team when you're developing a large project with a
huge number of collaborators it is extremely important to have communication between the collaborators
while making changes within the project commit messages and get paid a very
important role in communicating among the team the bits and pieces that we all deploy lies in the version control
system like get to succeed in devops you need to have all of the communication in
Version Control hence git plays a very vital role in succeeding at devops now
let's move on to the next tool that is selenium selenium is one of the most widely used open source web user
interface automation testing Suite it was originally developed by Jason Hawkins in 2004 as an internal tool at
thoughtworks so selenium supports automation across different browsers platforms and programming languages
so this tool can be easily deployed on platforms such as Windows Linux Solaris and Macintosh moreover it supports
operating systems for mobile applications like Windows and Android selenium supports a wide variety of
programming languages through the use of drivers specific to each language language is supported by selenium
include c-sharp Java Perl PHP Python and Ruby currently selenium Webdriver is the
most popular with Java and c-sharp selenium test strips can be coded in any of the supported programming languages
and can be run directly in most modern web browsers browsers supported by
selenium include Internet Explorer Mozilla Firefox Google Chrome and Safari
so basically it can be used to automate functional tests and can be integrated with automation test tools such as Maven
Jenkins and Docker to achieve continuous testing it can be also integrated with
tools such as test NG and junit for managing test cases and generating
reports so now moving on what are the advantages of selenium why should we choose selenium since selenium is open
source there is no licensing cost involved which is a major advantage over other testing tools there are many other
reasons behind selenium's ever-growing popularity the first one is that test scripts can be written in any of the
programming languages that is Java python C sharp PHP Ruby pearl.net Etc
desk can also be carried out in any of the operating systems that is Windows Mac or Linux test can also be carried
out using any browser that is Mozilla Firefox Internet Explorer Google Chrome Safari Opera we've all discussed this in
the previous slide and the last use is that it can be integrated with Maven Jenkins and Docker
to achieve continuous testing so in short selenium can be used for automated testing also because it has
cross browsers compatibility it is more preferred and it also increases test coverage and reduces test execution time
now moving on let us understand an important Concept in selenium that is the selenium grid selenium grid was
developed by Patrick lightbody and initially called it hosted QA it was used in a combination with remote
control to run tests on remote machines in fact with grid multiple test scripts
can be executed at the same time on multiple machines parallel execution is
achieved with the help of Hub node architecture one machine will assume the role of Hub and the others will be the
nodes Hub controls the test scripts running on various browsers inside various operating systems test scripts
being executed on different nodes can be written in different programming languages grid is still in use and works
with both Webdriver and remote control so we'll be learning about the Webdriver in the next slide however maintaining a
grid with all required browsers and operating systems is a very huge challenge for this there are multiple
online platforms that provide an online selenium grid that lets you access to run your selenium automation scripts for
example you can use Lambda test it has more than 2000 browser environments over which you can run your test and truly
automate cross browser testing now let's understand what is selenium Webdriver
selenium Webdriver is a web-based automation testing framework that can test web pages initiated on various web
browsers and various operating systems in fact you also have the freedom to write test scripts in different
programming languages like Java Perl python Etc it is important to remember
that Mozilla Firefox selenium webdriver's default browser now some benefits of selenium Webdriver are that
they support seven programming languages they test on various browsers and they work on different operating systems also
it overcomes limitations of selenium version 1 like file upload download pop-ups and dialogues video now let's
move on to the next most important tool that is Jenkins continuous integration is the most
important part of devops that is used to integrate various devops stages
Jenkins is the most popular continuous integration tool it is an open source
automation tool written in Java with plugins built for continuous integration purposes
Jenkins is basically used to build and test your software projects continuously
making it easier for developers to integrate changes to the project and making it easier for users to obtain a
fresh build it also allows you to continuously deliver your software by integrating with a large number of
testing and deployment Technologies so basically with Jenkins organizations can
accelerate the software development process through automation Jenkins integrates development lifecycle
processes of all kinds including build document test package stage deploy
static analysis and much more so this tool also achieves continuous integration with the help of plugins
plugins are extremely important it allows the integration of various devops stages if you want to integrate a
particular tool you need to install that particular plugin for that tool for example git Maven to project Amazon ECT
2 HTML publisher Etc there are many advantages of Jenkins and
I'd want to discuss each one of them in detail so the first one is it is an open source tool with a great Community
Support the second one is it is extremely easy to install the next one is it has more than
thousand plugins to ease your work now if a plugin does not exist you can simply code it and share it with the
community the fourth Advantage is it is free of cost the last one is it is built
with Java now if you have been coding for some time now you must be knowing that Java is portable to all the major
platforms so this gives an added advantage to Jenkins so what makes Jenkins different from the
other continuous integration tools there are two reasons the first one is adoption the second one is plugins so
Jenkins is widespread with more than one lakh 47 000 active installations and
over 1 million users around the world the adoption of Jenkins is extremely high the second one is that Jenkins is
interconnected with well over thousand plugins that allow it to integrate with most of the development testing and
deployment tools so these two features that is adoption and plugins make it one of the best continuous integration tools
currently available in the market now let's move on to the next tool of devops tools that is puppet today the
most mature tool for configuration management is puppet but I know you must be wondering why puppet is so popular
and what makes it unique when compared to other configuration management tools puppet is a configuration management
tool that is used for deploying configuring and managing servers so now
let's understand the functions of puppet so the first one is it helps in defining distinct configurations for each and
every host and it also continuously checks and confirms whether the required configuration is in place and is not
altered on the host the second function is dynamic scaling up and scaling down of machines the last
one is it provides control over all your configured machines so a centralized change gets propagated to all
automatically puppet basically uses a master sleeve architecture in which the
master and slave communicate through a secure encrypted Channel with the help of SSL now let's see this architecture
in detail so as you can see the diagram on the screen the puppet agent sends the fax to the Puppet Master facts are
basically key value data pair that represents some aspect of the slave state such as its IP address uptime
operating system or whether it's a virtual machine Puppet Master uses the facts to compile
a catalog that defines how the slave should be configured catalog is a document that describes the desired
state for each resource that Puppet Master manages on a Slave puppet slave reports back to the master
indicating that the configuration is finally complete which is visible in the puppet dashboard
now with this we come to the last tool of devops Nag iOS not iOS monitors your entire it
infrastructure to ensure systems applications servers and business processes are functioning properly it is
an integral to the devops life cycle so it is basically used for continuously monitoring of systems application
servers and business processors in a devops culture so in the event of a
failure nag iOS can alert technical staff of the problem allowing them to
begin remediation processes before outages affect business processes end users or customers
with this tool you don't really have to explain why an unseen infrastructure outage affects your organization's
bottom line now the iOS runs on a server usually as a Daemon or a service it periodically
runs plugins residing on the same server the contact hosts or servers on your
network or on the Internet one can view the status information using the web interface you can also receive email or
SMS notification if something happens the NOG iOS Daemon behaves like a
scheduler that runs certain scripts at certain moments it stores the results of those scripts and will run other scripts
if these result changes now the iOS also use plugins so these
are compiled executables scripts that can be run from a command line to check the status or a host or a service nag
iOS also uses the results from the plugins to determine the current status of the hosts and services on your
network now the iOS is built on a server or agent's architecture usually on a
network a nag iOS server is running on a host and plugins interact with local and
all the remote hosts that need to be monitored so these plugins will send information to the scheduler which
displays that in a graphical user interface with this we come to the end of devops tools let's move on to the
next part of the session that is devops automation automation is the crucial need for
devops practices and automating everything is the fundamental principle of devops we discussed this initially in
the session automation kickstarts from the code generation on the developers machine until the code is pushed to the code and
after that to monitor the application and system in the production automating infrastructure setup and
configurations software deployment is the key highlight of devops practice devops practice is dependent on
automation to make deliveries over a few hours and make frequent deliveries across platforms
Automation and devops boosts speed consistency higher accuracy reliability
and increases the number of deliveries Automation and devops encapsulates
everything right from the building deploying and monitoring in large devops team that meet in extensive massive I.T
infrastructure the tools can be classified into six categories that is infrastructure automation configuration
management deployment automation Performance Management log management and monitoring so for infrastructure
automation a great example is the Amazon web services being a cloud service you
don't really need to be physically present in the data center they're easy to scale on demand and there are no
upfront Hardware costs it can also be configured to provide more servers based on traffic automatically for the second
one Chef is a great example Chef is a handy devops tool for achieving speed scale and consistency it can be used
these are complex tasks and perform configuration management with the help of this tool the devops team can avoid
making changes across 10 000 servers rather they need to make changes in one
place which is automatically reflected in other servers for deployment automation Jenkins is a
great example as we have already discussed before it helps facilitate continuous integration and testing it
integrates project changes more efficiently by quickly finding issues as soon as bills are deployed
for the fourth one that is Performance Management application Dynamic is a great example it offers real-time
performance monitoring the data that is collected by this tool help developers to debug whenever an issue occurs
the fifth one that is for log management Splunk is a great example this devops
tool solves issues such as storing aggregating and analyzing all logs in
one place so for the last one that is monitoring nag iOS is a great example it notifies
people when infrastructure and related Services go down it is a tool for this purpose which helps the devops team to
find and correct problems immediately [Music]
foreign let's start out with what's Version Control so to understand what is Version Control
System we need to understand first why we need a version control system so we'll see why do we exactly need a
version control system so for that let's take a scenario where there are three developers working remotely on a web
application or a mobile application now for simplicity's sake let's just assume
that it is a streaming application and one of them is working on a streaming
page let's call them developer one developer 2 is working on a user
information page and developer 3 is creating the payment portal now all three of them are done with their
respective Pages at 10 o'clock 11 o'clock and 12 o'clock respectively now
thing is all three developers are working in isolation there may be their own changes there may be changes in
somebody's else's page they're all kind of adding some new files or modifying
older files changing the source code or something like that throughout the course of this project
but how exactly are they going to collaborate considering that they are working remotely the solution to this is
a version control system with this system I'll tell you what happens at 10 o'clock developer 1 uploads all the
files regarding the streaming page with the Version Control System this modification is recorded and updated to
one Central directory or folder so every time you're modifying an older file or
adding a new file it creates a snapshot of the latest version or the latest
update of the changes that you've made or all the files that you've uploaded so the snapshot of your update at 10
o'clock is saved and then when developer 2 adds their user info page this
modification is also saved as a snapshot now there are two snapshots saved and
finally the payment portal page is added and this is the last modification done
at 12 o'clock and this is the most recent snapshot saved along with other snapshots of other updates and
modifications done throughout the course of this project every change in your project a snapshot is created and your
entire project is saved that way and these snapshots are actually known as
different versions which are basically the state of your project at a
particular time or at the current time it means that it will contain the kind of files your project is storing at one
particular time and what kind of changes you have made so far between the
previous version and this particular version and this is exactly how a version control system works so Version
Control System basically is a system that records changes to a file or a set
of files over time so that you can record specific versions later these
versions are recorded in a repository and they can be called any time from these repositories during the course of
your project so person Control Systems essentially are of three types now the local version
control system is one of the simplest forms of VCS and has a database that
keeps all the changes of the files under revision control so all the files are
saved by this one user and they manage all of the files but it's actually
really hard to maintain the changes of the file and you can accidentally replace one or the other file that you
need it also needs something known as RCS or revision control system which
keeps patch sets or the differences between updates in a special format on
your disk all of this your local Version Control Systems maintains all the track
of all the files within your local system as a user you can track specifically our versions later and it
basically works as an independent Standalone system for an application so
your applications like spreadsheets and word processors have this control mechanism
so that's one type apart from this you also have your centralized version control system which basically uses a
central server to store all the files and enables team collaboration so it's not one local system but multiple
systems which work on a single repository to which users can directly
access as a central server so the repository is a central server that
could be local or remote which is directly connected to each of the programmers workstation and the Third
Kind of VCS is a distributed version control system now these systems do not
necessarily rely on a central server to store all the versions of a project file in a distributed Version Control System
every contributor has a local copy or a clone of the main repository so everyone
maintains their own local repository which contains all the files and the metadata present in the main repository
and they can push or come commit or update the main repository if and when
required now we are going to cover more on the latter two Version Control
Systems further in the session so now let's try and understand in depth
what's the difference between a centralized version control system and a distributed version control system as I
had mentioned before in a centralized Version Control System each programmer working on a branch on a project has
absolute access to the central server so the centralized version control system uses a central server to store all the
files and everybody on the team can directly access the central server as it
is one single repository so if you have three workstations workstation one workstation 2 workstation 3 all of them
can directly commit to the repository and get updates from the repository
which is your central server the repository here indicates a server that
could be local or remote directly connected to each programmer's workstation every programmer can extract
or update their workstations with the data present in the repository or can
make changes or updates or commit in the repository and every operation is
performed directly on this repository even though this seems pretty convenient when we are just talking about three
people to maintain a single repository like this has some major drawbacks one
of them is that it's not locally available meaning every change that you have to make you will have to be
connected to a network to perform any action also in any case the central
server getting crashed or corrupted will result in losing the entire data of the
project also another small thing which is revertible but is if one person
accidentally makes some unwanted updates instead of his or her local system they
will be making that changes directly in the central server so recovering that
part back will be again a time consuming process this is where distributed Version Control Systems come to the
rescue now these systems do not necessarily rely on a central server to
store all the versions of a project file in the distributed Version Control System each and every contributor has a
local copy or a local workstation clone of the main repository that is everybody
maintains a local repository of their own which contains all the files and metadata present in the main repository
you will understand it better if you take a look at the diagram as you can see every programmer maintains a local
repository or a local copy on its own which is actually the copy or clone of
the central repository on their local hard drive they can commit and update
their local repository without any interference from one another and then they can update their local repositories
with new data from the central server by by an operation called pull once they
are done with their bit of the code they can affect the changes of the main repository by an operation called push
from their local Repository now this system or this act of cloning
an entire repository to your workstation and making all the changes on a local repository gives you a couple of
advantages over a centralized Version Control System first of all all the operations except push and pull are very
fast because the tool only needs access to the hard drive and not a remote
server hence you do not always need to be connected via a network connection or
you always do not need an internet connection committing new change sets can be done locally without manipulating
the data on the main repository once you have a group of change sets ready you
can push them all at once so you complete your set of code on your local system and then you push all the files
once and for all into the main Repository third since every contributor has a full
copy of the project repository they can share changes with one another on wanting to get some feedback before
affecting the changes in the main repository so this enhances the essence of collaboration amongst a development
team and finally if the server gets crashed at any point in time the loss data can be easily recovered from one of
the contributors local repository and that is why distributed version control system has been picking up popularity
over the past decade so why version controller is important first of all it
helps collaboration it allows remote development shared workspace and
real-time updates all versions of your code are preserved hence it helps you manage versions of
the same code easy rollback from current version suppose a part of your project
has been headed to a wrong direction you can easily roll back from the current
version to the last version which was stable now because you can reverse
faulty updates you can save time and hence it reduces downtime of your
project development and finally it improves visibility you can analyze and
compare different versions and that gives you like a 50 foot top view of your entire project which just
accelerates your product delivery so now that you have a bare sense of what is
Version Control and why do you need Version Control let's understand one of the most popular version control systems
that exist today and that is git so git is a distributed Version Control
tool that supports distributed non-linear workflows by providing data
Assurance from developing quality software so basically what it does is that it lets you and your team of
developers work together on the same project remotely from anywhere across the world your team members can work
efficiently on files and easily merge their changes into one source without
the fear of losing or deleting anything of importance due to the easy rollback
feature it is primarily used to manage a project comprising of a set of code or
text files that you may want to change from time to time now git is an integral
part of devops devops as most of you might have the idea of is the practice
of bringing agility to the process of development and operations a couple years ago it was this entirely
revolutionary ideology with swept the organizations worldwide boosting project
life cycles and in turn increasing profits devops promoted communication
between development engineers and operations participating together in the entire life cycle from design through
development process to production support now in the entire life cycle of devops starting from planning of the
projects to its deployment and monitoring git plays a vital role when it comes to managing the code that
collaborators contribute to the shared repository this code is extracted for
performing continuous integration to create a build and test it on the test server and eventually deploy it to
production tools like git enable communication between the development and operation in Steam when you are
developing a large project with a huge number of collaborators it is extremely important to have communication between
the collaborators between the developers while making changes in the project project comment messages in git play a
very important role in this particular collaboration or communication amongst the team the bits and pieces that we
deploy or update lies in this version control system such as git to succeed in
devops you need to have all the communication in Version Control hence git plays an extremely vital role
in succeeding at devops and due to this very reason git has earned way more
popularity compared to other version control tools available in the market such as Apache subversion concurrent
version systems and Mercurial if you compare the interest of git buy time with other Version Control Systems you
shall realize that larger companies products are generally developed by developers using git all around the
globe and some famous names out of them are Facebook Yahoo Zynga quora Twitter
eBay Salesforce Microsoft and many many more now lately all of Microsoft's new
development work has been in git features Microsoft migrating.net and
many of its open source projects on GitHub which are managed by git one of
such projects is the light GBM it's a fast distributed high performance gradient boosting framework based on the
decision tree algorithms which is used for ranking classification and many other machine learning tasks here git
plays an important role in managing this distributed version of light GBM by
providing speed and accuracy so basically to enable Version Control git
is your go to solution it's fast and suitable for handling massive code bases
scattered across multiple developers which makes it the most popular tool used today now that you know about git a
terminology we'll be using a lot while talking about git and GitHub is a repository now repository or repo as its
most commonly known as is a directory or a storage space where your projects can
basically live it can be local to a folder or your computer or it can be a
stored space in another online host such as your GitHub and in this particular space in this particular directory you
can keep your code files text files images you name it all of it inside a repository through the course of this we
shall also be talking about something known as GitHub which is nothing but a
central repository the kind that we spoke about when we discussed centralized versus distributed Version
Control Systems there's a central repository and that is where all of your code will live
you have your local repository where you'll make the changes and there you have the central repository to which you
will push all of the changes yeah and the central repository is something that all of the developers involved in this
particular project in a certain project have access to all right moving on let's discuss a few
git features which make it so popular amongst organizations as well as individuals first of all it's economical
it's free and open source git is released under GPS general public
licenses open source license so you don't need to purchase Git it is
absolutely free and since it's open source you can modify the source code as
per your requirement second is its speed now since you do not have to connect to
any network for performing all of the operations it completes all the tasks
really fast performance tests done by Mozilla showed it was an order of
magnitude faster than other Version Control Systems fetching version history
from locally sold repository can be 100 times faster than fetching it from the
remote server so the core part of git is written in C which avoids runtime
overheads associated with other high-level languages which makes it
extremely fast compared to other Version Control Systems next is that git
supports non-linear development it supports rapid branching and merging and
includes specific tools for visualization and navigation of a non-linear development history a core
assumption in git is that a change will be merged more often than it's written as it is passed on various reviewers
hence branches in git are very very lightweight and is only a reference to a
single Comet with a parental comets the full branch structure can be constructed
next let's talk about the robustness nearly every single task in git is
undoable git gives each developer a local copy of the entire development
history and the changes copied from one repository to another these changes are
imported as additional development branches and can be merged and deleted
and recovered the same way as a locally developed Branch next is the snapshots
which are recorded changes made to a file rather than the file itself
followed by which you have integrity which means no changes can be made without git recording it since every
contributor has their own local repository on the events of a system crash the loss data can be recovered
from any of the local repositories you will always have a backup of all of your files and none of the changes made to
the central repository will go unlocked by git git uses the sha1 or the secure
hash function to name and identify objects within its repository and every file every comment is check summed and
retrieved by its checksum at the time of the checkout the git history is stored
in such a way that the ID of a particular version a commit in gets terms depends upon the complete
development history leading up to that Comet once it's published it's not possible to change the old versions
without it being recorded then it's a distributed system which means every user has their own copy of their
repository and the data is stored locally git gives each developer a local
copy of the entire development history and the changes are copied from one such repository to another these changes are
imported as additional development branches and can be merged in the same way as a locally developed branch and
finally it's easy branching now Branch management with Git is very simple it
only takes a few seconds to create delete and merge branches feature branches provide an isolated environment
for each change to your code base so when a developer wants to start working
on something no matter how big or small they can create a fresh new branch which
is not interfering with the main branch which ensures that the master branch of the main branch always contains
production quality code so any little experimentation that you want to do or little things that you want to try out
can always be on a separate feature Branch instead of the changes made to
your master Branch because ultimately your master Branch should contain your publishable product so now that you know
the basic features of git let's try and understand what the basic workflow of
git is or how does git work so the basic overview of how git Works goes like this
you basically create a repository or a project with a git hosting tool like git
or bitbucket you copy or clone the repository to your own local machine or
your working directory you add a file to your local repository and commit the
changes which basically means you save the changes then you push the changes to
your master Branch then you make changes to your file and commit then you pull
the changes to your local machine you create a branch or a version make a
change commit the change same process open a pull request and merge your
branch to the master Branch now this is a little complicated listening to it like this but once we get on to our
demonstration I think things will be way more clear you can still take a good look at this diagram
but if you don't get it don't worry I will be explaining these operations one by one when we get to the demonstration
part of this git tutorial moving on let's get to the good part the Hands-On
section starting with the installation and setup of git now there are two ways in which
this goes first of all for Windows users you need to install git bash for Windows
and for Linux users you'll have to start by updating the package index and then
install git using the terminal so first of all for our Learners using
Windows you'll have to log on to get scm.com downloads and then click on downloads
for Windows button and then run the executable file and follow through till
you finally install git Bash I'm not going to install it completely
as I already have git bash downloaded but I'm still going to open the page to
show you guys all right so the latest Source release is get
2.30.1 on the 8th of Feb and you can just click on this button the moment you
click on this button it's going to start downloading it to you and once you have
the executable file all you have to do is open it or run it in your own system
right next for people using Linux this is an
example considering you are using Ubuntu if you're using Centos a lot of the
commands don't change but instead of Apt install you will be using yum install
for Centos so for that now I'm going to open my terminal on a virtual machine so
here I have running an Ubuntu machine using the oracle virtualbox
so I'm gonna start by running an update
all right this is going to take a while kindly be patient
so here I have my Ubuntu running on a virtual machine so to install git on your Ubuntu machine
you have to start by updating so this will take some time
if you're going to use sudo apt update if you're using Centos you will be using
yum instead of Apt let's scale that out and
then we're going to install git
and then you can look for the Git Version yeah all right now that we have our git
installed let's go ahead and look at the operations and commands that we are
going to take a look at today you have your repository set up which include the
commands init clone config and Alias then you have your save changes
operations which have your add commit diff and stash all of these commands we
are going to go on and off and look at collectively mostly then we are going to
look at the inspect operations your inspect repository operations of which
the git status command is something we are also going to look at throughout this demonstration right from the
beginning so apart from that you have your kit log git tag and get blame commands so you have your merge rebase
command so let's move on to our first set of get operations first of all you have your
git init command which basically creates a new empty yet repository now this can
be used to convert an existing inversion project to a kit repository or initiate
an absolute fresh git repository now most of the git commands are not available outside of the initialized
repository so this is usually the first command that you will run in a new
project that is if you're using git Bash so when I run git in it it
re-initializes an existing git repository which I already have here now
what this does is it re-initialized my existing git repository in this particular directory
in this dot get directory yeah what git init does is that it creates an empty
git repository or re-initializes an existing one like it did for me right
now it basically creates a DOT get directory with subdirectories and
template files and when you run the git init command in an existing repository
it will not overwrite the things that are already there it'll rather pick up
the newly added templates so that's the repository initialized you
can go ahead and create some files in the directory or repository whatever you call it now that is about initialization
now what about if there was an existing repository on git that you had to act
upon that you had to make updates to that you had to add files to things like that that's where you use the git clone
utility yeah now you have the git clone utility which you use to Target an
existing repository and create a clone or copy of this particular repository on
your local machine now this is one of the features provided to you by a distributed version control system as we
had discussed before so this is a blank file called edureka rep with just the
readme text file let me just go ahead and copy this
and then I can just get clone and paste its
https URL
and as you can see it's cloning this into a Eureka rep
and now if I change directory to edit a wrap
you can see the readme file is right there yeah
so we're basically going to work off of this particular repository today at
Eureka wrap yeah so now that we created a clone moving on we have in the git
config command which basically is a convenience function that is used to set up your git configuration values on your
Global or local project level this command is basically used to modify your
configuration text file so if I typed get config
globaluser.email and here by type
so there I given my email ID as the global user email ID so if I ask for
what the global user email ID is it will return my ID back to me yeah
basically these configuration levels like a username and your user ID correspond to your dot get config text
files now executing this basically will help you modify the configuration text
file git uses this series of configuration files to determine non-default behavior that you may want
yeah so the first place git looks for these values is in the system-wide
configuration file which contains settings that are applied to every user
on this system so basically if you pass the option to your git config it will
read and write from this file specifically and the next place the git looks is the config slash git config
file which is specific to each user now you can make git read or write to this
file by passing your Global option which is what I did and the final command in the setup
operations that we are going to look at is an alias now Alias as the name suggests is basically used to create
shorter commands that map to longer commands they enable more efficient
workflows when you obviously have to use lesser keystrokes to execute a command
there are these lengthy commands which obviously kill a lot of your time so
these aliases you can use to shorten those commands and these are also
something that you will create via the git config command there isn't really a
direct way or a direct command of creating aliases yeah this is just something I wanted to add in here along
with the git config command like a very common Alias created is co for checkout
or anything else EK for checkout so yeah this is something that you can do using
the git config command just creating aliases moving on let's look at
operations that you use to add changes and save changes to your git Repository
now the first two commands here that we're going to look at are git add and git commit now this combination is
basically used to create these snapshots that I was talking about initially in this session right so basically what you
do is first you use the git add command to add a change in your current
directory or your working directory and you add that to the staging area it
tells git that you want to include a particular set of updates or changes in
a particular file in the next comment then what you do is you move on to git
commit after making the changes to your working tree then you use the git commit
command which captures a snapshot of your Project's current state
so basically you use add to update the index using the current content found in
your working tree and then you basically prepare the content in the staging area for the next commit then you run the
commit command to take that snapshot of your Project's current status now
remember this is very important and you have to do it each time you must use the add command to add any new modified
files to the index you can't directly commit anything without adding it to the stage area
first now what I did was I quickly went ahead and added three files to my repository
at Eureka rep so if I go into this repository you can see
there are three files at Eureka file one file two file three now let's see if these files are in my index or not using
the command get status now git status basically displays the state of the
working directory and the staging area it will list all the modified files
which are ready to be added to the local repository it lets you see which changes
have been staged and which haven't which files are being tracked by get and not
so if I typed get status
you can see on your main branch you have three untracked files which are
the three files that I just added and you break a file one two and three these are not added to the index yet
this means I cannot commit these changes unless I added them explicitly to the
index so now we're going to add these three files this command will update the index
using the current content which is found in your working tree as you can see in front of you and then it'll prepare the
content in the staging area for the next comment so either I can just get add and put in
the directory name or I can just use git add with an extension of a which means
all and this will add all of these files to the index which are in the directory but not updated in the index yet yeah
now I've done that let me go ahead and use git status again and you can see all
of these file names are written in green and you can see changes to be committed
written here which means all of these changes are ready to be committed they are added to the index now these files
are added to the index you are ready to commit them now as I had mentioned before we're
going to commit them and committing refers to recording the snapshots of the repository at any given time so your
committed snapshots will never change unless it's done explicitly so I'm going
to clear this and we're going to come at this by typing git Comet and then adding
a tag or a message called committing
three files let's try this out as you can see three
files change three insertions you can see our eduraco files one two three have
been committed the git comment command has committed the changes in the three files in your local repository now if
you want to commit a snapshot of all of the changes in your working directory at once you can always go with commit
hyphen a now let me add a couple more files to my directory just to show the
difference between your files added to the index versus the ones that are not
if I type LS again you can see there are two more files over here
as you can see I have two other files
I'm just going to add the edureka file 4 and now the file 5
and then I'm just gonna get commit commenting files
and as you can see only one file changed only one insertion happened and that
only happened because edureka file 5 is not added to the index
and even if I use the extension a I cannot add edureka file 5 because that
has not been added to your index now if I just go ahead and add iduraca
file 5 voila
the change has occurred in our git Repository now if I typed git status
there are no more files to add or commit our branch is ahead of the origin or
main branch by three commits and you can use git push to publish your local commits now what is get push this is
something we will cover later in this session Now understand that before you affect the changes made to the central
repository you should always pull changes from the central repository to your local repository just to get the
most updated version of your branch you'll get all the work of your collaborators that have been
contributing in the central repository for that you use the pull command
now as you can see there's nothing to commit and you're working very scheme there are other commands like the git
status command which you could use for similar purposes for example you have
the git log command which only operates on committed history it basically will display to you the committed snapshots
and will let you list the project history filtrate and search for specific changes so if you're typed git log it's
going to show you all the comments that have happened since we started with this
git file okay there you can see at the bottom your original main there's your
initial commit right then our first comment where we committed three files
then we committed the fourth file and finally when we committed file number
five right these are all the comments including the initial commit the first
one yeah along with the author and the date and time
remember when I told you about everything is recorded when you're using git this is what I meant you will can
always access who made comments at what time and what comments were made when
you use a Version Control System such as kit now at a point like this I think it's crucial to mention uh one more
command which is the git tag and it's used to basically capture a tag in
history or a point in history that is used for a marked version release right
so it's basically uh like a branch that doesn't change and after being created
they have no further history of comments yeah so that was one and another one is git
blame now the git blame function displays of the author's metadata
attached to a specific committed line in a file it's used to examine specific points in a file's history and get
context as to who the last author was that modified that line so if I went
back and typed get blame [Music] a Eureka file 5 dot txt
it's going to show who the person is that is to blame who committed this
particular file or made this particular change right it'll also if you paid
attention give you the commit number as you can see this E9
d63ef0 comment number this also has this E9
d63ef0 which is the first few characters of this particular comment number yeah
so now that you know how to check the status of the changes and see who made
the changes it's time that you understand how you fetch changes from
your report repository to a local repository and how you push comments
from your local repository to your remote repository so here we're going to look at two commands called pull and
push the first of all you have the git pull command which fetches changes from your
remote repository to a local repository basically it merges Upstream changes in your local repository which is a common
task in git based collaborations but at first you need to set your
central repository as the origin using the command git remote add origin and I'm going to
add the link to my central repository which is this
and as it says the remote origin already exists now that your origin is set and now
we're going to pull our main branch so I'm just gonna type git pull origin
Main and it says we are already up to date now certain number of tutorials or
documents that you might be following might even give you the command git pull origin master in which case you're
working on the master Branch now to know which branch you're at you can just use
a command git branch Levy yeah I'll tell you which branch are
you working on yeah so yeah we are working on the main branch and we are already up to date
with the branch since my local repository was already updated with files from the main branch this is the
message of an up-to-date branch now you can also like use git pull origin
and your whatever Branch name here I'm using Main that's a mistake on my part maybe I was
trying to clear the screen all right so here even if I pull from the branch
using git pull origin and the branch name to my local get repository you can
see it's already up to date so now my local git repository is now all updated with the recent changes
so now it's time to make the changes to your central repository by using the push command
so we're going to use the push command this command transfers commits from your local repository to your remote
repository and it is basically the opposite of your poll operations pooling
will import Comics to your local repositories whereas pushing will export commits to your remote repositories
it's going to ask you for your username
and password before it lets you push into your main branch there so I've put
in the username and password for my GitHub account and with that this has
pushed the changes from my local repository to the remote repository along with all the necessary commits and
internal objects now this has created a local branch in your destination repository the use of git push is to
publish your local changes to a central repository after you've accumulated several local comments and are ready to
share them with the rest of the team you can then push them to your central repository by using the get push command
now the files which have already been committed previously in the comment section and they were all push ready
you will use git push origin Master to reflect these files in the master branch
of your Repository all right so let's go ahead to our repository and
see if changes have been made and yes you
can see all of our files on our GitHub repository here when you break our file one two three four and five all the five
files that we had created on our local repository yeah you have all these five
files and the readme file here here also you have the readme file and all the
five files that we had created now at this point in this git tutorial I
hope you have understood the basic commands of git you have pulling pushing
adding committing things like that now let's take a step further and understand the three basic commands of parallel
development here I'm going to be talking about branching merging and rebasing so
first of all let's talk about merging now merging is a way to combine the work
of different branches together this will basically allow you to Branch off develop a new feature and then combine
it back in as you can see in the diagram it shows you two basically different
branches new branch and master now when we merge the work of the new Branch into
the master it creates a new Comet which contains all the work of the master and
New Branch now if we merge the two branches with the merge command
as I had mentioned before git allows you for easy branching which means you have parallel branches with your master
branch that help you carry on your individual work while not interrupting your main product
which is your master Branch or your main branch so branches and git are nothing but pointers to specific comments yeah
so git generally prefers to keep its branches as lightweight as possible now
there are basically two kinds of branches we are local and remote tracking branches a local branch is just
not the path of your working tree on the other hand the remote tracking branches
have special purposes some of them are linking your work from your local repository to your central repository
and automatically detecting which remote branches to get changes from when you do
a git pull so you can check what your local branch is by using get
branch so you know you are at your main branch and the one thing you should understand
one thing is like a main Mantra of people who use git are Branch early and
Branch often yes so to create a new Branch you're going to use a command get
branch and your branch name and this case I'm just going to use a branch name
at Eureka images all right and here you can see this diagram shows the workflow
of a new branch that is created when we create a new Branch it originates from
the master Branch itself so since there is no storage or memory overhead with
making many branches it's usually easier to logically divide your work up than
having big chunky branches hence the branch often part now let's see how to commit using
branches now branching includes the work of a particular commit along with all
parent comments right so as you see the new branch has detached itself from the
master or the main branch and hence it'll create a different path so we are going to
get check out the branch name which in our case is edureka images
and then we are going to git comment so you have untracked files and Eureka
images and nothing added
so you made that branch now merging is a way to combine the work of
different branches together so basically what this does is now that we've branched off to something known as
edureka images you can go ahead add files to it develop a new feature and then combine it back into your main
branch here you can see two different branches you have your new branch and your main branch and when we merge the work of a
new Branch into the main branch it creates a new comment which contains all the work of the main branch and the new
Branch so now if we merge this Branch as you can see it's already
up to date it's important to know that the branch name here should be the
branch that you want to merge into the branch that you are currently checking out so make sure that you are checked
out of this destination Branch so let's just merge all the work of the branch at
Eureka images into your main branch for that I will first check out my main
branch and then merge edureka images with kit
merge edureka images and as you can see all the data from the
branch at Eureka images has merged into the main branch
now notice that merging in git creates a special commit that has two unique parents now this is one way of merging
into the main branch there's also another way of combining the work between two branches which is called
rebasing now rebasing takes a set of comets copies them and stores them
outside your repository the advantage of rebasing is that it can also be used to
make linear sequence comets the commit log or the history of the repository stays clean if rebasing is done now our
work from the new branch is placed right after master and we have this nice
linear sequence of comments this command doesn't copy but moves all our work from
the current Branch to the master or the main branch they look like they've been developed sequentially instead of having
developed parallelly
so if I just did another branch called git Branch at Eureka
images too and then maybe get checkout
okay then we get Commit This and then I could just get rebase min and
your current Branch will be up to date this command basically moves all of your work from your current Branch to your
master branch and it'll basically look like it has been developed sequentially instead of having developed parallelly
now apart from these there are certain operations that you ought to know
firstly archiving your repository you can use this command it stores all of
your files and data in a zip file rather than a DOT get directory this creates
only a single snapshot omitting Version Control completely though this comes in handy when you want to send these files
to a client for review who doesn't have git installed in their computers apart from this you can also bundle your
repository which basically turns your entire repository into a single file this pushes the master Branch or the
main branch to a remote Branch only contained in a file instead of a repository you could alternately change
the repository clone the repo bundle create a copy and use git log but that
is like five commands instead of one why not just use one command and finally
stashing uncommitted changes when you want to undo adding a feature or any
kind of added data temporarily we can always stash them temporarily you can
use git status stash status and then you want to reapply the changes you stashed
using git stash apply having said that let's move on to another very important
topic in this git tutorial let's go through a brief introduction to SSH now
what is ssh what is an SSH key now an SSH key is an access credential for
secure shell Network protocol it's basically an authenticated and encrypted
secure network protocol and is used for remote Communications between machines on an unsecured open network
for remote file transfer Network management and remote operating system access
so basically it's typically used to log into a remote machine and execute commands in it but this also supports
tunneling forwarding TCP ports and X11 conditions it can transfer files using
Associated SSH file transfer or secure copy protocols it provides you several alternative
options for strong authentication and protects the communication security and integrity with very strong encryption
it's a secure alternative to the non-protected login protocols and
insecure file transfer methods such as FTP so basically how do you create SSH
Keys you start by using the command SSH hyphen Keygen
and then you'll be prompted to enter a file in which to save the key
so home at Eureka then we're going to enter
a pass phrase I'm just going to leave it empty again and you can see our identification has
been saved in the file that we specified and we have something known as a key
fingerprint and finally what we have to do is add the new SSH key to the SSH agent
and then you will see identity added and that's it you have created an SSH
key [Music]
the need for GitHub it is extremely important for software developers to work on a web-based
platform to share their projects and collaborate with other developers this platform must be a version control
system that is it must enable multiple people to simultaneously work on a
single project each person edits his or her own copy of the files and chooses
when to share those changes with the rest of the team this application must also be capable of Hosting millions of
programmers and hobbyists that download and evaluate each other's work GitHub is
one such platform of choice for developers that can host multiple programmers and review their code
GitHub has several competitors for instance git lab gitlab is an open
source web interface and Source control platform based on git whereas Microsoft
team Foundation server is an Enterprise grade server for teams to share code track work and ship software for any
language all in a single package bitbucket on the other hand stores all
of your git and Mercurial source code in one place with unlimited private repositories
so what really makes GitHub so powerful and popular among Developers GitHub is an open source platform and
the community is really what fuels it moreover GitHub is the platform of
choice for developers from various large corporations too Microsoft is the number
one contributor to the system but there are also Google sap Airbnb IBM PayPal
and many others exposure and insight that you can get on GitHub are simply
unmatched by any other platform here you can discover code written by others learn from it and even use it for your
own projects versions control on GitHub works very much like Microsoft Office or Google
drive it simply tracks all the changes made to your code and who makes them you
can always review the detailed change log that neatly hosts all of the relevant information
using GitHub eliminates the need for complex corporate security solution because everything is on cloud the
platforms protects code branches verifies commit signing and controls access
now that we know why we need GitHub let us understand what is GitHub GitHub is a
git repository hosting service that provides a web-based graphical interface with many features a repository is
usually used to organize a single project repositories can contain folders files images videos spreadsheets
anything your project needs let's say for example a team wants to work on a
particular project here they can simultaneously write and update the code to a central repository which is present
on GitHub so GitHub is a highly used software that is typically used for
Version Control it is helpful when more than just see one person is working on a
project for example a software development team wants to build a website and everyone has to update their
codes simultaneously while working on this project in this case GitHub helps
them to build a centralized repository where everyone can upload edit and manage the code files
most software projects have a bug tracker of some kind github's tracker is
called issues and has its very own section in every repository issues
basically are a great way to keep track of tasks enhancements and bugs for your project
moving on people often get confused between the terms get and GitHub now let
me clearly explain the difference between them git is simply a version control system
that lets you manage and track changes within your project whereas GitHub is a
cloud-based service that lets you manage git repositories so basically git is the tool and GitHub is the service
now that we know the difference between git and GitHub let us move on and understand how these two work hand in
hand we already know that git is a Version Control tool that will allow you to
perform all kinds of operations to fetch data from the central server or push data to it whereas GitHub is a code
hosting platform for Version Control collaboration GitHub is basically a company that
allows you to host a central repository in a remote server now without any
further Ado let's get started with the demonstration on how to use GitHub so
for this demonstration we're working on the website version of GitHub there's another version of GitHub that is the
desktop version which you can download it to your personal computer so we're simply going to search for
GitHub in our search engine the first link will lead you to the official website of GitHub so I'm going
to click on that so this will redirect me to the main homepage of GitHub as you can see there
is a search GitHub option there are also two buttons that says sign in and sign up if you're new to GitHub you can
simply enter in your credentials that is a username email password and sign up for GitHub but if you already have an
account like I do I'm simply going to click on the sign in button and it'll redirect me to a page where I have to
enter the credentials that is my email address and password I'm going to do that now and I'm going to click on the
sign in button now this is the main page of my account as you can see I have no
repositories it's all new it's all fresh but if you're not new to GitHub you can view all of your repositories on the
left hand corner now before we move on I'm going to explain you all the
features that are present within GitHub so you can see a search bar here so the
search bar will allow you to look for profiles certain keywords look for different kinds of projects that are available on GitHub all of those can be
done using this bar here and you can see four options next to the bar that says pull request issues Marketplace explore
pull requests will learn later on in this session but the issues in Marketplace we won't be discussing in
this video for now the explore button on the other hand is an extremely important and interesting button so once I click
on that it'll redirect me to a page with some activities that are going on around
in GitHub you can see here the trending repositories they're tendering developers basically this is a fee that
will allow you to interact with developers and other people collaborators from all around the world
basically in Instagram too you have an explore button which will allow you to interact with different people from all
around the world so the same concept is implied in the GitHub explore button too so you can explore top effects you can
explore tending repositories developers basically it's an interaction with other people from different parts of the world
so I hope that's clear now the most important part of the session are the
three buttons that are available in the right hand corner of the navigation bar so you can see there's a bell icon
there's a plus icon there's a pixelated icon on the right hand corner so the Bell icon allows you to read
notifications of your activities that occur in GitHub so that's what it really is you can see the inbox will allow you
to view all of the notifications you can also view the Android notifications by clicking on this unread button as of now
I don't have any notifications so there's nothing available you can also group these notifications by the date or
repository by clicking on this group by button here you can also view your save notifications by clicking on here and
the done button on the other hand will let you mark all of your notifications that you're done with your previous
notifications so these are the three important buttons you have to know in
this Bell icon and the filters are not necessary as of now so I'm not going to discuss that this button on the other
hand will allow you to manage your notification settings and your subscriptions too so that's all for this
Bell icon the next important button is this plus icon as you can see there are five drop down options that appear here
the first one being new repository followed by import repository new just new organization new project so new
repository we've already discussed previously in this session a repository is a place where you create your files
for your project it's basically a storage space right so you repository can directly interact with your git
right so the new repository option will allow you to make files a repository to
your GitHub account right the git on the other hand the tool that which we use to make local repositories in our personal
computer can be directly pushed on the local repositories can directly be pushed onto your GitHub account so
that's what the new repository pattern allows you to do so but the new project
on the other hand is a place to track issues features and other tasks that are
related to the code within the repository you can also connect with the devops build and deploy process assign
people to tasks and so on by using this button that is the new project button so
the difference between the new repository button and the new project button is that projects in GitHub are
only a part of GitHub but not git but the new repository option is a part of
GitHub and git so that's the main difference between new repository and new project I hope that's clear so the
next button will drop me down some interactions that I can make with my
profile so if I click on this your profile option it will redirect me to a
page where I can edit my profile I can really create my identity using this
page so here if I click the edit profile I can add a bio about myself I can add the company in which I'm working in the
location at where I am the website Twitter username Etc all of that I can add here all of the information about
myself I can also view the repositories I'm working on currently or the repositories I've worked in the past
projects that I'm working on the packages and the entire contributions I've been making on GitHub from the last
year so basically it allows me to build an identity or it'll help me build my
profile on GitHub so I hope that's clear now if I click on this button and if I
want to sign out from a profile I can simply scroll down and click on the sign out button here and this will sign me
out of my account so that's all for getting started with GitHub these are the basics on what GitHub is and what
each of the button and options really do now if I want to move back to the main page of my GitHub profile I can simply
click on this octocad that's github's logo so I'm just going to click on this octocat logo and here I'm back to my
main page now before we move on and work on the different operations and options within
GitHub and learn different things about GitHub I'm going to give you a brief
overview on how to download the desktop version of GitHub so I'm simply going to
search for GitHub desktop on my search engine and I'm going to click on the first link that's available on this page
now I can simply click on this button that says download for Windows 64-bit that's compatible to my current version
of my personal computer if you have a Mac you can simply click on the Mac version and download it to your desktop
but as I've already mentioned previously that we're going to work on the website version so I'm going to Simply switch
back to this now let's quickly move on to the next part of the session create a repository so firstly let us
understand what a repository is it is simply a storage space for the correct
project that you're working on GitHub is a very popular Central repository that allows you to share your files whereas
git allows you to create local repositories that are present on the system you are working on
so you can basically push your local repository into GitHub and share it with other collaborators via the central one
now that we know what a repository is and how it works let's go on to the demonstration part and create a first
Repository so you can do this in two ways either you can click on your create repository
button that is present on the left side or you can as I've already mentioned in the previous part of the session you can
click on to this plus icon and you can click on the new repository option so this will redirect you to a page that
says create a new repository you can add your repository name I'm going to name my repository as edureka and it's
available all of your repository names must be unique from one another to identify them easily you can also add a
description which is optional I'm just going to add the description this is my
first Repository and a description allows people or other collaborators to understand what your
repository is all about but as a good developer or a good programmer you would definitely want to add a description and
give an overview of what your repository is all about there are two options now available that says private or public
now you can choose your repository to either be public or private so the
private One lets you decide who can access your profile whereas the public One lets anyone View and access your
repository but you can choose who can commit to it that's the difference between public and private repository
I'm going to let my repository be public as of now now if you scroll down you can
see that you can initialize your repository with three options the first one being add a readme file the second
one being add a git ignore file you can always choose a add a readme text file
to your project which often contains information about the project and other necessary details the user must be aware
of when he or she is accessing that particular project now I want a readme file for my repository so I'm going to
click on this button here that's going to check it the next option is add a DOT
get ignore file so this file will let you ignore a list of files when the user is pushing files to GitHub that's what
this option really does but I'm going to let this be unchecked for now for your repository to truly be open source you
will need to license it so others are free to use change and distribute the software you can simply click on choose
a license option and pick your the required license for your project there are several licenses like MIT GPL
Apache License 2.0 BSD Etc but for this repository we don't really need a
license so I'm going to untick this too and now you can see there's a piece of information that says this will set
master as the default Branch but I'm going to ignore this for now I'm going to explain about branches later on in
the session so this is all you have to do to create your first new repository you add a name you choose a description
you add an optional description you let your repository be either public or private and you initialize a repository
with either of these three options and I'm simply going to click on my create robustry option now this will redirect
me to page with all the information and the files that are currently present in my repository you can see here my
repository name is present here with the optional description that I gave and the
number of files currently we have only just one file that's the readme text file and that's present here
so this is all we have congratulations you just created your first repository now you can see there are some options
that says issues pull request actions projects Wiki security Etc we don't have
to really talk about all of these right now we will just learn about one option that says code here so this is really
important if you click on this button you can see that there's a link that is available here and https link so if you
copy this link and paste it on your git terminal that's present on your computer you can download this entire project
directly to your local system so that's what the link is for I hope that's clear and the next option that says open with
GitHub desktop will allow you to open this entire repository in your GitHub desktop version and you can also the
last option that says download zip will allow you to download this entire repository in the form of zip files so
all of your project files will be within that zip file so that's all you have to really know about your repository and
and I'm going to click on the readme text file that's available it will take me to another page with
some extra information about that file you can see currently we have two lines and the memory space that is allocated
to this file so we currently have two lines that is edu Recon this is my first repository and you can also see the
number of contributors to this project that is just one that's just me for now and you can view the history of the
commits or the changes that have been performed in your file so we'll come back to that part later on this session
but you can move back to your main page of this repository by clicking on the name button here so edureka is the name
of my repository so I'm going to click on that so now I'm back to the main page of my Repository
before we learn how to create our first Branch let us understand what branches are branches allow you to work on other
features that can be included and merged with the master Branch if required so what is the master Branch the master
branch is the main branch where your project resides on so all of the changes all of the activities that you do with
your main project lies or is on your default branch that is named as the master Branch so what really GitHub
allows you to do is it allows you to create additional branches so on these additional branches you can work on the
other features or you can experiment with your project and if you're happy with this you can simply merge these
features to your main branch that is your master Branch this is what branches are really for so they simply allow you
to work on other features which is R so let's move on to the demonstration part and look at how we can create our own
branches so now if you look on the left corner you can see a button that says Master So currently we're on the master
branch and there's only one branch and the master branches of originally mentioned is the default Branch so when
you create a repository you're automatically creating a master Branch so this is where your project will be
residing on and now if you want to create another Branch say let's name this Branch one branch
so this is what I want to name my additional Branch I'm simply going to name it and I'm going to click on the
enter button so it will redirect me to a page so this is the exact replica of
your master branch and you can work on this Branch you can work on any other feature or you can add something you can
remove something you can really experiment on this branch and if you're happy with this you can merge back this
feature or the experimentation that you've been working on to your master Branch right so you can see this readme
text file it's exactly the same there's the name of your repository the description of the repository you can
click on the readme text file and everything's the absolute same you can quickly switch back to the main page but
the only difference is that you're currently on a branch named Branch one branch you're not on your master Branch
now if you want to switch back to your master branch and work on it you can click on this button and you'll find the
master here you can click on that and it will take you back to your master Branch here you can work on your project so the
currently two branches you can see that and everything's normal everything looks simple that's all for branches it's
really easy I hope it's clear so you can look for branches here on this bar here
that's present here you can also create new ones in the same option so that's
all for branches let's move on to the next part of the session make a comment now what are comets
comments simply record changes to one or more files in your branches so basically
they save the changes that you're making in your project git always assigns each comment a unique identification which is
called sha or a hash that identifies the specific changes so for any changes are
made to your project files you can simply go back and look at the version history or the history of the each
commit you've performed on your project files so that's what really commits are all about it's extremely easy let's go
ahead and make our first comment now I'm going to switch to my Branch one branch
and I'm going to make my first commit I'm going to click on my readme text file that's the only file currently in
our repository so we'll make the change in the readme text file there are three really important icons that are present
in the right corner as you can see the first one is a PC icon that says open this file in GitHub desktop so if you
click on this file this entire file will open in your GitHub desktop version the next one that is the pencil icon will
allow me to edit this particular file that is my readme text file and the third icon is a bin icon which will
allow me to delete this file now what we'll be working on is the pencil icon that's the edit this file option I'm
going to click on this and I can simply view a space or a file that will allow
me to make changes to my readme text file I'm going to add another line here
that says this is my first comment this is what I want to add to my readme text file and if I want to preview the
changes I'm going to click on this preview changes button you can see that this is my first repository this is my first Commit This is my first comment is
the additional piece of information that we'll be adding to my readme text file and it's highlighted in blue so we know
that that's the additional information I'm happy with this change I'm going to switch back to my edit file I'm going to
scroll down and if I want to add a description about the change that I'm
performing to my file I can do that a good programmer would always add a
description to the change that he's making to the project file so other collaborators will view the commit or
they view the change they can read the extended description and understand what the change is about so that's a good
habit that you must follow but as of now we're not going to do that so I'm going to leave this blank as you can see there
are two radio buttons that are currently available the first one says commit directly to the branch one branch this
will allow me to make the change or save or make the commit directly to my Branch
one branch only so the change that I'm making currently is only implemented to my Branch one branch the second option
allows me to create a new Branch for this particular commit and start a pull request we're not going to talk much
about this option right now but the first option is extremely important so we let this be stuck onto the option
that says commit directly to the branch one branch and I'm going to click on the commit changes options and this will
simply implement the entire change to the file you can see that the change is implemented the additional piece of line
that says this is my first comment is added to my readme text file now the
interesting part is if I switch to my master Branch the change is not
implemented in my master Branch so the change is only currently present in a branch one branch and now if I want to
view the history of the changes that I've made I've already mentioned in the previous part of the session that the
history button will allow me to do so so I'm going to click on this history button and you can see that I made my
first comment 23 minutes ago and I made my new comment 41 seconds ago and there
is also a hash number unique hash identification number that allows me to distinguish between both of these
changes so all of my comments that I'll be making on this Branch will be available here so that's the main point
of a version control system isn't it understanding and keeping a record of all the changes that we're performing in
our files and our projects so this is gives full Justice to the word Version Control so that is what GitHub is all
about now that we learned how to make our first comment two let's move on to the next part of the session
open and merge pull requests so what are pull requests pull requests let you tell
other developers about changes you've pushed to branch in a repository on GitHub so once a pull request is open
you can acknowledge and review the changes with collaborators and add follow-up commits after which your
changes are merged into the base Branch so there are two ways to create a pull request the first one being pulled
requests from a forked repository and the second one being pulled request from a branch within a repository currently
in this demo we will work on the second one that is pull request from a branch within a repository now I'm simply going
to switch to My Demo part okay now currently I'm on my master Branch I'm going to click on this pull request
option that's here now it says Branch one has had recent pushes three minutes
ago compare and pull requests I'm not going to click on that I'm going to Simply click on the new pull request
option here so this will allow me to compare the changes there's a base branch and a compare Branch the based
launch is the master branch and the compare Branch I will compare my master Branch to my Branch one branch so this
notification says that the merge between the branch one branch and master branch is definitely possible so it's a green
signal so if I scroll down I can view the difference between both the branches so the left hand side indicates the
information that is present on the master branch and the right hand indicates the information that is
present in my Branch one branch plus sign indicates the additional information that is present in my Branch
one branch I'm happy with this so I'm simply going to scroll up and create the pull request and click on that I can
also leave a comment and I can preview the change that's not necessary for now so I'm going to go ahead and create the
pull request now this will redirect me to a page so this page says that the opal request has
been opened and now I can choose to merge this pull request that is I can
merge the branch one branch to my master Branch so it says this branch has no
conflicts with the base Branch merging can be performed automatically and that's good news right so I'm going to
click on merge pull request and I'm going to confirm my merge update my readme text file I'm happy with that so
I'm just going to confirm it now it says pull request successfully merged and
closed you're all set the branch one branch can be safely deleted I'm not going to delete the branch I'm going to
compare both of the branches and see if my master branch is exactly the same as
my Branch one branch so I'm going to click on this edureka I'm going to go to my main page of my repository so my
master branch has the additional piece of information that says this is my first comment now if I switch to my
Branch one branch it has the exact piece of information so the information that was present in
my Branch one branch has been successfully implemented to my master Branch so that's all for the pull
request part two we've reached the end of the demonstration part now let's quickly look at the case study of how
Microsoft implemented GitHub I'm pretty sure most of you have heard of Microsoft Microsoft cooperation is an American
multinational technology company it develops manufactures licenses supports
and sells different computer software consumer electronics personal computers and other related services
so initially Microsoft was against the use of the open source because they held very tightly to the Internet Protocol
they were completely hesitant to adapt to this new concept of sharing code to the entire world but in 2010 they
rethought this entire scenario and now Microsoft is one of the biggest contributors to open source today about
2000 to 25 000 Microsoft Engineers maintain typescript.net Windows terminal
dot Helm and more than a thousand other open source projects so first what they
did was they released new processes in measured containment but later on they released only license software so here
developers can learn from the company's source code but they couldn't really build on it eventually the stigma died
and now even close code like dotnet is open source under an MIT license teams
realize that they need to accept contributions to get feedback and learn from other Developers
to organize and understand this approach Microsoft created their open source programs office which enables
distribution and centralization of knowledge so the ospo provides the
resources and maintainers to manage thousands of repositories and contributors effectively on GitHub even
though Microsoft invests in its tools they expect other individuals and organizations to lead the way Microsoft
believes that github's value isn't in any one feature but its entire Community
GitHub is the place to collaborate it's where everyone is and where most of the
entire world's open source is already happening it's not just a feature but the whole thing
[Music] let's first talk about why we need
continuous integration let's talk about a scenario where we are not using continuous integration now when we work
in a development team there are multiple developers now all the developers work on different different components for a
project or for a code a developer a may be writing code thinking scenario let's say python 1.0 and developer B will be
writing code thinking of a scenario 1.2 1.3 and so on one of the biggest challenges that we face in an industry
is how to make sure that all the developers codes get collaborated and that we are having properly release
getting deployed that is the biggest challenge in the industry now another challenge that most developers fail is
debugging now whenever you write a code there are a lot of bugs and issues now the code is often written thousands and
thousands of lines now if you don't have any proper continuous integration mechanism or a packaging mechanism
you'll see it takes days even weeks and months to debug your code to make it 100 bug free now that is a challenge that
really makes the development process very slow so how to test the code now testing used to be a monotonous task now
their team used to spend months of a life cycle to make the code complete the test code and make sure it's bug free
now what used to happen is guys two things one is you are not able to deliver on time and secondly business
never see value coming out of the ID if businesses see it not adding value ID
is not delivering product on time they will start rethinking and giving a second thought why do we need to invest
so much money in the ID industry or in the IIT software these things were hampering big time in the development
and release of modern day deployment software Cycles in the modern day world we want to make sure our deployment
Cycles are robust we're able to get the main need of the market and deploy the product as soon as possible but with
this approach we're not using the CI tools it used to take six months to develop a product deploy a product and
make it bug free now things change drastically when we start using continuous integration Now using
continuous integration we can deploy codes at the quickest Amazon for instance deploys code every 11.6 seconds
now how is that possible if you ask it's because they are using the continuous integration mechanism using CI tools
like Jenkins where we can automate the process like building packaging deployment testing and so on so that we
can do all these things with the click of a button in a matter of seconds CIA process helps us to deploy multiple
releases parallely now in one day we can do thousands and thousands of release without impacting our environment and
making sure these release are in line with our requirement the CI tool has an awesome notification mechanism where it
notifies instantly of any success or failures happening so that if I have to debug the code or troubleshoot it I will
know exactly at the Pinpoint location where it is failing you can do multiple notification management mechanism like
an email an SMS any itsm tools now all those modern ways of notification are
possible using the CI tools like Jenkins now we integrate these CI tools like
Jenkins with modern day Bill and packaging tools like Maven and Gradle so that we can automate the common problem
which developers spend most of their time on now developers spend half of their life cycle like packaging their
code code review code deployment unit testing now all those things can be automated and can be done with the click
of a button using these CI tools now what is continuous integration now we
have talked about how it helps save time how would help to save the energy and how it helps to make release more
frequent and it help us to streamline the process now what actually is continuous integration now if I go
traditionally continuous integration is basically you can see a processor have us to combine or make the smooth in the
automation of tasks like compiling the code doing the testing and deploying the code into our live Cycles so how does
this conducive integration work so developers who works on projects generally commit their code in any
version tools like GitHub gitlab getbucket SVN Etc so using continuous
integration we pull the code from the version control system like git for example now we automate the process like
code compiling code testing code review and then finally deploy the code in the lowest lifecycle environment
you can say continuous integration is sort of a development practice where developers pull the code from the
version control system and make the code deploy in the lower life cycle environment now the beauty of using the
CI tool is we can automate the process in such a way that whenever a developers make a new comment or a new change in
the version control system tools we can create a store of a pipeline and the pipeline automatically picks the code
from the VCS tools and compile it and then deploy the software another good thing about it is it does
not require any human intervention so once I have set up a pipeline then I don't need to worry about running the
code again and again or making any changes it automatically happens in a fully automated process as I mentioned a
while back now we can use the process whenever a new comment happens or you can run it on a scheduled basis or
something now Ci or continuous integration was first time proposed in 1991 as part of the extreme program
concept at that time it was not adopted so well but in the modern day industry it has become the bread and butter of smoothing
Up release or for our development process in a modern day world continuous integration can be said as a practice of
merging all developers working copy to a mainstream where you can develop and deploy the code multiple times in the
same day with as many changes as possible now there is no limit of how many lines of code it can be tens and
thousands of lines of code which can be compiled multiple times throughout the iteration let's see a case study where
we try to see how things change for an organization after it adopted to the Ci or continuous integration mode
here we have picked up an example of a dope to see how it helped in the development process Adobe relies at some
point of the time that they have not released very frequently when this happened the market presentation was
down as they were not following the country's integration approach now they were using the native method of
compiling the code and debugging the code testing the code because of which they were noticing that it used to take
up to 6 months release updates now six months was a long duration whereas their
competitors were releasing very frequently which was helping them to capture the market share Adobe decided
some point at a time that they need to change the software development and deployment strategies and they try to
adopt the country's integration mechanism once they adopted The Continuous integration mechanism they
found that the response time was a lot better and a lot faster now they were able to deploy the updates more
frequently with every times a change was made by a developer or new release was being released and it helped them reduce
the response times and they were 60 more faster the development teams were getting more effective and they were
able to really deploy losses far more in a better and structured way than what they were doing without using CI you can
see how CI helps industry Amazon is the best case scenario Amazon in modern day
scenario deploy codes every 11.6 seconds almost 5 times every minutes now this is
all possible using the continuous integration now what is Jenkins now we've talked
about what is CI we've talked about why we need CI now let's talk about what is Jenkins and how does Jenkins fit in this
particular picture now Jenkins is an open source automation server which helps us to automate the
process of development relating to testing deployment packaging and others it is a server-based systems that run on
software Apache Tomcat now it support VCS store like git big bucket Etc it
also support build automation tools like Apache Maven and it helps to make and facilitate our continuous integration
and continuous delivery process now Jenkins was first released in the year 2011 and its completely open source
software as a part of the MIT license Jenkins is a plug-in based profile in
Jenkins you have got plugin which basically used to interact with different different tools and components
in Jenkins plugin are primary release in language other than Java plugins are
available to integrate Jenkins with most version control system tools and most databases using these plugins we set up
purposes for example unit testing now you can do compiling you can do packaging now you can create some
reports now you can do some lodging now all these things are possible using plugins and Jenkins
another important component that we've talked about in Jenkins apart from plugins is Jenkins notification now
which we call it as mailer using mailer guys we can configure email notification
for instance you can configure scenario for example you want mail notification when Bill is successful now all these
things can be done using mailer as the Jenkins component another important component in Jenkins is the SSH agent
Jenkins also follow a typical Master Slave topology now why do we need a Master Slave
topology and a CI tools like Jenkins now when you work in the organization we
have got multiple teams which you want to build and deploy their own pipeline or deploy their own software project
codes Now teams don't want the piece of code to be run on a server which is owned by some other teams so what we can
do is we can set up a Master Slave topology where Jenkins may run on a server X but you can connect multiple
other servers which will act as a slave so when you are deploying a code executing a pipeline you can run those
codes on those XYZ servers now this is the beauty of using Jenkins Master Slave
topology now another good thing about Jenkins is Jenkins can be deployed on any operating system in Jenkins you can
use in a window mode a Linux mode a Mac mode on all these Jenkins jobs can be deployed on any operating system it is
not at all dependent now both master and safe needs to be of the same operating system another important core component
of Jenkins call is javadoc this is our plugin in Jenkins which basically is a
part of a genuine score that helps us to publish results like build action build directory what is the expected Bill
output all those things that we see in Jenkins they all are happening using javato now let's talk about security
injection apart from that Jenkins have other sorts of authorization like project-based
strategy metric based authorization which help us to make Jenkins more secure we'll talk more about this when
we talk about the Jenkins overall so as you can see on the diagram Jenkins is pulling code from the source code
control system like git big bucket Etc to start deploying it in Jenkins we have
got a notification management system using mailer where if the bit is successful we get a war file or a zip
file we can Define how we want the bill to be created and if it is a failure it will send the notification management to
a developer that something happened wrongly with the specific code lines and you need to rectify your codes now that
is the beauty of using Jenkins so Jenkins is still the bread and butter in the industry there are some Jenkins
competitors also in the market like gitlab Jenkins and some others but by
default Jenkins is the to go tools for the CI industry so far we've talked
about Jenkins now let's let's talk about what is pipelining in Jenkins now let's say I want to deploy a code the code may
have certain steps like first step I'm building the code Second Step I'm compiling then code reviewing then
packaging then deploying so how do I automate a combined of all these steps which can be executed in one go
now to deploy all the scenario we have create a delivery pipeline now pipeline is nothing but it's a combination of
different different steps or different tasks that we need to perform in order to deploy our code Now using a pipeline
instead of doing this task or code deployment manually we combine them and deploy them as a sort of one common
approach of a delivery pipeline now there are multiple ways on how you can create a pipeline the two most common
ways or common approaches of using a pipeline as you can see from Scenario a declarative approach and the second one
that is used in the industry is a scripted approach this particular is an example of a Jenkins pipeline script
here you are saying we are first doing a stage called build stage where we will be combining multiple tasks like first
we are using Java then we are printing something called Echo hello pipeline then we are using Maven as the bill
automation tool and try to deploy a package and then we are doing a shell script output of what all things are
present in that particular directory structure this is a typical example of syntax of how to create a pipeline in
Jenkins now let's talk about the two predominant pipeline that I use in the IT industry
scripted and declarative now let's talk about what is scripted and declarative
and how they are different from each other a scripted pipeline is a traditional Jenkins pipeline approach and a
declarative pipeline is a modern day pipeline approach that is used in the industry in a scripted pipeline the
syntax was first strict while in declarative pipeline views something called ask groovy syntax
when you use a declarative pipeline you can get a code from any version system tool like it you can create something
called as Jenkins file and you can download Jenkins file from a virtual constant tool and that can be used to
run your Jenkins code when you use a scripted pipeline you define a code something called as a node block but one
declarative pipeline will Define a code in a pipeline block as we can see in the previous example now let's talk about
what is Jenkins file and how it can be used now Jenkins file are like a text
file where you define your entire structure and syntax of your Jenkins you know only how to download the file and
in particular you can just give the power of your git repo if it is true then get the credential the private repo
and it can pull all the contents from there and just execute your code it is a modern deployment mechanism that we use
when we work with a Jenkins pipeline using Jenkins pipeline we use certain things which help us let's talk about
this first is code review now you do a Jenkins pipeline you can easily review
your code your code can be reviewed by multiple developers before being executed now how that works is in
Jenkins file they are stored generally in a Version Control tool like it you can put something called as a pull
request for a code review now different members of your team can review the code before you actually pull the Jenkins
file and deploy it now you can do an audit Trail or refer to as a long audit you can log each and every steps output
that is being executed as part of the Jenkins file and can see what is happening when the Jenkins file is
getting executed it's sort of like a verbose lock then you can find a single place to store all your data or all your
outputs you don't need to have developers scattered here and there and writing their own codes for executing a
Jenkins pipeline whole organization can approach a Jenkins file mythology where core can be stored in a VCS tools and
from there the code can be downloaded and can work as a single source for your entire pipeline
now let's talk about the Jenkins workflow in particular how does Jenkins workflow work Jenkins workflow always
start with the version control system or you can say a source code repository multiple developers collaborate and they
put their code in a source code system like git from where Jenkins tried to pull the data from that Source control
system for your system there is no mandatory or the source code needs to begin it can be big bucket SVN whatever
your organization uses with Jenkins server now we execute tasks like build deploy compiling packaging code
reviewing now all these things happen using Jenkins server where your Jenkins actually install was running now Jenkins
server can be any operating system it can be a window machine Ubuntu machine Linux machine or a Mac as well it can be
any operating system of today's modern world availability using this Jenkins server will execute our different
different tasks and scenarios in Jenkins server we use the mail functionality for
the feedback mechanism we notify developers to commit or the code deployment successfully or it is a
failure it also has some challenges now what can be done using the feedback mechanism possibly using the Jenkins
server so using Jenkins server we always first deploy the code on a lower life cycle environment that is recommended
standard practice so what we do is we first deploy a QA server on a testing server and once the
code deployment is successful on the lower life cycle then we go ahead and deploy the code on a production server
that is how the entire Jenkins workflow mainly particularly in the rack first now guys let's talk about the Jenkins
installation and how to install Jenkins in a machine as you all know Jenkins by
default run on 8080 so please make sure that if you are using any VMS in your organization the port 8080 firewall
settings are open and configured so that it can be accessible I will get into some of the
prerequisites required for installing Jenkins pose which will go ahead and install Jenkins on Ubuntu box now there
are a few first configuration that needs to be done and I'll be covering those as well so once I have Jenkins install and
configured properly I will get into the user administrative part then create few users and I will get into some of the
plugins for setting up various kinds of access permissions for these users and also put in some of the freestyle jobs
freestyle job is basically nothing but a very simple job and I will also show you
the powerfulness of Jenkins by scheduling this particular job to run base upon time schedule then I will also
connect Jenkins with GitHub now GitHub is a source code source code repository where I've got some repositories put up
there so using Jenkins I will connect to GitHub pull up a repository that is existing on GitHub onto the Jenkins box
and then run a few commands to build this particular repository that is pull from the Jenkins sending out emails now
it's a very very important configuration of Jenkins or any other continuous integration server for that particular
matter now whenever there is any notification that has been sent as a part of either build going bad or by
being good or building propagate to some environment on all these scenarios now you will need the country's integration
servers to be sending out notification so I will get into the bit of details of how to configure the Jenkins for sending
out emails now I will also get into the scenario where I will have a web application such as a maven-based Java
web application which we label from the GitHub repository and I will deploy it into the Tomcat server now the Tomcat
server will be a locally running on the my system eventually I will get into the other very very important topic which is
the Master Slave configuration it's a very important and pretty interesting topics where distributed builds is
achieved using a Master Slave configuration now all this can be very very easily base up using a plugin so
this is what I'm going to be covering as a part of this tutorial now Jenkins is a web application that is written in Java
and there are various ways in which you can use and install Jenkins now I've listed the popular three mechanism in
which Jenkins is usually installed in any system now the topmost one is a window or a Linux Based Services so if
at all you have a window like the way I have and I'm going to use this mechanism for this demo so I will download the MSI
installer so whenever you start as a service it goes out and nicely install all that is required for my Jenkins and
I have a service that can be started or stopped based upon my needs any flavors of Linux is one of many other ways of
running Jenkins in downloading this generic War file and as long as you have a jdk install you can launch this war
file by the command opening up a command box prompt or shell prompt differ to later on Linux box specifying jar and
the name of this profile typically bringing up your web application and you know you can continue with your
installation now the only thing being if at all you want to stop using Jenkins you just go ahead and close this from
even into a control C and then bringing down this prompt and engine cam server and then you also have the other older
versions of Jenkins where run popularly using this way in which you already have a Java based web server running up and
running so you kind of drop in this war file into the root folders or the HTTP
root folder of your web server so Jenkins will explore and kind of bring up your application all user credentials
or user Administration is all taken care by the Apache or the Tomcat server or
the web server on which the Jenkins is running this was a very old way of running but still some people use it
because if they don't want to maintain two servers now if they already have a
Java web service which is being nicely maintained and backup now Jenkins can't run a test to it all right so anyways it
doesn't matter however you're going to bring up your Jenkins instance the way we're going to operate Jenkins is all going to be very very same or similar
but the subtle change in terms of user Administration now if at all you're launching it through any other web
server which will take care of the user Administration otherwise all the command or all the configuration all the way in
which I'm going to run this demo is going to be same across any of these installation all right as I mentioned
earlier Jenkins is nothing but a simple web application that is run in Java so all that is needed is Java privately jdk
11r17 with a 2GB Ram which is amend RAM for running T Jenkins and also like any
other open source toolsets when you install jdk ensure that you set in the environment variables now Java home to
point in the right directory now this is something very specific to jdk now but for any other open source tools that
I've installed there is always a preferred environment variable that we got to set in which is specific to that
particular tools that we're going to use this is a generic thing that there is for you to know now for any other open
source project because the open source project discovered themselves by using the environment variables so as a
general practice or a good practice always set these environment variables according so I already have a jdk 17
installed on my system but in case you do not what I will recommend is just navigate to the browser to the Oracle
home page just type in your search for install jdk mm page then you'll have the
accept the license agreement now what I will recommend is just navigate on your browser to the Oracle home page and just
type in your search install jdk now you're gonna navigate to the town here and install the 64 installer that is the
dot exe now let me show you once I have installed what I've done with regards to the parts so if you get into these
environment variable so I have just set in the Java home variable now you can go
back to your the is PC go to C programs go to Java and then copy this part then
click in the environment variables I'll select the first one that is the
Adaptive system environment variable and we click on the environment variables
and we'll set a path here as you can see here Java home and we'll go down here on
the path so this is the path for your Java home screen so simply click on edit
and click on new and we will just paste the path which we have just copied from
the install.exe Java so I've already installed it as you can see the C
program file so I simply click on OK so the only difference between the system variables and the user variables with
diamond is this is control the whole system of these jdk the system variable
so we want to edit in the system variables so the path I've been changing here is in the system variables all
right now the home variable is where I have the sqls and program friend of the jdk 17. now you see this is my Java home
all right now one thing to do is ensure that in case you are on Java or Java C for a command you ensure that you also
add the part into this part variable right so if you see somewhere I would say there you got the column program
files or you click on that and I'll show you the bin as well I which I have copied and pasted here as well
okay so simply just go back so that's how you get the bin copy this path and
paste it here which I've already done it so I'll just simply click on okay okay and okay let me minimize the screen as
well so the next thing we need to do now is go to CMD to check if your Java has
been installed on your system so you simply type on Java space slash version
and you can see I already have Java version 17.0.5 installed in my system and it's
up and running so as you can see the runtime environments and it's a 64-bit
server this is a VM virtual machine okay so as you can see Java is installed in
my system and it's up and ready so the next thing we need to do now is
install Jenkins I'm just canceling out and open a new browser again new tab and
type in jenkins.io as you can see this is the home page of Jenkins all right so
you can even read the documentation or you can simply click on download will take you to this page now here you can
see the hardware and software requirements then what packaging is required for downloading the installing
Jenkins so this is the part for on the left hand side we have the LTS and on
the other side we have the which is a more stable and on the right hand side we have the regular weekly releases so
we'll definitely go for the stable LTS download which is a long term support
which is released every 12 weeks whereas the regular releases changes weekly so
I'll be going ahead and scroll down and click on window so as you can see there
are different parameters where you can do you can have on Dockers then you have Ubuntu 2 then you have the Centos Fedora
or the Windows like I'm going to install in this case in this demo then but we have different aspects as well like the
FreeBSD the gentos Mac OS Etc so as you can see on the first top we have the dot wire we can even install
that and unzip that but in this case in this demo part we'll simply click on windows so go on ahead and click Window
Guys now you can see the download has started
so we just wait a few seconds until it's done installing the MSI and as you can see it's done so this is my download in
Jenkins instance so what I'll be doing now is simply click on next now as you
can see guys on top here it's written the Jenkins run on localhost 8080 so
keeping that in mind click on next as you can see this is where our gen consensus is installed that is in C and
program files and by the name Jenkins so you can always change this location
where the Jenkins has been installed but in this case I'll rather keep it on the C drive itself so the next thing you
need to do now is you will run server as a local system so that we don't have to put any administrator or the password so
the next step now is to test the part so as you can see you got a green tick mark so next now you can see here that there
is no it has failed because there is no compatible Java version 11 or 17 but as
you can remember guys we have already install Java 17 and we have already verified that we're using the command
prompt where it has shown us the version Java 17. so what you need to do is simply click on change and select the
Java 17 from java 8. so simply click on Java 17 jdk70 as you can see click on
next and click on the install part done and dust it so I've already told you
that Jenkins run on the localhost 8080 right so we're gonna go localhost colon
8080 and hit on enter now as you can see please wait while Jenkins is getting
ready for work right so this will take a few minutes and then you'll be having to provide the credential of the
administrator all right so just give it a few minutes while it's starting up all
right now you have to provide the administrator password right so you can go ahead and go down out here you can
see the password is in the C and goes to the program data then Jenkins then we
have a DOT Jenkins then secret then you'll find the initial admin password so we need to head out there so what we
need to do is we'll go down there so I'm just going to minimize the screen now minimize it once again and as you
remember we go to the C as you can see right where it is I'll just show you minimize the screen here so you can see
and do it along with me so you can see this in PC then scroll down to C disk
and this thing popped up sorry it's in this so you have a program data right as you can see here down there is no
program data right so what you need to do is it's in the hidden file so what you need to do now is click on the three
buttons and view click on show and click on the hidden items now let's see if
it's there scroll down program data you can see at the last yeah simply click on that double click on it as you can see
then we have to go to the folder name as Jenkins I can see on the top Jenkins and
Don Jenkins then we need to search for Secrets there it is folder and as you
can see there's an initial admin password now you simply click on that we'll try with the notepad so we can
copy so simply copy that and paste it in the administrator password and click on
continue so in this case I will not in save it in my computer or system so
there are two installation plugins available which is custom by Jenkins one is they have suggested plugins and the
other one is you can select the plugins which you want to be installed so in this scenario I'll simply click on the
install suggested by the Jenkins now what happened is the plugins are all related to each other so it's like the
typical RPM kind of a problem where you try to install some plugins and got a dependency which is not installed and
you get all those issues in order to get rid of that what Jenkins recommend is that there are a bunch of plugins that
is already recommended and they are good to go right so just go ahead and blindly click on that recommended by the plugins
which is a suggested or custom by the Jenkins and as you can see these getting
started process is going ahead so it will take a few minutes depending on
your internet speed or on a system now you can see all these plugins are installing it for you so it will take a
few minutes time so let's just wait till we get all the plugins installed in a
system properly so this is a good combination to kind of begin with and keeping that in mind Jenkins need a lot
of bandwidth right now in terms of network so in case you know your network is not so good it'll make for a few of
these plugins will kind of fail and these plugins are all you know are available on openly or mirrored site and
sometimes some of them may be down so do not worry in case some of these plugin
kind of fail to install you get an options to kind of really try installing them but just ensure that you at least
at the most 90 to 95 of all of these plugin are installed without any problem
as you can see it's we are almost there it's around 60 to 70 percent
now once all these plugins are installed now my plugin in the installation is all
good there's no failure in any of the plugin so after that we've got to create this first username that is the first
admin user so you need to give in the username and the password then you have to confirm your password give it your
full name and your email address right so we'll start it by giving the username I'll keep it plain and simple now you
got to remember these password and the confirm password because if you kind of lose the or you don't remember the
password of the Jenkins it'll be really hard to get back your account so right keep it as simple as possible and
keeping in mind that you have to remember the password because getting back this account will be really difficult all right so I'll be putting
the password as well give it my full name okay my as small and go to the
email address dot memory add their Fade Into record
code all right so once all done so just simply click on Save and continue
so click on never don't want to save the password and as you remember I've told you by default chain can run on
localhost 8080 right so you can simply click this or you can copy this also
just write save and finish you can even copy this link save and finish now start using Jenkins all right now that's kind
of complete my Jenkins installation as you can see it was not that tough at all now the Jenkins is installed correctly
now let me quickly walk you through some of the very minimal configuration that
is required now these are kind of the first time configuration that is required so and also let me warn you the
UI is little hard for many people to wrap their head around especially the window guys but if you're all a Java
guys you know how painful it is to write UI in Java you kind of appreciate how the effort that has gone into the UI
bottom line it was a little hard to know wrap your head around it becomes like start using it possibly you start liking
it all alright so I will get into some of these important on something called as configuration system or the
configuration a system this is something kind of put in your configuration to complete a Jenkins instance now few
things to kind of look out for this is the home now configure system is where
you kind of put in the configuration for your complete Jenkins instance now few things to keep a look out is the home
directory all right now this is a Java home where all the configuration all the workspace anything and everything
regarding Jenkins is stored out here in the system message now in the system
message you can put in some of the message on the system you can just type anything you want probably show up
somewhere here on the menu now the number of executors is a very very important configuration this just lets
Jenkins know at any point in time how many jobs or how many threads can be run so you can kind of visualize like a
thread that can be run on this particular instant as a thumb rule if at all you're on a single core system the
number of executor you should be good enough in case at any point if there is any multiple jobs that kind of get
triggered at the same time now in case the number of executors are less compared to the number of jobs that are
out there to open up the new so you don't need to panic because they will get queue up and eventually Jenkins will
be get up and running so just keep that in mind that whenever a new job is kind of gets triggered in the cpu's usage and
the memory usage in terms of the disk it's like a very high of the Jenkins instant so that's something you got to
keep in mind all right the number of executors for my system is kind of a good label for my Jenkins I don't want
any of these usage so how do you want to use your Jenkins this is good for me because I only have a primary server
that is running so I want to use this as low as possible all right as you can see
it's two now this is usage so I can leave everything to default so what I want is I want this SMTP server
configuration now Jenkins will be sending out some emails and what I've done here is I've configure the SMTP
details of my personal email details in case you are you know in your organization you want some sort of an
email that is like an email ID that is set up for a Jenkins server so you can specify the SMTP server details of your
company so that you know you can authorize Jenkins to kind of like send out emails but in case you want to try
it out like I have configured my personal email ID which is of the Gmail for sending out notification for the
same TP server will be the SMTP Gmail so it's right as username as
so if we have used this MTP artification and then we're given the password
now you can just send out an email to see if at all this configuration works again now Gmail will not allow you to
allow anybody to send out notification on your behalf so you just have to lower the security level of your Gmail ID so
that you can allow programmatically somebody to send out email notification on your behalf so I've already done that
so let's see if this work
Let's test this again since I've entered the wrong password let's see if this works or not and it
has given me a password has not accepted so now as you can see since my Gmail has
not been set up so what you have to do is you have to lower your security level of your Gmail ID so that it can be
allowed to test your email configuration all right so simply click on save so now
we're heading back to the dashboard now we should go to the global tool
configuration now look at the scenario where Jenkins is a continuous integration server now it does not know
what kind of a code base it's gonna pull in what kind of tool says there's a
required or what is the code that is going to pull in and how it is going to build so you'll have to put in all the
tools that is required for building the appropriate kind of code that you're going to pull in from you know like a
source code repository so for example in case your source code is a JavaScript code and assuming that you have you know
because this demo this is my laptop and put all the configuration jdk everything on my laptop because I am a developer
and I'm working on the laptop but my country's integration servers will be known as a separate server without
anything being installed on it so in case I want Jenkins to you know like run on a Java code and I need to specify the
jdk location of this out there so okay since I've already have the jdk install
and have already put in the Java home directly or rather and the environmental environment correctly so I don't need to
do it so now if at all you want the Gen Con server to use git now as you know that git is a command bash or the
command prompt for running git and connecting to any other git server so you will need to get to be you know
installed on that particular system now Gradle and may win if at all you have some even as you want to do this in any
other tools that you're gonna install on your system which is your quantities integration server so you'll have to
come in here and try to configure it and in case you have not configured it then Jenkins will not be able to find his
tools for for building your task so since I don't want to install anything we'll go back to man system now you can
see the global configure Global Security you can see there's an SSH server now what does this mean if you ask me
Jenkins own user database so this is nothing but the security access control
which is set to Jenkins own user database so this mean is you know Jenkins by default it uses files system
where it stores all the usernames which hashes up like these username that kind
of stores them as of it so as of now Jenkins is configured to use in own
database assuming that you are running an organization so you will probably want to you know have some sort of ldap
server which will in turn have a control of your access to your Jenkins repository so you will specify your ldap
server details the root D and password or the manage TN so the ldap as you can
see all the ldap which the Jenkins itself will tune in for a specific servers now you have the manager DN the
manager password now all these should be filled all these details should be filled in case you want to connect a jenskins instance with your ldap server
or any of the authentication server that you have in your organization but for now since I don't have any of these
things I'm going to use its own database that's good enough all right so I'll simply the Jenkins own user database so
I will set up some authorization methods so for now let me not get into any of these details of this and just be aware
that dungeons can be connected for authorization to an ldap server or you can have Jenkins managing its own server
which is happening as of now let me put in a very very simple job alright so let's create a new job in this video so
simply click on new atom or simply you can add any item name here
so the most important here are the freestyle and pipeline now for the first demo we'll click on the freestyle job
and click on OK so in the description you can write anything in your description this is an optional thing so
you can say this is my first job demo for so in this demo I don't want it to
connect it to any source code for now so I'm just going to leave it to none now we don't need to write any build
environment as well now in this build step we simply click on execute windows on so since I'm on the window box I will
say execute window bash command all right so what do you want to do so let me just equal something Echo hello world
this is hello welcome to my plus demo
job in the record they will specify that date and time as well so put in date all
in as time as well so you can see it's a very very simple command you just have to put an echo and we have something
along with the date and time so we'll keep everything as simple as possible so you save this job so once it is shipped
so you can see there's a bill history so once you have saved this job you know the job name comes up here as you can
see project first job by basically I've written Double D in my first job now as you'll see here there's some builds
history so there is nothing as of now because I've put in a job I have not yet running it all right so let me try and
build it now now you can see there's a number my first job so you will see a date and you will see the time since
I've clicked twice so if I've come one and two now if you click on this uh you can see the console output now if you
click on this now you can see is that and this is the output that is run hello welcome to my first demo job in iterica
and along with the date and long the time now if you see this now if we
navigate this now if you navigate this we go back to here now as you can see there's a folder created as jobs right
so this is the directory what I was mentioning earlier regarding Jenkins home if you can just open it up either
you can see this for the jobs so this has mentioned earlier regarding Jenkins home so all the job retailers and stuff
it's specific to this particular Jenkins installation is all here now all the plugins that I installed the details in
each of those plugins can be found here right so the workspace is where the jobs that I've created whichever I'm running
would you know there'll be an individual folder specific to their jobs that has been put up there and all right so one
job one quick run that is how it looks like so the workspace is where all the jobs that I've created whichever I'm
running which you know there will be an individual folder specific to that jobs that has been made up all right so as
you can see there's the first job that we have just created just now so one job one quick run that we have done this is
how it's look like it's pretty simple now let's go back so let us put on a second job so we can go back to
dashboard click on new items and say second job let's get this spelling
correct this time second I wrote second first again second job and I can put X
got in here underscore second job you click on freestyle again click on OK and
you can see this is my second so I just want to demonstrate the powerfulness of
the automation server and how simple it is to automate a job that is put on the Jenkins which will be triggered
automatically now remember what I've said earlier about Jenkins because at the core of Jenkins is a very very
powerful automation server all right it's my second so this is my second job
which will be triggered automatically okay so in the build step we'll put bash
terms then we'll write here Eco again so let's say here something as second job
is going on here the tools sorry with
the tools of the and automation tool ation tool
and then I'll give some data as well date put on the time but this time I'm
going to demonstrate the powerfulness of the automation server that is there so if you see here there is a build trigger
so build can be triggered using various triggers that is there so you'll get into the GitHub Trigger or a hook or a
web hook just kind of triggering so we'll see that later on but for now what I want to do is I want to ensure that
this job that I'm going to put in will be automatically triggered let's say every minute I'm on this job to be run
on its own so we'll put as Bill periodically and we will set the time
now if you can see here there's a bunch of help if you want to go through it and
you can go through it let's just put a very very simple regular expression for scheduling this job every minute all
right so five star is All I'm gonna put in one two three four five now let me
save this come back so when you build periodically is now you can see here now we just copy
that and now how do we check if it gets triggered every 15 minutes so I just don't do anything but wait for 15
minutes and if all everything goes well Jenkins will automatically trigger my second job in 15 minutes now you can see
here is still processing so we have to wait for 15 minutes we can go through
all this command of the scheduling time so you can see how many stars required
for each minutes or for each hour now as you can see it's done and just go and go
here and go to console output now you can see the Finish is success and you
have this now we can go back and copy this and see if they're on your ads
already there you can go to workspace you can see your web second job has also been created automatically by Jenkins
now that I have my Jenkins up and running a few jobs that have been put up here on my Jenkins instance I will need
a way of controlling access to my Jenkins server this is where I will use a plugin called role based access plugin
and create a few rules the rules are something like a global Rule and a project rules for a specific role I can
have different rules and I can have user who have sign up for the users for my create kind of assigned to these roles
so that each of these users fall into some category this is my way of kind of controlling access to my Jenkins
instance and ensuring that people don't do something unwanted alright so first thing first let me go ahead and install
a plugin for doing that so we're going to manage Jenkins then we go to manage plugins we will plugins installed
plugins we have advanced settings so these are the available plugins or and
then if you go to install now these are the install plugins now let's go and search for role base now you can see
here this authorization strategy so this will happen is it enables user authorization using a role-based
strategy our rules can be defined globally of particular jobs or notes selected by a regular expression this is
the plugin server so you will install it without restart it's in success it looks
good looks good no waiting for this yeah success done now so go back to the top
page now one thing to remember guys is Jenkins is running on a Java using a
Java instance so typically many things will work the same same way unless inherently you want it to restart so but
a good practice is whenever you do some sort of a big installation or a big patches on your Jenkins instance just
ensure that you can restart it otherwise there will be a difference in terms of what is installed in the system and what
is there on the file system so you need to flush out few of those settings later on but for now these are very small
plugin so this will run without any problem but otherwise if at all there
are some plugins which they will need to restart you know you can kindly go ahead and restart your Jenkins instance but
for now I don't need that it looks good I have installed the plugins so where do I see my plugins and install the plugins
which is specific to the user controlled Access Control now let's go we go to the
configure Global Security now we can go ahead and check the authorization you
can see here authorization role-based strategy popping out here right as you can see the role-based strategy has been
installed and it's been shown here this is because of my installation of my role based plugin so this is what I would
want to enable because I already have my own database setup and for the authorization part in that sense that
you can do what I'm going to install I mean I've only installed a role-based strategy right now I'm going to enable
that strategy so simply click on enroll based strategy okay and click on save so
now I'm going to enable that strategy all right now I will say okay now I've installed the role base access plugin
now we'll need to just set it up and check you know I will go ahead and create some rules and it shows that I've
assigned users as per these roles so let me go to manage the configuration manage
Jenkins and see how we can create our roles not here as well so we can do
manage and sign rules for creating a rules here now you'll only be able to see this once and only when you have
installed those plugins now I've enabled the plugins now I've enabled the role-based access
control as well now we'll go ahead and create some rules for this particular Jenkins instance so first I will manage
roles so this is a very high level which is a global role now there are some role
to add which can always sell some item rules for some project or something so let me create a role which kind of
visualize like a group so we'll create our own call the developer typically the
Jenkins instance or the ca instance are kind of own up or controlled by a QA
guys so that's why I'm creating a rule called developer and I'm adding this role at a global role so I will add this
here and then I can click rear as well so we've given this administrator right
so we add this so as you can see here you will see this develop a rule that
there is each one of these options that you can hover here and sort of see what kind of permission you have given it to
this developer role that you have just created so you can read and right then you can create some changes we've given
a permission as credential which you can create then you can delete then the
manage the domains so this kind of help or permission that they're giving it so
this is some kind of sort of a help for a permission specific so you want to see
it's like how it sounds a little you know different but I would want to give you a very very little permission for
the developer so from an administrative perspective I just want him to have a read and write kind of a rule
credentials okay let's read and write and I can allow him to view as well I
don't want him to create agents or give him some delete no I don't want him to configure Bill provision or connect or
disconnect anything so I'll just give him a read and a view and I don't even want him to have access to my workspace
as well so I'll just give him now for run so I wasn't even given the access to
that now I can give view as well you don't want him to delete what I'm doing here is just creating a
global rules for the developers and I've given him a very very limited roles in
the sense that I don't want this developer to be able to run any agents nor create jobs or build jobs or cancel
jobs or configure jobs at the maximum so I just want him to read that is already put up there okay so I would save now
and create a rule that is done now that I've created row now I still don't have
any users that is in there in the system so let me go ahead and create some users on the system
so we'll go back to manage Jenkins and go to manage users now let me create
another user okay by clicking on plus users and we'll say this one as a
developer give him some password and name him as develop into three an email
and just simply give my email here and click on create users okay one two so we
have created now this developer one two is a user that I've configured but still yet I have not set any rules for this
particular user so I will go to the manage and gain I will give him the manage and assign rules so now we'll go
manage and send rows and we'll as intro so what you're gonna see what I'm gonna do now is I'm gonna assign a role that
is specific to that particular developer so I'll find that particular user and
assign him what means the developer role that I've already configured the rules okay so this is the role that we have
created so we will search the user that we have just created developer one two
now if I assign this so go ahead and save the changes now let's go ahead and
check the permission of this particular user by logging on to my main account and logging back as the developer now
let's log off from here log out and say developer developer one now if you
remember this role was created with very less privilege so there you go I have Jenkins but I don't see a new item I can
trigger a new jobs I can do anything I see these jobs however I don't think I'll be able to start this job now I
don't have the permission to set forth at the maximum so what I can do is look at this job see so this is a limited
role that was created and I've added this to that particular role which was a
developer rule so that the developers don't get to configure any of the jobs because the Champions instance is owned
by a QA person he doesn't want to give developer any ethnicity or write so the
rights that you set out by creating a developer roles or any user who's tag as
a part of this developer role will get the same kind of permission and these permission can be you know fine grain it
can be a Project Specific permission as well but for now I just demonstrate the high level permission that I have set so
let me quickly logged out of this user and get back to my admin account so I need to because I need to continue my
demo with the developer roles that was created now one of the reasons for Jenkins being so popular as I mentioned
earlier is the bunch of plugins but it's got plug-in for connecting anything and everything but you can find the Jenkins
plugins then you will see the index over here there are so many plugins that are there all of these are wonderful plugins
whatever connectors that you will need if you want to connect Jenkins to the aw instance or you want to connect Jenkins
to a Docker instant or any of those containers you will have a plug-in you can go and search up if you want if you
want connect Jenkins to big bucket simply click and big so as you can see these are available so bottom line
Jenkins without plug-in is nothing so plugins is the heart of Jenkins for you to connect or in order to connect
Jenkins with any of the containers or any of the other sets of tools you will need the plugins now if you want to
connect or you want to build a repository which has got Java and Melvin you will need to install Maven and gtk
on an instance now if at all you're looking for a or a Microsoft bill you
will need to have MS Bill installed on your Jenkins instance and the plugins that will be triggered let me spill if
at all you want to listen to some server side web hooks from GitHub you will need a GitHub specific plugins you want to
connect to Jenkins to AWS you need those plugins now if you want to connect Docker instance that is running anywhere
in the world as long as you have the URL which is publicly reachable you just have a darker plugin that is installed
on your Jenkins instance so if you want to connect to a Docker instance that is running anywhere in the world as long as
you have the URL which is publicly reachable you just have a plugin that is installed on your Jenkins instance now
sonar cube is one of popular statistic code analysis so you can connect a Jenkins bill you can build a jobs on
Jenkins and push it to a sonar Cube and get sonar Cube to run analysis on that and get back the result in Jenkins now
all of these Works work very well because of the plugins now with that let me connect Jenkins instance to GitHub so
now let us connect our Jenkins instance to GitHub now already have a very simple Java repository up on my GitHub instance
so let me connect and consider this particular GitHub instance and pull out the job that is put up there right so
this is a very very simple and you know respiratory this is called hello Java and this is what is there in the
repository this is a simple class file this is called some one line of system dot out so this is already present on
github.com at this place and this will be the URL for this repository so I will
pick it up so what I'll do is I will connect my gen skin instance to go to my GitHub and provide my credential and
pull out this repository which is on the cloud hosted in the github.com and get it to my Jenkins instant and then build
this particular Java file now I'm keeping the source code very very simple it is just a Java file now how do I
build my Java file how do I compile my Java file I just save travel C and name
now the name of my class file is hello Java and how do I run my Java file I will say Java and hello okay so remember
I don't need to install any plugins now because what it all needs is again plugin so git plugin which is already
installed on the system so let us go back create new items we'll create the first
job here let's say this is a git first project now let it be a freestyle okay
now we'll go to the source code management remember right we always have been keeping it undone so in this part
we'll put it as git now as remember earlier in examples we do not use any source code because we were just putting
up some Echo kind of jobs that did not need any integration with any of The Source Code system so now let us connect
this so I'm going to put up my source code and it will show up because the plugin is already there now I have only
copy the HTTP address from the GitHub and simply paste it here now what is the
current so simply add that if you add credentials same so I'll just click and
cancel so if it just add Jenkins so we'll add a username here and if there
are any errors in terms of that ability Jenkins to be able to find their get or the git.xe or if the credential are
wrong somewhere down here so you'll see a red message okay like something is not right for now it's okay now I'm gonna
build trick now what will be there as a part of my build step because this capacity just have the Java file correct
low Java so in order for me to build this I will say execute window bash
command and I'll say Java hello Java Java now let's see if this okay so let
me try to run this okay we'll now click on this console output rate now as you
can see here there's a lot of executed get on the behalf it goes on there to provide my credentials and so you know
it will arm a repository and my default I will pull up the master Branch this is there on the post string and the kind of
builds this whole thing on Java C hello world finish and it's giving me a success so it's kind of build this whole
thing Java C hello to Java and it runs this Java hello hello world and as you
can see this is the output it's there and here you can see if you want to look at the content that was created on the
repository now if you can go there into my workspace like I've showed earlier you can just simply click here and you
can already see here it's been created kit first project and say hello now as
you can see it's already there now if you can go back as you can see and simply close that now what it did was
Jenkins only be half went over all the way to GitHub will this repository from there and then you know it brought it
down to your local system on my Jenkins instance now it's compile it and it's done this particular application now
that I've integrated Jenkins successfully would get simply within the Java application
[Music] what is Docker well Docker has a very
simple definition Docker is an open source platform for developing shipping and running application
it enables you to separate your applications from your infrastructure to deliver the software quickly with Docker
you can manage your infrastructure in the same way you manage your applications
by taking an advantage of docker's methodology for shipping testing and deploying codes quickly you can
significantly reduce the delay between writing code and running it in production
Docker has some amazing features like Docker has the ability to reduce the
size of the development by providing small footprint of the operating system via containers
with containers it becomes easier for teams across different units such as development quality analysis and
operations to work seamlessly across the application you can also deploy Docker
containers anywhere on any physical or virtual machines and even on cloud since
Docker containers are pretty lightweight they are very easily scalable
because of these features many organizations have switched to docker
now that we have got a brief understanding about Docker now let us see how to install Docker so we will be
installing Docker in Windows as well as in Ubuntu and once we install the docker we will
see how they run in each one of the environments with the hello world program so I'll be using the Ubuntu through a
virtual machine so this is your ubuntu's operating system now here we'll be installing the
docker for this we will first need to open our terminal
so to install Docker we will be following the guide to install a docker
here I have the official document of Docker where we get to know how to
install the docker engine on Ubuntu so as you can see here there are some prerequisites
The Voice requirement the different versions of Ubuntu are available over here so you should have 64-bit or
version of any one of these Ubuntu versions if you have Docker previously
so you can uninstall your old version and we can set up a new version of
Docker so this is the installation methods you can follow this guide and install Docker in your Ubuntu or any
operating system so now let's quickly go back to our terminal so before you install engine for the
first time in your host machine you need to set up a Docker repository afterward
you can then install and update the docker from that depository so first to set up a repository you need
to update the package index and install the packages to use the repository over
the https so let's get started so our first command is sudo apt
get update they will ask you your password so you can give a password
and now here you can see reading all the packages in the operating system
now once this is done you need to install the some of the packages so we
will just copy this code and we'll just paste it over here
and let's run so here it will take some time now
let's hit yes now the packages are configuring
your packages are installed now what you need to do is set up the docker repository so here first you need to
import the Dockers official jpg key to verify packages signature before
installing them with apt get for this we need to copy this command that is sudo
mkdir Etc apt keyring so from here we will get the jpg key and this is the actual
Docker installation link so from here they will install the docker repository
we'll just paste it and hit enter so here we need to run this curl command so
we'll just quickly paste this command and let's see we'll just give them the access
now what you need to do is upgrade the index and install the docker Community Edition for that what you need to do is
sudo apt get update all our packages are installed now let's
install the docker for that we will write the command sudo kpt get
installed Docker CE hit enter let's see just hit yes
so it may take a while they are just unpacking the whole Docker
package let's wait for a while now in order to check whether our Docker
has been installed successfully or no let's check with checking the docker version so for this we need to just type
docker slash version and hit enter so as you can see guys
your Docker has been installed successfully you can see all the different commands and the functionings
of how to use the docker as we mentioned here you can see the different type of
commands they have specified here now let's run an hello world program and see
how Docker works on Ubuntu so here you just need to type sudo Docker run hello world and hit enter
so here what has happened is he as he had typed here the sudo Docker run hello
world so what Docker has done it has downloaded the test image of hello world
and then it has been passed to the container it prints a confirmation message so when
the docker runs it prints a confirmation message as you can see over here hello from docker
so this is how we install Docker in Ubuntu now let's see how to install
Docker in Windows so let's get back to our Windows environment
so here first we need to install the docker installation package for that we need to go to our main page of Docker so
let's type Docker installation so here you get this link for get Docker
so we'll quickly get into this so once you land on this page you can see
you have Docker installation package for three different environments that is for
Mac windows and Linux So based on your requirement you can
choose any of one of your platforms where you would like to install docker currently we're gonna
install this Docker on the windows so let's go choose the windows
version so as you can see here this is the installation document or a guide for
Docker on Windows so in Windows you need to have some of
the system requirements like the latest version of Windows that is Windows 11 or Windows 10 with a 21 H2 pro versions and
it can be more than that apart from that you should have the dustblue SL2 feature
to be enabled on Windows so to run the the docker container
whereas you should have the hardware prerequisites like 64-bit processor 4GB system RAM and bios
level hardware virtualization support so these are the system requirements you you need while installing the docker
what we need to do is to First download the installing package of Docker so you just
click over here and then it automatically gets
so once you double click here you will Docker installation package so it will
take a while to download the installation package
so here as you can see it is initializing here they are verifying the
package and now let's move ahead well this is a very easy step in Windows
like you just need to install the whole Docker package all right so our installation is
succeeded now let's close it and let's go to our desktop
and open our docker now here the docker asks for the
subscription service agreement now you just need to accept it so once you come to this desktop here
this is how the docker desktops look so first we need to sign up I already had an account so it has been
signed in now let's see how to sign up as well for that what you need to do is
go to sign up here first you need to create your account in Docker so for
that you need to give your username so you can keep any username for example
one so here you need to add your email ID so just type down your
female and you need to have a strong password so it
is up to you what kind of password are you putting now you need to accept the
agreement what they have asked and then you need to verify
once you get verified then you get you just need to sign up so here as you can see get started with
Docker with few easy steps so we'll just start and here you can see you can clone your
repositories you can use your GitHub so here is a tutorial what Docker
provides while you start with the docker so we'll just quickly skip it up
and let's get started so once you sign up you need to log in
and then you finally get landed to this Docker desktop it automatically opens so
you can find it over here all your login credentials and profile and here you see the containers image
volumes so as of now we have nothing on this Docker desktop
so first we'll open our command prompt in this command prompt we will first
check the docker version so for that we need to just type docker
hyphen version so here you get the uh version of the
docker which is 2 0.10.21 and now let's run the hello
world program like we did it in Ubuntu so for this you just need to
type Docker run hello iPhone world
and hit so as you can see you have got the same output as you had gotten in Ubuntu we
work on with the containers this is how containers work on the docker now it's
time to delve into the technical concepts of Docker first is Docker image
a Docker image is a read-only template with instructions for creating a Docker container
Docker images are the key Concepts in the world of containerization and they are important part of Docker ecosystem
they allow the developers to create and distribute software in a standardized containerized format making it easier to
deploy and run applications consistently across the different environments if we
go with the definition a Docker image is a read-only template with instructions for creating a Docker container let's
make it more simpler they are built using a series of commands that are stored in a text called Docker file
these commands specify the base image to use any additional packages or libraries
that need to be installed and any other necessary configuration
these Docker images are stored in the docker registry such as Docker Hub will be studying in further
which is a public registry that anyone can use to store and distribute Docker images
well you can also set up your own private registry to store and manage your own images
Docker image is also lightweight Standalone and executable package that includes everything needed to run in a
piece of software including the application code libraries dependencies
and runtime so this was about Docker image so now let us understand what is a
container a Docker container is a runnable instance of an image it is defined by its image as well as
any configuration option that you provide to it when you create a container so the docker containers are
the way to package and distribute software in a portable and consistent manner so that it can run anywhere
regardless of the environment containers offer several benefits like isolation
here containers allow developers to isolate their application from underlying infrastructure so that
applications can run consistently across different environments next is portability
here containers can be easily be moved between the environments allowing developers to easily test and deploy
their applications in the different environments next is resource efficiency
this means that containers allow developers to package only resources that their application needs resulting
in more efficient use of resources and lastly they are highly scalable that
means containers can be easily scaled up and down to meet changing demands making
them well suited for use in Cloud environments now while they were learning about Docker image we had used
the word dock of file well adoco file is a text document that contains all the commands a user could
call on the command line to assemble an image it's basically a text file that
contains the instruction for building a Docker image it specifies the base image to use for
the build as well as any additional dependencies or libraries that are needed to be included in the image
Docker file is basically used to automate the process of building a Docker image so that you don't have to
manually install all the required dependencies and libraries instead you can simply specify those dependencies in
the docker file and the docker build process will take care of installing all of them so these are the key components
which are used in Docker ecosystem now that you have come to know about the key components let's see how these
components work in the docker architecture architecture consists of the following
components first is the docker engine Docker engine is the core component of
Docker which is responsible for running containers and handling container related tasks it consists of a domain a
rest API and a command line interface that allows users to interact with the domain the docker engine can run on
variety of operating systems including Linux Windows and Mac OS next is Docker
Hub Docker Hub is a public registry as I had discussed before that Docker Hub is
a public registry that hosts a large collection of Docker images which are pre-built packages that contain all
necessary components to run an application users can search for and pull images
from the registry to use a base for their own containers next is the docker
client the docker client is nothing but the command line interface that allows users to interact with the docker domain
the client sends request to The Domain through the rest API which then carries
out the requested actions such as building running and shipping containers we all know what are containers that is
nothing but a standalone executable package that contains all the necessary components to run an application so to
build a Docker image we can use the CLI that is nothing but the client to issue a build command to the docker domain
then the domain will then build an image based on our inputs and save it into the
registry which can then either go to the docker Hub or a local repository if you
do not want to create an image then we can just pull an image from the docker Hub itself which will would have been
built by different other users so that we can issue a run command from the CLI
which will create a container so this is how the docker architecture works now
let us move on with our next topic that is important Docker commands so here
we'll see some of the important and basic Docker commands that are used while you are working with Docker and
we'll see each one of them respectively so starting with Docker version basically this command is used to get
currently installed version of Docker so whenever you install Docker and you want to know about its current version then
you can just type down this command to know what is the current version of that particular docker
next is Docker pull so this command is used to pull the images from the docker repository so
whenever you like through Docker file create an image when you run your image so what does the docker engine do is
like pull that image to run that particular image the next one is Docker run here this
command is used to create the container from an image so as I said once your image is pulled
from the repository now it gets converted into containers and then you can run that those particular containers
so the main function of the docker run is to create the containers out of that
particular Docker image so the fourth command is the docker PS
here this command is used to list the running container so if you have created
more than one containers and you want to see how many containers are running at that moment then you can just hit this
command and you can see how many containers are running after this we have the docker PSA so
here as in PS we can only see the list of running containers and in PS we can
see all the containers which you have created that includes your running as well as
the exited containers now our sixth command is Docker execute
this command is used to access the running containers next we have Docker stop here we use this command to stop a
particular running container so in if any container is running and you wish to stop that container then you can just
use this command next is Docker kill this command kills the container by
stopping its execution immediately so well you must be thinking the docker stop also stops the container whereas
Docker kill also has the similar functions well Docker kill command is
slightly different from Docker stop let me tell you how well when you use
Docker stop gives the container time to shut down gracefully in situations when
it is taking too much of time for getting the containers to stop then you use the docker kill command well this
was the only difference between Docker stop and Docker kit next we have Docker Comet this command creates a new image
of an edited container on the local system and then we have Docker logins use this
command to log into your Docker Hub repository you can just use this command
when you are logging into the docker so on your command prompt itself you can
log in using this command next we have Docker push so this command is used to
push the image to the docker of Repository that is nothing but export the image to the repository next we have
Docker images so here this command lists down all the docker images whatever you
have created in the docker then comes the docker RM Docker RM is basically a
command used to remove one or more Docker containers so when you run this command Docker stops the container and
remove the containers file system from The Host this is useful when you are finished using the containers and want
to remove to free up the space on your host this was a function of Docker RM
next is the docker RMI this command is used to remove one or more Docker images
don't get confused Docker RM is used to remove the docker containers and Docker
RMI is used to remove the docker images so here when you run this command so
Docker removes the images and any untagged parent images that were referenced by the deleted image so if
you have any sub images like you have a parent image and a child image so if you
delete the child image then automatically the parent image will get deleted here you can also move multiple
image At Once by providing a list of IDs you just need to type Docker RMI and you
need to specify all the image IDs which will remove all at once it is important
to note that removing an image can cause problems if image is being used by other
containers or if the image is the parent of other images be careful while you are
using the docker RMI and make sure that you understand the consequences of
removing it so if you delete a particular image which is relatable to other containers or images then it will
affect to those containers and images so make sure that before deleting an image
you should be aware of the consequences last we have the docker build here the
docker build command is used to build image from the docker file so here the basic syntax of Docker build is like you
write Docker build and you then and specify a path and the URL where part of
the directory containing that particular Docker file and the context and URL is the URL of the git repository which
indicates that Docker file should be read from a standard input so these were
the some of the basic and important Docker commands which are used while
creating image or working with Docker these comments it is very easy for you
to work with docker well this was all about Docker we got to know about the
architecture and we also saw some of the docker commands now let's use these
Docker commands and create our own project and see how they run so now for running our project we will
be using the Linux environment let's quickly switch to Ubuntu so and here we
will create a Java project and through that we will create images and then
we'll try to run that Java project so for that we will just open our terminal
so as I said we will be working on the different commands whatever we had discussed in this slide
so we will work with all the commands over here and we'll see how the docker works so let us start with
the First Command that was the docker version so let's type down docker
hyphen B this tells you the docker version now let's login to our Docker
account so you just need to type down Docker login and then you need to give your
username and password like you're not able to login from the
terminal you will get an option here to login through the stalker Hub so they
directly give you the link to login so you just open link or from the terminal
and you can head over automatically gets open on the your web browser so here
once you come here it will here you can log into your account so I
have already logged into my account so so these are some of the containers
which I had created before from here also you can log in whereas also login
from the terminal itself now let's quickly go on with our demo as I said we
will be creating a Java project so now let's move into our Docker folder which
we had created so let's type down CT Docker so now once you are in the folder
now let's as we are creating a Java project so let's create one more folder
into it where we will specify it as a Java project and kdir
Java project now we'll enter to this folder and we'll then start creating a
Java project let's quickly get into it so let's move on to our Java project
folder for this you need to just type c e project Java project
enter into that folder now here we'll be creating our Java project so we will be
using the vs code itself to create the Java project over there so let's open
our vs code just need to type Port face Dot and it automatically opens your vs code
so here we will be creating a Java project and also our Docker file so that
we can send the whole file to our Docker so
first let's create a Docker file so our Docker file has been created now before adding the commands or the information
in the docker file so first we'll create our Java project so let's quickly open
new file name it as Java demo
dot Java and let's open it we'll start with our classes class so let us name it
as class test then we will use our main method
that is public static void mean and we'll Define the strings
inside this we will use this system.out.printellin code or and inside
we will just write welcome to Java
now let us add some methods to make the project look more interesting so we will
just add public static void print system properties and
now we will print the properties we'll type this out command that is system
dot out Dot print Ln
and inside this we will write the printing properties
you can name it your way the way you design it so it's just a simple program
just to see how we can run these in Docker so it's totally on you now as we
are using the properties method so we need to get the properties so how do we use it we use the system class
that is system dot get properties and
now you specify it proper days prop is equal to
we have called the method now how do we print it we'll use the same sort command
to print the properties so system dot print Ln and drops now we will call this
in our main method print system properties now let's save it and now
let's run the command here now our Java project has been
created now let's quickly run this on the terminal let's see how does it run so we'll just quickly run this here as
you can see enlarge this you can see here so what we had printed are welcome
to Java project and the printing properties and they have extracted all
the properties of java the whole version whatever properties of
java are there and they have just printed over here our Java project is created and it has
run successfully now put this whole project into Docker
and let's run them through the docker engine now for that
we need to First create a Docker file so let us just name it as Docker Java dot
docker file and now here we need to
give all the commands to run the project
so now here we need to First create the image so we'll first need to give the
base image so let us give them the base image
from Payson so here you need to specify what is your base image that is your
the jdk so here you can also specify the jdk
version if you want to whereas would be fine if you don't mention also
it will take it automatically now it's time to create a work directory
where our Docker file will be stored so let's create a work directory for that
we write work dir and then now we need to give
the path so here you can see Docker and Java project folder is there so we will
take that path itself here and we will specify it Docker slash Java project
and slash my app now our next step is to copy the whole
file into this directory so how do we do that is
you just need to type copy so here we'll just put a dot which means
that it will copy all the files from the source and now we need to provide them the destination which is Docker Java
projector after this we will run few commands through Docker I will add some commands
like run Java C
and our top Java project name that is Java
s dot Java after we use the Run command we need to
use one more command that is entry point and here we will specify Java and
after that we will also specify our class name that is test and then let's
save it now once this is done now we need to run this project through our
docker so for that we will go back to our terminal and here what we will do
is first we need to create an image so for that we will type Docker build and
give the image name as Java test image
and Dot so it will take some time here you can see it has copied all the files
and the Java project whatever it has to be taken and the docker file which had to be extracted and it has created into
an image so now let us see whether it is listed in our images or no for that we
will just type Docker Docker images and here you will see the list of images
being created in this Docker repository so here as you can see we had recently
created Java test image after this we will now run this image
that will get converted into a container and then it executes so for that we will
type Docker run and Java test image
so here you can see the our Java project has been executed successfully
so while we had run previously so it had shown welcome to Java then your
properties the printing properties and here you can see all your properties which are mentioned over here
so this is how you build images through Docker and then you containerize it and
then execute this let's run few more commands
now we had seen the list of images now let's see the list of Docker containers
for that you just type docker DS so this mentions you the running
containers So currently there are no containers now let's see all the
containers which have been created so here you can see there are
almost a lot Docker containers and our Docker container which we had recently
created has been also specified here so now if you want to remove a container
for instance we can remove this Java image for that we will just write a
Docker RM and we will specify the container name
that is Java image try to remove Java project
so here you can see your Java project has been removed if you want to see in
the list so here you won't find your Java project
so here there were three Java projects now if you wish to remove an image what
you need to do for that you need to just type docker RMI that is remove image and you specify
the image name that is let's take any image
let's run so here you can see the test image has been removed now let's
see check in the list so here you can see your test image has
been removed successfully this is how you work with Docker
[Music] why do we need devops as a service
an effective devops as a service strategy enables a business to adopt a
more flexible approach to its markets and bring forth new products and services as the market changes now
devops and devops as a service can co-exist with traditional development and deployment processes this is exactly
why we need devops as a service and its demand is increasing by the day
Now by integrating chosen elements of devops tooling into single overarching
system devops as a service aims to improve collaboration monitoring management and Reporting devops as a
service is a philosophy adopted by many it companies all around the world it is
a culture which every software development firm must adopt because it makes software development faster and
risk-free now what could be the main reason to offer devops as a service
devops as a service helps its customers to migrate their existing applications to the cloud and make it more reliable
efficient and high functioning the main aim of develops as a service is to make sure that the changes or actions carried
out during the delivery of software are trackable it basically helps companies to achieve breakthrough outcomes and
excellent business value from software by simply implementing devops strategies such as continuous integration and
continuous development now that we know what is devops as a service and why do we need it let's go check out some of
its benefits now the first benefit is it enables better collaboration there are immense
benefits of devops as a service some of the prominent benefits one can achieve by implementing devops as a service to
upgrade their legacy systems the first benefit is that it helps in achieving better collaboration cloud-based devops
makes it easier for developers designers and testers to collaborate because of easy accessibility to all the files the
second benefit is that it helps in Faster testing and deployment cloud-based automation testing tools
provide great power to the testers where they can test and deploy a piece of code faster than ever the third benefit is
that it coexists with internal devops it is not necessary to have devops in each and every process of development and
delivery devops mainly helps to reduce turnaround time and increase collaboration among various team members
of the project the fourth benefit is that it reduces complexity we all know that devops
reduces the complexity of data and information flow it means that now a software development can test his or her
code and an operations manager can make changes in the code wherever necessary through code and configuration
management tools which will obviously save overall development and deployment time the last benefit is that it
increases the quality of the product since devops is a continuous process it
leaves no room for enhancements and upgrades at the end of a delivery software unless there are changes in
requirements moving on to the next part of today's session let's discuss the various devops
services in the cloud so in terms of source code management AWS uses the tool AWS code built whereas
Azure uses repositories and Google Cloud platform uses source code repositories
and in the next comparison in terms of packet management AWS uses Amazon elastic container registry whereas Azure
uses artifacts and Google Cloud platform uses control registry moving on to the third difference we
have continuous integration and continuous deployment pipeline AWS Cloud uses the very famous code pipeline for
this whereas Azure uses pipelines and gcp uses Cloud build
now in terms of unit and integration testing AWS uses code pipeline Azure
uses test plans and Google Cloud platform uses spin maker now in terms of managing the entire
project AWS makes use of code start whereas Azure makes use of boards to
keep in track of what's Happening Google Cloud platform on the other hand makes use of third-party options that are
available now in terms of deploying the application into the environment AWS uses code deploy whereas Azure uses
Automation and Google Cloud platform uses Cloud deployment manager
and finally the last comparison is in terms of infrastructure automation your
AWS uses cloud formation Azure uses Automation and resource
manager templates whereas Google Cloud platform uses Cloud deployment manager
with all of this said now let's discuss some of the other companies other than AWS Azure and Google Cloud platform that
provide devops as a service now with the vast number of devops as a Service Company Catering to the needs of
national as well as International clients the task to hire the right provider can be excruciating time
consuming and confusing now to ease your work companies like Sigma Data Systems its vit code Q pixel
crayons Etc are all companies that provide devops as a service now let's talk about Sigma Data Systems
Sigma understands the criticality of each piece of data in today's world and in the Next Generation it was born to
give its expertise in the world of Big Data it has a predefined workshop patterns to understand the problem and
based on this it provides unique solutions to every customer using various tools and Frameworks Sigma Data
Systems is located in United States and Australia and there are about 250 to triple nine employees and they provide
devops as a service at an hourly rate of 25 dollars to 49 dollars so there are
many other companies like Sigma Data Systems that provide various options for devops as a service
[Music] so first of all let's talk about devops
so what is devops now as most of you all might know by now devops is basically a
methodology it's a set of practices to ensure faster delivery of software deployment to stay relevant in the
market these days companies are expected to deploy quality software in defined timelines hence the roles of software
developers and system admins or the dev and the Ops Team have become extremely important and a lot of juggling of
responsibilities happen between the two teams programmer or software developer
is responsible for developing the software so basically he is supposed to develop a software which has new
features new security patches or upgrades and Bug fixes from the previous
installment but a developer may have to wait for several weeks for the product to get deployed which is also known as
time to Market in business terms so this delay may actually put pressure on the developer because you he or she is
forced to readjust his dependent activities like pending or old code new features and products also when the
product is put into production environment the product may or may not exhibit some unforeseen errors this is
because the developer writes code in a certain development environment which might be different from the ones of the
product environment so from the point of view of an Ops person or the operations
team who are basically responsible for maintaining and assuring the uptime of production environment something that
has been sent to them to test which has been confirmed runs is not running on their systems when the code is deployed
the operations team is also responsible to handle these minor changes or minor errors on the code at time the Ops team
may feel pressured and it may seem like the developers have pushed their set of responsibilities to the operation side
of the responsibility wall but as we all know it's all happening due to the inconsistencies in the environment and
no team is actually up to blame now as the company keeps investing more time and money into products and services the
number of servers admins have to take care of also keep growing and this gives rise to even more challenges because the
tools that were used to manage the earlier amount of servers may not be sufficient to cater to the needs of the
upcoming and growing number of servers so we all are in a fix right but what if
these two teams could work together they could break down the sellers share responsibilities erase inconsistencies
in the environments start thinking alike and work as a team well this is exactly what devops does it helps you get
software developers and operations teams in sync to improve productivity and if I
pull out an official definition devops is the process of integrating developers
and operations team in order to improve collaboration and productivity and all
of this is achieved through autumn information of workflows and productivity and basically continuous
everything starting from development to delivery to deployment as well as
monitoring of an application devops focuses on automating everything and
lets developers write small chunks of code that can be tested monitored and deployed in hours which is different
from writing large chunks of codes that takes weeks to test and deploy now that
was all about devops now let's talk a little bit about devops and cloud or
devops on cloud why do we need devops on cloud Now understand that devops and Cloud go hand in hand they can be
implemented individually of course but devops becomes twice as much efficient
and beneficial when clubbing with the cloud it can help an organization deliver new software features much
faster in a more effective manner many organizations try to fix their application development processes by
shifting from waterfall to devops now they have the sound understanding that devops alone won't be that effective so
public and private Cloud Solutions are now evolving together with devops this
brings in products at a faster rate to the market through quick access to development environment and streamline
developer processes infrastructure as code and automation together reduces the cloud complexity and maintenance of
servers and resources which are Ops Team previously was very concerned about the
security also highly increases with automated repeatable processes that serve to eliminate error that can cause
further issues and even more importantly it builds security controls from the
very beginning to the very end of the process now because you don't have any servers and your continuous operations
are cloud-based it also eliminates a lot of downtime and last but not the least
scalability which is one of the most important factors for applications as they are developed when devops and Cloud
are clubbed together it reduces the cost of infrastructure and Global reach also increases with this now that you know
why you need a cloud platform let's go ahead and look at our Cloud platform of
choice today which is ews so what is AWS now back in the day handling and storing
data was way different than it is now companies preferred storing data using private servers and that was obviously
for security reasons but however with better usage of the internet the trend
has seen a paradigm shift for Industries as they are moving the data to the cloud now this enables companies to focus more
on core competencies and stop worrying about storing and computation for
example back in the day if a streaming platform or a search engine platform
anything with a high volume database suffered a corruption it would take days
before their operations resumed they would face problem scaling up and only
then would they realize the need for a highly reliable horizontally scalable and distributed system is what they need
but now with cloud services and public Cloud platforms this wouldn't be a
problem now since every company has started to adopt cloud services it can be claimed that the cloud is the Talk of
the Town and AWS in particular is the leading cloud service provider in the
market AWS which stands for Amazon web services is an amazon.com subsidiary
which offers Cloud Computing Services at an extremely affordable rates therefore
its customer base is strong and it targets everyone from individuals to startups to take joints running the it
landscape if you might wonder what cloud computing is is basically the use of
remote servers on the internet to store manage and process data as opposed to an
actual physical server or a personal computer to do the same as we are talking about AWS it is kind of an iaas
or in infrastructure as a service which basically gives you a server in the cloud that you have complete control
over in IAS you are responsible for managing everything starting from the OS
completely up to the application you are running so now that we have discussed about devops and AWS let's move on to
our CI CD pipeline now what is a pipeline or CI CD pipeline it's nothing
but a series of steps that must be performed in order to deliver a new version of the software at its Bare
Bones at its most basic you have your build test and deploy stage the CI CD
basically stands for continuous integration and continuous deployment so CI is basically continuous integration
which basically means bringing together all the developers working copies to a shared main line all the developers
working on Parallel branches of a certain upgrade of an application merge their changes into one main branch and
CD stands for continuous delivery and deployment now while the CI includes
building and testing of your application continuous deployment is about the
processes that have to happen after the code is integrated for the app to be delivered to the users these processes
involve testing staging and deploying the code so at the end of this session what we aim to do is build a CI CD
pipeline for our demo app on AWS so for that we will have to look at a few
components of AWS so devops when implemented on AWS becomes a lot more
efficient and effective for a productive life cycle and the steps that are involved in AWS devops are code Comet
code pipeline code build code deploy and optionally code star so first of all we
have AWS code commit which is a fully managed Source control service somewhat
like GitHub that hosts secure more and highly scalable git based repository
without the need of operating the system it's mainly designed for developers who
are supposed to store and version their code securely and reliably for example
you have your it administrators that store their scripts and configurations and your web designers who can store
their HTML pages and images Etc the code commit is fully managed has
great availability is secure scalable and patens up your development life
cycle for people not aware of code Comet think of it as versioning in your S3 but
how it's different is that S3 supports versioning but not collaborative file tracking features which could commit
obviously does it manages batches of changes across numerous files made by
multiple developers parallely how it works is that you create a repository in
AWS code commit service via the console or CLI and later using git from the
development machine you can run git clone to connect the local repository and the AWS code commit repo you can
then modify your files on your development machine via the local repository and then run git add git
commit and push it to the AWS code comment repository as you do with GitHub
again like get even a git pull can be used here to synchronize the files in
AWS code commit repository with your local wrapper which ensures that you are working with the latest version of the
files next on our list we have AWS code pipeline which is a combination of
continuous integration and continuous delivery services for a quicker and more reliable infrastructure and application
updates it automatically builds tests and deploys a user code Whenever there
is a code change and it is completely based on user-defined release process models it also integrates with AWS
services like AWS code Comet Amazon S3 code deploy elastic Beanstalk Ops works
and AWS Lambda you can configure the pipeline either with your CLI or your
graphical user interface and like most services on AWS even with pipeline you
only have to pay for what you use all of this is great and all but why should you use code pipeline simple by automating
your software build test and release processes AWS code pipeline enables you to increase the speed and quality of
your software updates by running all new changes through a consistent set of quality checks it automates your release
process it speeds up your delivery with quality it allows you to choose your
tools of choice establish consistent release processes and provide a pipeline
history detail as your source of Truth code pipeline basically breaks up your
workflow into a series of stages like your source Bill test and deploy and
gives you a revision option as a Deployable content each stage can process only one revision at a time even
though multiple revisions can be processed in the same Pipeline and each stage will have at least one action to
be performed which is some kind of task performed on the artifact once all of
the actions that are configured in a stage is complete the stage is considered as complete after stage is
complete it transitions the artifacts created in that stage to the next stage of the pipeline where you can manually
enable or disable it so it prevents changes from running through an entire pipeline an approval action is granted
only by the IAM user and if an action fails it does not transition to the next action or stage at all so when a
developer completes her working on his code he or she commits it to the source repository and code pipeline
automatically detects the changes and builds those changes after that the build code is deployed it to the staging
server for testing and then additional tests such as integration or load tests are run by the code pipeline once all
the tests are run if the code receives manual approval then AWS code pipeline
deploys the tested and approved code to the production instances also every time
when a user creates a pipeline the code pipeline creates a folder for that pipeline in an S3 artifact bucket in
that particular region to sort input and output artifacts next on our list we
have AWS code build now this is a fully managed build service in the cloud which
compiles your source code runs unit tests and produces artifacts that are ready to deploy it eliminates the need
to provision manage and scale your own build servers it provides pre-packaged build environments for the most popular
programming languages and build tools and skills automatically to meet your Peak build requests now one prerequisite
is that but you must provide the AWS code build with a build project which should include information as to where
to get the source code from build environment build commands and where to store the build output how it works is
that the code build will use the build project to create a build environment AWS code build then downloads the source
code from the build environment and performs tasks that you would specify in the build specifications if there is any
build output the build environment uploads its output to the S3 bucket and
while the build is running the bill environment sends information to code build and cloudwatch logs you can use
the code build console AWS CLI or AWS sdks to get summarized Bill information
from code build and detailed build information from cloudwatch logs as well now that was all about code build now
finally let's talk about code deploy now AWS code deploy is a service that
coordinates needs your application deployment and updates across the fleet of AWS ec2 of any size it automates your
code deployment to any instance handles the complexity of updating them it also
avoids downtime during application development and rolls back automatically if failure is detected apart from that
it also integrates with third-party tools and AWS to make your job easier it
has six primary components to be specific you have application revision compute platform deployment group
deployment configuration and the code deploy agent now the basic skeleton of
how your deployment workflow pursues is as you can see on your screen you
basically create an application with a unique name and set up a deployment Group by specifying the instances to
which you want to deploy your application and the deployment type if you're using the Lambda platform you
just deploy your deployment groups name followed by which in your deployment configuration you specify the success or
failure condition of deployment and to how many instances you want to deploy parallely then you upload the
application specification file to the S3 and deploy your application as specified
with the help of your code deploy agent which is running on each instance on an
ec2 platform or as specified in your specification file to the deployment group when you're using a Lambda
platform and finally you check your deployment results and if you face any bugs or issues you can always roll back
and redeploy finally let's discuss a little bit about codestar which is
basically a cloud-based development service that provides tools that you need to quickly develop build and deploy
applications on AWS it's basically a very templatized format where you start
developing on AWS in certain minutes and you could choose from a variety of project templates to all of the software
delivery is easily managed and you can work across your team very securely using the code stuff and apart from all
of this you also are provided with a project management dashboard to monitor your application continuously all you
have to do as an AWS code star admin is create a project and add users your
users or team members will come at changes which in turn will be built and deployed and through consistent
monitoring of the application if there are any updates required or any bugs the
developers take a decision make updates fix bugs and then the loop closes back
in on the team and that's about it the development process takes little to no
time using Code stop now that we have spoken about all of these different components of AWS devops I hope building
the CI CD pipeline would seem a tad more tangible to you if you're new Learners
you can always sign up for a free TI which is free for the entire year obviously there are limitations to all
of the services Beyond which you shall be charged but that rarely happens so if
you're starting out with AWS and want to try out all of its features and services the free tier is a pretty good idea we
already have a video on how to create a free tier on the AWS console if any of
you want to know how to create a free account you can just go ahead and check that out so we're going to be building a
CI CD pipeline using AWS using Code Pipeline and I'll try to be as slow and
verbose as possible as comprehensive as possible so as to help you guys to
follow me each step of the way so what we'll be doing here is that we'll be creating a demo application platform as
a service application and we shall be deploying it using the code pipeline so let's go ahead and create our
application first so to create an app we will be using the Amazon elastic
Beanstalk which basically has a bunch of templates ready for you now since this is a demo about code Pipeline and not
web development we'll be creating a pretty rudimentary app so let's go ahead and type elastic Beanstalk using elastic
Beanstalk we can make simple apps very very quickly and if you have already created an application created a few
applications they are going to appear right here on your screen but since we have created no applications using this
particular account this is a screen that will greet you so we're going to go ahead and click on create application
so let's call this deployment app we're not very original are we we're not
going to put in any application tags you could put them if you like but currently they are completely unnecessary on
platform let's pick PHP and it automatically fills up the platform Branch inversion with the latest ones
available then I'm just going to be using the sample application code and click on create application now this is
a platform as a service app so basically once I click on create application I
wouldn't have to worry about any background processes or creating the infrastructure elastic Beanstalk is
going to do that for me on its own
and here it's going to show you all of the steps that are taking place like create environment is starting using
elastic Beanstalk as Amazon S3 storage bucket for environment data so on and so
forth now this will take some time so let's go
ahead and parallely do something else I'm going to open another tab
another AWS console okay thankfully you did not ask me to
sign in again so while my app gets created what I'm gonna do is I'm gonna create the pipeline so I'm going to type
in code pipeline and the Funda is simple even here guys
you're just going to put in certain details and your pipeline will be created for you now this is your
dashboard this is where you all of your recent pipelines will appear since in this free tier we have no pipelines
created hence you have no results to display so first things first I'm going to create a pipeline so click on this
big orange button which says create pipeline I'm going to give the pipeline a name let's just call it
demo pipeline so as a default I'm just going to fill
in a role name AWS
code pipeline service role
us East
demo pipeline then click on next so for Source provider I am going to use
GitHub and for that I'll have to connect to GitHub
and allows me to authorize the code Suite
okay
okay once you have successfully authenticated the account you can go ahead and search for the
repository here I'm going to choose the demo hyphen code deploy app which is
actually a sample app I have forked from the AWS code pipeline repository you can
go ahead and look for it as well it will be available in the AWS code pipelines
GitHub account yeah and in the branch let's select Master branch
and click on next Now we move on to the build stage because I have nothing major to build I'm just going to go ahead and
pick skip build stage then you will be prompted with a notification which will
ask you if you're sure to skip your build stage now since I have nothing major to build yes I am going to skip
this build stage and for deployment here we are going to choose a deploy provider
so here I'm going to use elastic Beanstalk and put in my application name and my
environment name now our application name was deployment app and our environment name would be deployment app
environment this is something which gets created automatically so we're going to go ahead and click on next and finally
we are at the review stage where you can go ahead and review all of the details that you have just put in for your code
Pipeline and I am just going to create this pipeline it's pretty simple to do
now this might take a few moments so kindly be patient
and then you are greeted with a notification which says success congratulations your pipeline has been
created now here you can go ahead and release changes if you want our IM role
is in place that we had created earlier our pipeline is a success now all we have to do is wait for our application
to get created and then we can go ahead and deploy our application
all right with that our application has been created it has okay health and you
can see the platform here you can see the running version you can see all the details all the recent information
regarding the app now this is a platform as a service it basically runs all of
your processes in the background now if you would have gone ahead created this app configured your IM user configured
everything else that was needed this would have taken you a lot of time but because you are using a platform as a
service feature of AWS this entire process is automated and your work is simplified so if you go down and look at
the recent events one thing you can see is your instance deployment is completed successfully just in the line above that
you can see that your new application was deployed to running ec2 instances so
basically an instance is running on ec2 so let's go ahead
and go to ec2 click on instances running there is one instance running obviously
we all know which one that is and this is your deployment app let me go ahead
and click on it here you can see the instance State it's running and your
pipeline your Source has succeeded your deployment has succeeded you go here you
can go down take a look at your IM role your subnet ID
so with that your instance has been deployed and you realize that your demo pipeline has worked
[Music] thank you so why should you actually go ahead and
consider this amalgamation of AWS and devops I mean I talked about AWS in
particular where I said that it is a popular cloud service provider which provides you with various Services devops again is an approach which um
lets you have a different look at building software applications so why
why combine these two well actually these two go very much hand in hand I mean AWS provides you with the meat or
with the metal to actually go ahead and build the applications and support devops approach in a very good way let's
try to understand how AWS and devops go well together
again this is a piece of text I do not believe in general definitions I like
keeping things simple so I'm going to talk about these things in a very simple perspective again
let's try to understand these pointers straight away again when you talk about AWS cloud formation
why do we need it we've already discussed what AWS what devops is that is it supports continuous integration
and deployment that means you'd be developing um you'd be having various instances
you'd be dealing with a lot of resources so how do you keep track of all these things so there are two resources or two
services that AWS has to offer to you which would actually simplify your job a lot now AWS cloud formation what it does
is it lets you create Json templates basically those templates would let you
kind of group your resources or the services that you use into a bundle so
that you do not have to worry about keeping a track of all those things in simple words or nutshell that is what it
does it lets you have a template for your application which can be recreated again and again so that you do not have
to do repetitive task and you are actually a classification or your organization Works actually becomes
simplified the other service which I also want to talk about is AWS Cloud watch now it is
something that lets you monitor your applications or your services in a much better way that means whatever happens
on AWS it can be tracked by using AWS Cloud watch so you know what happened at what time that way you can keep track of
all the developments that have taken place on your application building process
then you have something called as AWS Cloud pipeline or code pipeline data
sorry so what AWS code pipeline does is it actually helps you automate the process of deployment
and again integration that means you'd be having a pipeline which lets you put
in data so that you can put that data or move that data to your instances or in simple words your servers so that you
can have replications and you can run various applications on various instances or servers parallely
now uh for people who have worked in this domain or for people who have worked maybe even on Big Data or
something like that they might have come across the process of pipelining and they actually know how difficult it is
to create pipelines I mean you have to write scripts for that this is where AWS simplifies your job it makes sure that
pipeline creation is very easy and that is why the process of devops becomes
much easier with AWS again AWS instances are something which are very important
and the reason they're important is you would be dealing with a lot of resources
so how do you do that you have AWS instances which let you run any kind of machine whether it's a Linux Windows you
can actually go ahead and scale it up scale it down the way you want to suit your needs and this happens at mere
clicks I mean think about just upscaling a particular instance all you have to do is just go ahead and say that I need a
larger instance and AWS does that for you and that too with minimal rates as well so all in all it is a win-win
situation for you that is why uh when you talk about AWS instances in particular they play a very key role in
building a fault tolerant and scalable applications so these are some of these Services now
as I mentioned some of these Services if you actually do go ahead and research AWS website you would have so many
services that actually help you go ahead and perform devops with ease and what
AWS does is it lets you have all the devops approach incorporated into AWS
thus making AWS a very potent or a very powerful cloud service provider to have
with you and as I've mentioned there are various other services as well but these are some of the key ones which I wanted to
focus on and I hope that by talking about these things I've Justified my point as in why AWS devops forms a good
combo now let's move further and try to understand AWS certified devops engineer
as in let's try to understand what would this particular individual do and why is
this particular certification valued
yes when you talk about AWS certified devops engineer well if you've ever taken a look at how AWS certifications
work you realize that they have some associate level certifications and devops on the other hand is an advanced
level certification where you'd be expected to have advanced knowledge of maybe solution architecting your admin
roles and your development roles and once you combine these roles and then you move further you get into advanced
certification which is called as devops it brings in your operations that is your admin teams approach and a
developer's approach together so you would be expected to go through the basic certifications and then combine
them into this particular role so if you talk about these individuals they're actually responsible for maintaining monitoring
and building applications and probably you can actually go ahead and make careers in either of these individual
domains as well given the fact that if you are certified in this particular domain you would be a well sought after
resource resource rather so what are the skills that you actually need to go ahead and do or what are your
responsibilities that you would be concerned with well first and foremost you are expected
to implement and manage continuous delivery and deployment which again is a
key if you talk about devops and then you need to understand how this implementation works and you need to
understand a very important process called as automation because time is a constraint here and automation is
something that helps you plus as an individual you need to have an understanding of what access rights you have what access rights customers should
have and all those things and accordingly you have to go ahead and build a particular application
Define and deploy monitoring metrics and logging systems yes monitoring is an important part as we move further we'll
be discussing that as well do not worry and then you would be expected to build
fault tolerant and scalable applications finally it says that you should be able to design manage and maintain tools that
help you automate and build these applications and guys an important point to note here is
you should be actually having an understanding of doing all these things
on AWS and the reason you need to have an understanding of this particular thing is as we move into the demo part
you'd be understanding how easy AWS makes devops approach
so guys let's move further and do one thing let's just go ahead and see how we can use AWS to do something that relates
to devops now guys while I do go ahead and perform this particular demo if I've
missed out on certain pointers that is we haven't talked much about AWS certified devops administrator so we
would be discussing that as well and parallelly I would be going through certain pointers if I feel that I've
missed out on talking about something so let's quickly switch into the demo part and see how we can utilize AWS and
devops together so guys this is what the AWS console
looks like for people who do not have an AWS account I would suggest that you actually go ahead and create an account
AWS gives you a free TR account in which you have free tier limit where you can
use certain AWS services for free for one complete year now again I won't be getting into the details of how can you
create an account you can actually go ahead and take a look at the other videos that we have on YouTube and you'd
get a clear guidance as in what are the steps that you need to follow to create an account once you actually do go ahead
and create this particular account this is the Management console that would be there at your disposal and it actually
does wonders let's walk through that first and then move further and take a look at the demo part as well so these
are the services that AWS has to offer to you you can see that these services are cons structured or built in such a
way that they meet particular needs like these are all the compute services that are there at your disposal you have some
blockchain Services you have management and governance service Services you basically have your analytics services
and various other services again we would be dealing with code pipeline
today so let's stick to that so let me give you a picture as to what we are going to do now firstly we would be
building an application so we would be using a template application to do that we would be using elastic bean stock now
what elastic bean stock is and why are we using it once we create the application we would be having some five
to six minutes of buffer where the application takes to build and in that
time you'd be actually going ahead and discussing what why we used EBS and all those things not EBS elastic bean stock
so once we have an application I would be using Code Pipeline and then I would be basically deploying my application to
my instances so guys let's just go ahead and create an application first to do that either type elastic Beanstalk here
and you'll be having that or you can just click on your history if you've used that in your recent times
so guys this is what it looks like I've refreshed my account so that it starts fresh so you'd be getting started here
else it gives you the details about the applications that you've created having said that let's just get started
so guys creating an application in elastic Beanstalk is very easy because it is platform as a service when I say
platform as a service it provides you with a platform on which you have ready to use templates you just have to put in
your code or use the existing code to build an application so the process becomes simplified again this is where
AWS comes into picture because it simplifies your task we'll be talking about this particular thing in detail
but that is after we create the application first let's give it some name say demo
it's a fairly odd name but let's stick with it demo sample app is what I'm
going to call it platform again as I've told you it's a platform
as a service so you get to choose a platform so let's say we want to build it on maybe PHP
do I need to use my code right now I would be using a sample code so that it stays short and simple you can actually
go ahead and use your own quotes Hazard if you have any plans and then you just click on create
at times these processes take time guys because the reason it takes time is it would be summoning an instance creating
an instance and as you can see there's a log here it would be telling you what is happening in the background so guys what happens
exactly here is uh an instance is created um again you have security groups that
are associated with it plus you have IM users and all those things that happen in the background so
that is why this process takes a longer while now again it being platform as a
service now if you all know what cloud computing service models are you to understand that this particular service
that is a pass service gives you a platform which is nothing but a ready to use template in which you have to put in
certain details and then create an application so again if you were to create an application on your own you'd
be actually creating your own instance your own Security Group you'd be creating your own IM user and then you'd
be putting in your code and then you'd be developing in an application but this is where past service comes into picture
it simplifies this process of creating application it gives you ready to use or
templatized platform which you can actually go ahead and use to build an application now again this building
process might take a longer while guys so let's move further and build a code pipeline meanwhile or at least fill in
certain details so how do we do that again we come back we click on
services and then we need code pipeline which is here in my history so I would be coming
here and switching to this particular part so meanwhile our application is up and
running or it's getting created what we are going to do is we are going to go ahead and try to create a pipeline so how do we do
that we click on this icon create pipeline okay it says do you want to use a new
console I do not want to I would be happy with using my previous console as well if you need a better experience you
probably want to switch into something new you can do that as well you can always have this option of coming back
to this particular interface as well so guys um pipeline settings I need a
pipeline name so let's call it say
sample pipeline there you go
now it would ask me do you want to use an existing role if you had one you could have used that if no you can
create one so by default that is the one and this is the role name that it is creating allow AWS code pipeline to create a
service so that it can be used with the new Services what is asking or the pipeline do you
want to do that I would say yes let's stick with it and again where do you want to store your artifacts Well for
now let's store it in the default location when I set the default location what it does is it probably creates a
bucket if there's none and then it stores data into it you can actually go ahead and specify a particular bucket as
well in case if you have certain data that you've already created and put it in a bucket you can actually go ahead
and customize the location and change it here for now let's stick to the default one so that it creates a bucket for this
particular job and then I say next Source provider guys we would be
actually going ahead and deploying our application so for that I need a piece of code which I'm gonna go ahead and
pick from GitHub now again I happen to Tumble or fall across a particular piece
of code and I would be using that I'm not sure it is the best piece of code but that should do my job for now so
let's just go ahead and say GitHub because I want to pick it from there connect to GitHub I have an account on
GitHub so I would be authorizing that account it has authorized that account here and again it would be asking me for
that particular GitHub file or the repository now repository is nothing but a piece of code that is there which you
can actually copy and use for your own SEC or make a duplicate copy of it so I'm actually gonna go ahead and make a
copy of it so I probably have this particular
link here I'm going to copy it and paste it not very guys I would be sharing the
link with you people as well once the session goes on YouTube you can actually have that link as well so this is what
it says it is some old account that is why it says Vishal service now because I used
to work on servicenow back then and again Branch pick it from the master branch is what I would say yes there you
go GitHub box stick with it no problem so when I say next an application would be
created here no wait we have to actually go ahead and given a build stage in this case we don't need to do that so I'm going to
say skip yes and I need to find a deployment provider
which for which we are actually going ahead and creating an application that is our AWS elastic bean stock there you
go guys so that would be approved application name uh as is
demo sample app if I'm not wrong yes and the environment is demo sample app
EnV I hope I've entered this data correctly but that is what it should be guys let's check whether our application
is up and running I'm gonna refresh this particular piece of yes guys it's green
that means the application has been loaded now again one important point that I would like to mention is I talked
about everything getting created in the background so you can have information about everything that is happening as in at what time what happened
they've checked the environment tell us in whether the instances are up and running uh whether the instance was launched successfully how was the
application or whatever the processes that you implemented everything see it was waiting for ec2 instance to launch now that is a type of an instance which
was launched we would be taking a look at that as well but see it has notified saying that it might take few minutes so
meanwhile this was happening we actually gone ahead and put in the details for our code pipeline so guys this
application is up and running and our code pipeline is also almost
up and running or ready to go so let's say next it would give you a peak as in okay
these are the details that you've entered do you want to commit these details and I would say please create a
pipeline guys while it creates this particular
pipeline let's just go ahead and take a look at
okay no wait I believe it has done the process quickly so we can actually wait
for half a minute maybe so guys first your Source would get into
picture and it would see whether the source jobs are done that is getting in the GitHub repository and then actually
going ahead and fetching in that particular repository so it has done that now the next step is to deploy this
particular application so guys meanwhile this application gets deployed on my instance I would be showing you that do
not worry let's just take a look at um something else now
if you type AWS certified devops engineer
you come across a Eureka's life that is not something that you want to see if you
talk about AWS this is the page I would want you to visit
for now it is in some other language it asked me whether I wanted to translate it I said no you can actually go ahead
and translate it to English if you know English if you know this language you can do that as well so guys here's the information where you can actually go
ahead and find out as in okay what are the roles and responsibilities how does the exam Shape Up what are the um what
is the fees how much time do you need to invest in all those things so you can visit this website and get all the
information again if you're looking for a structure training there are people who normally prefer structured training
in that case you can actually go ahead and try out this website as well where at redirect we have this course which
would be it has already been launched but the first session would start somewhere in the end of the month and
you can actually go ahead and take this particular session that the fees is very less and probably if you stay tuned with
Eddie raycast Channel you might get more dis discounts as well because this price is very real time
again you can get all the information as in what is the course curriculum and stuff like that what are the topics that
would be covered duration and all those things so if you wanna take in or take a structured approach which is a very good
thing to do if you want to actually go ahead and succeed in the certification this won't be that bad of an option
let's just go back and see whether our pipeline is up and running as you can see guys the code has been
deployed so you can actually go ahead and make changes to this particular stage guys when I say continuous deployment and
integration you can see you can you always have an option of editing where you can actually go ahead and make changes to the piece of code that you've
uploaded you can actually go ahead and make changes to the storage monitoring rules and all those things the IM user
that you've attached so everything can be changed and that is why AWS makes this devops process very feasible
because it helps you automate all these processes so guys now that our application is um
up and running and the code has been deployed let's see whether it reflects uh so how do we check well you can just
come here and click on this service go to ec2
probably there should be some instance that was launched by our application you can see one instance is running
demo sample so this is our instance right so if I just go ahead and copy this IP address here
if I just paste it here if I click on it it says congratulations pipeline has
been created in your instance through the S3 bucket has been deployed now there's one error here it
says three Amazon ec2 instances the reason it does that is because as I've told you I have picked up a code from
GitHub and the fact that that person might have coded or hard coded this particular piece of application like
that or the notification like that so you can actually just go ahead and go to your code pipeline
and you can make changes to the application that you've built so I'm not going to go ahead and make changes to it
but I'm just gonna tell you like how would you do that I mean what is the process that you actually need to do so
you can come here you can say edit you have this option of editing your Source you have your GitHub repository
which you can actually again go ahead and make changes suppose you have
this particular session you come here you say edit so you have your Source
right so again this piece of code where yeah you've actually gone ahead and
connected this particular thing so this is where your repository lies right so this place if you come here and you open
your particular application you realize that there's an index HTML file
in that place there must be code somewhere which says okay see this is where the message has been displayed
saying that this is what has happened so you can actually make changes to the code you can update it and once you do that you just come back here you commit
those changes guys and again your continuous deployment your integration it continues
[Music]
what is continuous integration and delivery now basically this approach comes into picture that fills in this
world a little bit what this approach does is it basically helps you split the entire code into smaller chunks and
segments now these are manageable pieces of code which can be actually integrated
continuously and can also be deployed or delivered continuously so there is a
constant development process and if there are any changes that are committed by a particular developer or made
changes or certain changes that are Incorporated by the operator those can be committed right away and you have a
version control system that basically ensures that the changes that were committed the latest those changes or
that copy is made available to everyone so that there is no ambiguity or clashes between these two teams again how does
this approach work exactly well basically certain processes are automated you have a source code
repository or think of it as a Version Control infrastructure where you can put in all all your codes and the versions
of these pieces of codes are maintained again you have something called as a build server and a mechanism that
ensures that your software gets tested continuously so all these things ensure
that your software is built or your code is built it is compiled it is reviewed
it is unit tested integrated packaged and this process happens continuously so
that the process of developing software becomes easier faster and optimized so
this is what continuous integration and delivery does now next we have something called as or a main topic of discussion
that is AWS code Commander again I just talked about AWS I just talked about devops and now I am talking about AWS
code commit so you might be wondering as in what is the link now devops being a popular approach and AWS being a popular
cloud service both were in demand or both are in demand so when you talk about devops and when you talk about AWS
people one doesn't can you implement or carry out this devops approach on AWS well AWS responded to this request with
plethora of services or a set of services basically that ensure that you
could actually go ahead and carry out the process of continuous integration and deployment on cloud as well and one
of those Services is AWS code commit now in the previous slide I talked about
source code repository where you actually go ahead and dump in your versions of codes so when you talk about
this repository in general you have something called as git which is a Version Control infrastructure now on it
you can actually go ahead and push and pull all your codes or these codes or pieces of codes are called as
repositories basically so you can do that on get what AWS thought was what if
we could have this kind of version control device that is in-house or that is something that runs on the cloud
platform so they came up with a solution to this and they had their own version control device which was called as AWS
code commit well to go with the definition when you take a look at AWS code counts definition it is a Version
Control service which is hosted by AWS that you can use to privately store and manage assets when you talk about these
assets these can be documents your source code your binary files and you can actually go ahead and do all these
things in the cloud so yes as I've already mentioned it is an in-house repository or infrastructure rather that
lets you host or hold repositories so yeah this is what it does it basically
gives you an environment where you can actually go ahead and commit your code push it pull it what are its benefits
basically well what it does is it lets you store any type of code I mean there are very less restrictions on the type
and the extensions of course that you store and you can do that anytime and
the other benefit that happens here is it is highly secure your code that you push and pull it is basically encrypted
so security is something that you'll not have to worry about it ensures collaborative work now what do I mean by
this now when you talk about collaborative work yes I talked about code commit where you can actually go ahead and push and pull your piece of
code so the fact that you can do it continuously and you can give proper access to people who can access this
piece of code who can make changes to it under different IAM users and under different security groups this process
becomes very collaborative very engaging and many people can work on it commit on it or comment on it rather and actually
go ahead and suggest quite a few things scale easily now that is the other benefit where the pieces of codes or
your infrastructure that you have when you talk about Cloud platform and any service it ensures scalability and so
does the amount of code that you push on this particular service the best thing about it is it places your code in such
a locations or such places that it becomes very easy to integrate with your third party tools as well so Guys these
are some of the benefits of using AWS code commit so guys I've gone ahead and I've
switched into my AWS console so in this session we are going to go ahead and create a repository on code commit we're
going to push a piece of code we're going to create a branch for it and probably commit certain changes to it as
well to do that you have to set up your GitHub repository or basically you have to set up your code commit repository so
how do you do that you have to go ahead and create an IM user and a security group or a group basically under which
you can have those users which have particular access to these resources so I'm going to create one single user
which lies under one group only let us see how do you do that to do that first type IAM you can search it here and hit
the enter button once you do that guys there is something called as users here go there and say create a user let us
call it CC that is code commit demo
and I'm gonna give it AWS Management console access only for now it says
create a group for that let's say okay let's create one so let's give it a name as well say CC
underscore group and we would need to select access policy so AWS has some predefined
policies we can use those I'm gonna give full access for now if you are using
maybe more secure data or something like that you might want to customize your policies as well which you can do for
now let's stick to the basic one and say create group so the group is created guys now let us
just go ahead and say next key value pair I don't need it for now next again
and there you go guys a user is created so when you actually go ahead and use
this particular user you can decide what kind of access do you want to have to your code commit repositories do you
want to access it by SSH you can use SSH as well and https as well so in this
session we are going to keep it simple https you can do it by SSH as well but that is not for today's session in the
next session probably we would be talking about how can you connect to your repository through SSH so let's
just go ahead and switch into the code commit service so then you have your code commit
basically guys let us just go ahead and create a simple repository first I say create let's name it say
sample repository I'm very bad at naming conventions guys so forgive me and
description for today's demo the reason we put in these details is so that you can differentiate
between different repositories because while working on projects you might have various repositories
I'm signed in as a root user so I cannot SSH is what it says we won't be doing
that anyways we're gonna go ahead and HTTP it for now here are the details if
you wish to avoid SSH you can actually go ahead and go through the details as well or if you wish to don't want to go
through all this process you can learn more about it here basically this is credential helper now our repository is
empty guys we've just created a repository so let's create a demo file for now let's say welcome to the session guys there would
be an actual code here so thus being a demo part I won't be getting into the details so this is not how the files
look actually yeah there you go so guys in our past two to three minutes we've actually set
up a code commit group user and we've created a repository and now we are putting in data into that thing so let's
call this file say demo.txt author name let's stick to my
name there you go just putting in a comment
and there you go guys my repository is created and there is a piece of code that has been pushed it has been pushed
to the master Branch you can create a pull request as well guys if you wish to if you go to the repositories you can
clone this particular piece of code as well if you wish to clone https SSH you can do that as well if you go to the
repositories part you would see this repository here and you can actually go
ahead and do quite a few other things as well you can create branches of it as well say for example I come here and I
say create a branch let's call it say demo Branch to Branch from master and I
say create a branch and the branch should be created guys right away you can select that open that and that too
will have a demo text file right so if I just say edit it
I can come here and I can just go ahead and say changes were made there you go and let
me just scroll down let's say author name is again my name
my email address and I say commit changes
so there you go you've actually gone ahead and made changes to the branch this is my sample Repository
you can switch here and you can actually go ahead and make
changes to the master Branch as well and if you wish you can actually go ahead and pull this particular piece of code
and you can just compare it with your demo Branch as well compare
it's mergeable it says you can actually go ahead and say create
okay so now if I just go back to the branches I come to my master branch
it looks like this and if I go to my sample Branch if I go to the
repositories part and then to the branches
I can come here I can see all the changes that are here so if I actually go ahead and make changes to my master
branch which I did not do in this case I just made changes to my sample repository if you actually go ahead and
go back and make changes to your master Branch or any of the branches and then you actually merge those two pieces of
codes you can actually get the refreshed copy whatever you are using and the fact that you've put in your email IDs and
stuff like that you would be notified you can even actually go ahead and create a notification group using your
basically you have groups here in AWS that let you actually go ahead and pass messages you can use SNS and stuff like
that using which you can actually pass in messages as well so that in case if certain changes are made you would be
notified in the form of those messages as well so guys there you go in the session we've actually gone ahead and created a repository we've actually gone
ahead and created a branch out of it and we've made changes to the branch as well and then I've actually shown you people
as and how you can go ahead and merge the changes that are there and have the best copy with you so that you can actually go ahead and implement the
freshest of copies that you wish to use [Music]
so guys when you talk about AWS in particular you have something called as AWS code pipeline which lets you
simplify this process it lets you create a channel or a pipeline in which all
these processes can be automated so let's take a look at those processes as well first let's get through the
definition part let's see what it has to say I would be blankly reading this thing and then probably we'll be having
the explanation part that follows so as the definition says it is a code pipeline which is nothing but a
continuous delivery service we talked about continuous delivery already and you can use the service to model
visualize and automate certain steps required to release your software something that we've already discussed
in continuous integration and delivery so this is basically a continuous delivery service which lets you automate
all these processes so as I mentioned automating these processes becomes very important so once you do use the service
these are some of the features it provides you it lets to monitor your processes in real time which becomes very important because we are talking
about deploying softwares at a greater pace so if this can happen at real time I mean if there is any change and if it
is committed right away probably you are saving a lot of time right you ensure consistent release process yes as I've
told you deploying servers is a difficult task and time consuming task if this can be automated a lot of effort
is saved speed up delivery while improving quality yes we've talked about this as well and we have pipeline
history details monitoring becomes very important guys so what code pipeline does is actually it lets you take a look
at all the processes that are happening I mean if your application is built it goes to the source then it moves to the
deployment all these processes can be tracked in the pipeline you get constant updates as in okay this happened at this
stage if anything failed you can detect as an okay this is the stage where it is failing maybe stage number three stage
number four and accordingly you can edit the stuff that has happened at that stage only so viewing the pipeline
Ministry details actually helps a lot and this is where code pipeline comes into picture so this is what the
architecture of code pipeline looks like it's fairly simple guys so some of this might seem a little repetitive to you
people because the concepts are similar the concepts which we discussed those can be implemented by using Code
pipeline so yes I've talked about these things but let's try to understand how the architecture works and we'll be
using some other terms and discuss some terms in the future slides as well which we've already talked about but each of
these Services they do this task a little differently or help you automate these processes hence the discussion so
let's see to how much level can we keep it unique and let's go ahead with this discussion as well so let's see how the
code pipeline Works basically there are developers as I've already mentioned these developers would be working on
various pieces of codes so you have continuous changes and fixes that need to be uploaded so you have various
Services one of them is code commit which lets you have a initial source management system kind of a thing which
lets you basically take care of repositories and stuff like that so it lets you directly connect with Git I
would be talking about git what get is but for people who know what git is if you have to manage your git repositories
you have a service called as code commit so this is what happens if there are any changes those go to the source your
developers can commit those changes there and then it goes into the build stage this is where all the development
happens your source code is compiled and it is tested then it goes to that staging phase where it is deployed and
tested now when I say tested these are some final tests that have to be implemented before the code gets
deployed then it has to be approved manually and it has to be checked manually whether everything is in place
and finally the code is deployed to the public servers where customers can use it again if they have any changes as
I've mentioned those can be readily taken from them and it goes back again to the developers and the cycle
continues so that there is continuous deployment of code this is an another look at it it is very simple but this is
more from AWS perspective so if there are any changes that developers commit those go to the source now your data is
stored in a container called as S3 that is simple storage service in the form of objects so if there is any change that
has to happen the data is either fetched from the storage container which is S3 and the changes are built and then again
a copy of it is maintained in the form of zip as you can see here there are continuous changes that are happening
and those get stored in the S3 bucket now S3 should preferably be on the region or in the place where your
pipeline is that helps you carry out the process of continuous integration and delivery with ease in case if you are
concerned with multiple regions you need to have a bucket at each reason to simplify these processes so again here
to the code gets to the source it is probably submitted to the build stage whereas the changes happen a copy is
maintained at S3 and then it goes to the staging again a copy is maintained and then it gets deployed so this is how the
code pipeline works and to actually go ahead and Implement all the actions of code pipeline you have a service or
these services that is your code deploy build and code commit in AWS so these
Services actually help you carry out some or most of these processes that are there let's take a look at those services and understand what do they do
so first and foremost you have your code deploy code built and code commit so
this is not the order in which you deal with these things now these things actually help you in automating your
continuous delivery and deployment process they have their individual commitments let's talk about them one by
one first let's talk about code commit which is last in the slide so basically I talked about moving your piece of code
to a central place where you can continuously commit your code and get the freshest or the best copy that is
there right so code commit what it does is it helps you manage your repositories in a much better way I mean think of it
as a central repository so it also lets you connect with Git which itself is a
central storage or a place where you can commit your code you can push and pull that piece of code from there work on it
make own copy of it submit it back to the main server or your main or Central
operating place where your code gets distributed to everyone so that is get and what code command does is it lets
you integrate with Git in a much better way so you don't have to worry about working on two different things it helps
you in automatic authorization pulling in the repositories that are there in your git account and a number of other things so yeah that is what code command
does then you have something called as code built as the name suggests it helps you automate the process of building
your code where your code gets compiled tested certain tests are performed and again making sure that artifacts or the
copies of your code are maintained in your S3 and stuff like that so that is what code build does and then you have
code deploy as I've already mentioned deployment is not an easy task I mean if we are stuck in a situation where we are
supposed to match manage the repositories we are supposed to work on quite a few things in that case if we
are forced to kind of take a look at the servers as well for new instances upon new piece of servers that could be a
tedious task so code deploy helps you automate these processes as well so this
was some basic introduction to these things let's just move further and take a look at the demo so that we can talk
about some of these terms and the terms that we've discussed previously in a little more detail now in one of my
previous sessions I did give you a demo on continuous integration and delivery I believe there were certain terms that
people felt were taken care of in a speedy way hope that I've explained most of the terms with more Finance this time
and in more detail as we go through the demo to I would try and be as slow as possible so that you understand what is
happening here so let's just jump into the demo part guys so guys what I've done is I've gone
ahead and I've switched into my AWS console for people who are new to AWS
again you can have a free tier account with AWS it's very easy you have to go and sign in put in your credit card or
debit card details a free verification would happen and probably you'd be given access to these Services now most of
these services are made available to you for free for one complete year if there is certain limitation on these services
so you have to follow those limitations if you cross those limitations maybe you would be charged but that happens rarely
I mean if you want to get started definitely this one year free subscription is more than enough to get Hands-On on most of the services so I
would suggest that you create this free tier account if you've taken a look at my previous videos you would know that how to create a free tier account if not
it's fairly simple just go to your browser and type AWS free tier and probably you'll be guided as in what
details have to be entered it's not a complex process it is fairly simple and it happens very easily so you just have
to go ahead and do that once you do that again you'd be having access to this console guys so once you have an access
to this console you have all the services that you can use so in today's session we would be working on a similar
demo that we worked in our one of the previous sessions here we would be creating an application a pass
application platform as a service application and we would be deploying that application using our code pipeline
so there would be talking about other terms as well like code commit code different code build so do not worry we would be discussing those as well so
this is what the demo is for today's session so guys let's start by creating our pass application to do that we would
be using elastic Beanstalk which lets you have a ready-to-use template and using which you can create a simple
application now this being a demo guys we would be creating a very simple and a basic application so just come here and
type elastic bean stock so when I come to this page guys if you've created an application it would
show you those applications but the fact that if you're using it for the first time this is the console that you'd be
getting that is why I have created this demo account so that probably we get to see how you can start from the scratch
so if you click on get started guys creating an application here is very easy like extremely easy you have to
enter in certain details only it takes a while to create an application understandable I would tell you why it
takes the time but once it happens it happens very quickly so all you have to do is give your application name let's
call it say deployment app I'm very bad at naming conventions let's
assume that this is good you can choose a platform guys you can choose whatever platform you want say PHP is what I'm
choosing right now as I've told you it's a pass service pass that is platform as a service means that you have a ready to
use platform guys that is why you can just choose your platform and your elastic bean stock would ensure that it
takes care of all the background activities you do not have to set up your infrastructure it takes care of it so once I select the platform I can use
the sample application or use the code if I have in this case I would be using a sample code that AWS has to offer and
I say create there you go guys this thing is creating
my application so whatever is happening here it shows that these are the processes now it is creating a bucket to
store all the data and stuff like that so it would take care of all these things guys it might take a couple of
minutes so meanwhile let's just go ahead and do something else let me just open AWS console again
somewhere else I hope it does not ask me to sign in again I've already signed in so meanwhile that application gets
created let me just go ahead and create a pipeline guys so code pipeline again
as fairly simple guys what happens here is very easy I just go ahead and put in
certain details here as well and my pipeline would be created so do you want to use the new environment or want to
stick to the old one you can click on Old right and you can go back and create it the way it was done or you can use
the previous environment I'm gonna stick with that I was very comfortable with that so let's just stick with it if you
want you can use the new interface there's not a lot of difference certain little or minor differences so you can
just come here and add in the name of the pipeline that you want to create say demo pipeline
I see next Source provider guys I would be using GitHub here because I want to
basically pick up a repository from GitHub that helps me in deployment so I need to connect to GitHub for that it
would ask me to authorize if you have an account you can always do that so that it can basically ring in all the
repositories that you have so just say authorize if not you'll have to sign in once so my account has been added here
guys repository I need to pick a repository this is the repository that I would be picking do not worry I would be
sharing this piece of code or else what you can do is you can just go to GitHub and type AWS Dash code pipeline Dash S3
Dash code deploy Dash Linux now it is a repository given to you by AWS if you
take a look at it and if you type it just the way it is named here from AWS you should get that repository in GitHub
you just have to go ahead and Fork it into your GitHub account and probably you'd be able to import that repository
directly you can see that repository has been for locked here into my GitHub
account you just type the name here this name search it and probably there would be an option here Fork I forged it so it
does not activate this option for me in your case it would be activated you have to just click on it and the repository
would be forked into your account so I'm getting or importing a fork from my GitHub I've authorized my account and
then I can just go ahead and do the stuff Branch Master Branch yes and just
do the next step build provider no Builder I don't have anything major to build so I don't need
to go ahead and provide a build provider you can use code build right guys if you want to move or basically deploy your
code to ec2 instances you can use code build if you want in this case I have an
application in which I have an ec2 instance and stuff like that so I don't need to go ahead and do any building stuff hence no build for me so I say
next deployment provider in this case my deployment provider would be my EBS so
we have that option yes select EBS elastic Beanstalk not EBS EBS stands for
elastic block storage that is a different thing guys elastic Beanstalk make sure you do that application name
deployment app was the name right yep and the environment this is the environment it creates the environment
on its own I believe that it has created the environment it says it is starting I
hope the environment has been created so guys let's just see whether our application is up and running so that
probably I can pass into the details yes the application has been created guys so
let's just go back and select this say next now create an IM role is what it's
saying so let's say sample okay guys so what happens normally is
um an IM user gets created each time you create a role so in this case it is
asking me to create one it's access create a new M role AWS code pipeline
nice here love successful so a role has been created next step now it gives me
the details guys basically it would tell me what are the stuff that I've done so everything is here I don't think I need
to cross check it you might just cross check the stuff that has happened and say create a pipeline so guys the
pipeline has been created here as you can see these are the stages that have happened
if you want you can just go ahead and say release a change now these things are happening guys and let's hope the
deployment also happens successfully it just created an IM user let's see whether it falls in place everything is
in place as far as the source part is concerned it has succeeded and now the
deployment is in progress so it might take a while meanwhile just go back and take a look at this application so if I
open this application guys it would give me an overview of what has happened with this application guys as
you can see these were the steps that were implemented now the application is available for deployment it successfully
launched the deployment environment it started with everything that it was supposed to do like create or launch an
ec2 instance and stuff like that so everything is mentioned here what happened at what time
so this is a past service guys and it works in the background I mean if you actually go ahead and launch an instance
on your own configure IM users configure security groups it takes a longer while but what the service does is it
automates that process it understands that you need an ec2 instance it launches that instance it assigns
security groups vpcs and stuff like that all you have to do is run your application on top of it as simple as
that so it has taken care of everything and run a PHP application for me so yes
this is what has happened here if I just go back here meanwhile let's see whether
our code has successfully run you can see what has happened here I've released
the change as well and you can view the pipeline history if you want you can click on this icon and all the details will be given to you what happened in
what stage so these are the things that have happened till time now guys let's
just go back and take a look at something that we could so I'm gonna come here and say service ec2 because my
app launched an ec2 instance so there should be be an instance created by elastic Beanstalk see one instance is
running it has a keeper attached to it as well so these are the details guys I
have a public IP associated with it if I copy it there you go copy this IP and I
say run this IP you have successfully created a pipeline that retrieved this
source application from an Amazon S3 bucket and deployed it to three instances it did not deploy to three
instances using Code deploy it deployed it to only one instance you see this message that it deployed it to three
instances is because the code or the repository that I used it was supposed to deploy it to different instances if
there are multiple instances and hence this message would have made more sense then but the fact that we've deployed it
to only one ec2 instance it should actually display that message so the message that you're supposed to give you
can actually come back here and make change to the piece of code that you worked on if you go to the readme MD
file I think this is where the piece of code is there you go okay not here
Compares that file that needs to be edited let me just take a look at some other files as well yeah this is the
file sorry so if you go to the index.file here is the message guys so you can probably make a change to this
message instead of saying three you can say one here edit this piece of code and
then you submit the code again so when you do launch or type in this IP address probably that change would be reflected
so guys what we've done is we've actually gone ahead and created a pipeline successfully and in that
process we've actually gone ahead and moved or deployed our application from here so guys in case if I do go ahead
and commit changes to the code that I just talked about those would get reflected right away in my history when
I talk about this pipeline so it does give you continuous integration and deployment so I hope that this session
made sense to you people and we've talked our test upon most of the stuff that I wanted to talk about
[Music] what is AWS code star now in my recent
times I've talked about devops a lot I've talked about different AWS devops Services as well as in how can you
actually go ahead and use devops on AWS I've talked about services like code commit code built I've talked about
Services um that basically let you easily automate this process of continuous integration and deployment
so what is code star and why are we talking about it let us move further and
try to understand this point as well so guys what code star does is it basically brings in different services
that AWS has to offer to you in the devops space like code commit code
deploy code build and code pipeline for people who do not know what devops us and what are these Services well
basically what devops does is it lets you bring in an approach where you can continuously integrate a software
deliver it test it and also deploy it automatically so to automate this
process and to do it continuously you need certain Services certain tools that let you automate and make this process
continuous so devops is this particular approach that lets you do this basically it prints in the developers and the
administrators or operations team together and thus help you automate this process by using various tools now AWS
being a popular cloud service provider it has actually gone ahead and Incorporated this approach of devops as
well so you can actually speed up the process of software development and deployment so these are some of the
services that let you do it you have something called as AWS code commit you have code deploy and you have code built
now code commit is more of a repository where you can actually put in your code and repositories that are there with you
so it is more of a version control device then you have code deploy and code build which basically let you build
the code test it and then deploy it and core pipeline basically helps you have a
pipeline or basically think of it as a channel where you can practice all these things or carry out all these operations
so what is code star and why is it needed I mean yes we already have all
these services with us and the fact that everything is taken care of why do I need a separate service that brings in
all these things Under One Roof well to keep it simple instead of using all
these Services under different roofs and continuously switching amongst them it
is still a complicated process right I mean if you are working on too many applications too many projects then
continuously moving from one end to the other one service to the other can be irritating and can be time consuming so
AWS thought why not bring these Services which are used together under one umbrella now what AWS code star does is
it basically gives you a common dashboard where you can access all these services from one place and you can
accordingly make changes or actually go ahead and perform CI CD that is continuous integration delivery and
deployment uh using this one single service that is AWS code star so this is what it does it basically
lets you develop applications on AWS within minutes it works seamlessly
across different teams and it lets you manage software delivery as well plus you can choose a variety of project
templates now as we move further and get into the demo part you would probably take a look at the templates and the
options that we have but yeah this is what it lets you do so we've already discussed the definition as in what it
is and what it does but this is the basic definition as well which says that it lets you quickly develop build deploy
applications on AWS and it also lets you integrate various services for better
product development and Tool chain management so guys um this is what code
star is it basically brings in different devops services on AWS Under One Roof
and and it actually helps you save a lot of effort and time let us move further
and talk about something else now so Guys these are some of the important pointers that concern AWS code star or
some of the components is what you can say basically what we do is we have our projects teams extensions dashboards now
projects are nothing but the devops projects that you're going to create deploy and work with them teams the
different teams who are concerned with this project the access they have and stuff like that dashboard is a place
where you can actually go ahead and maintain or bring all these Services Under One Roof and then you have
extensions which basically deals with quite a few things as we move further into the demo part I would probably
discuss this part a little more so I'm hoping that by now you have a clearer
picture as to what AWS code star is what all it can do and the remaining terminologies so guys
this is what AWS code star is and these are the components it concerns basically now who are the people that should
actually go ahead and use this particular service well there are no restrictions on it as in who should
actually go ahead and use it but then the service is fairly new now seeing fairly new might not sound very correct
or won't sound that correct the reason for that is it is still there in the market for more than a couple of years
now so it has been there for a while but still there are quite a few things that
still need work and there are quite a few things that have been worked upon already so basically when you talk about
this particular service what happens here is if you take a look at the service a year back or so probably they
did not have a channel through which you could actually see what happened with the projects in the past but recently
they've introduced a proper pipeline flow where you can actually understand as in okay um these are the things that
I've worked on and stuff like that so the services are improving when you talk about this particular service that is AWS code start but yeah there is still
room for some improvement when you talk about businesses for them immediately migrating to AWS code star might be a
little difficult because when you talk about big businesses and organizations they are very rigid and not rigid but
they are working on platforms and unless and until it is very important they won't easily migrate to something else
so I don't see big businesses right away migrating to AWS code star but yeah it
is a positive step and we see code star being used more and more these days little more talks here and there and
probably businesses would soon move to it because AWS is the need of the r and
people are using AWS so they are going to use devops as well on AWS and if you
are an individual who basically likes trying out new things who likes adapting new technologies this is a very good
service to learn because as I've already mentioned in near future as well businesses would be moving to AWS code
star and having this skill would be very beneficial for you as an individual as
well so yeah uh who would use it or who should use it I believe everyone should use it but uh who would be using it I
feel for now beginners or people who are working Standalone or small businesses
would find it easier to basically transition to AWS code star and with time we are gonna sing and by time we
are going to see various pickup businesses moving uh towards this direction as well so yeah again if you
are somebody who likes experimentation trust me this is a very good service to have having said that guys we've
discussed quite a few pointers as in what this Services what it does what AWS is let us quickly switch into the
console part and see what does the service has to offer to you in the form of demo
so guys what I've done is I've actually gone ahead and I've switched into my AWS console this is something I tell in all
of my sessions if you are completely new to AWS and devops and if you wish to
practice AWS or devops on AWS you should actually go ahead and create a free tier account on AWS website all you have to
do is search AWS free tier in any of your browsers and you would be navigated to that particular link open that link
and actually you have to just go ahead and fill in certain details like why do you want to use this account certain
basic details like what your name is what organization do you work for and what is the purpose of you using this
particular service or AWS services and after that one important thing is you would have to enter in your credit card
and debit card details do not worry AWS does not charge you unethically or
without your concern all it does is it provides you some of the services for free for one complete year and there is
certain limitation on these services so if you exceed those limits then AWS has the right to charge you so basically
what you have to do is you have to actually go ahead and first take a look at the limits that these Services aren't and most of those services are in limits
so even if you use them properly or to a greater extent you won't be charged unless and until you leave a particular
server running for like us which is a paid server um you won't be charged so make sure
that you take care of these things and you can also set in alarms where you'd be notified if you're reaching a
particular limit for a particular service in that case you might be alarmed and you can turn that service
off so if you're worried about getting charged do not worry just go ahead and create a free tier account once you do
that guys you would be having an access to this console which is very friendly and nice to use now there are so many
services that AWS has to offer to you if you click on this Services icon you can take a look at all the services that are
there I mentioned that there are somewhere on 100 Services if you just come here and scroll down probably you can access all those services that AWS
has to offer to you there you go these are the services that are there and most of these Services we've talked about
these services in our previous sessions so if you are interested in knowing about those Services you can visit our
YouTube channel and probably look for those Services as well to understand those services to a better effect having
said that guys we are going to go ahead and talk about AWS code star in particular and we are going to see how easy it is to actually go ahead and
carry out all the devops operations with a lot more ease using this particular service
so what do we do we go to the search bar and type AWS code star
it says the service is under that is hard to believe yeah there you go
so guys this is what The Beginner's console looks like I've deleted all the
applications that I had created in the past because I wanted you to have this special look to the service as in how
does it look when um there's nothing or no application running I mean if you start fresh this is what the console
would look like and it and it gets updated every now and then so you can switch to newer console as well if you
want so guys um let's just go ahead and create a project first so what I'm going to do is I'm going to go ahead and
create an application now when you create an application guys I've talked about that you have a project when you
talk about code star I discussed this in the components part so when you talk about these projects basically you have
various options when you create one and you have n number of templates that are available say for example you can see
that there is a project template for go go is a programming language and these are the basic availabilities with it so
you're free to choose the programming language that you want to use you are free to choose the platform on top of
which you want to put in your application you can see that there are options here as well for filtering those things and you can decide what kind of
application do you want to put on top of that particular platform so you have all these options if you scroll down you
would actually take a look at or you can take a look at all the services that are there or types of projects that you can
create so should we choose one of these or should we just go ahead and filter one for ourselves so let us just go
ahead and create a web application guys uh where do I want to put it on um let's say node.js for now
and what is the platform let us have Amazon ec2 which is a basic virtual machine
Rider or I would say a service that lets you create computation instances so yes
we have these three so when we select these three options these are the options that we have let us create a web
application using Amazon ec2 and then it would ask you for certain details guys project name so let us call
it say my sample project Maybe
I'm very bad at naming conventions so forgive me for that and guys with that you would get this ID which is very
important if you want you can edit it right now the reason I'm telling you this is it would be associated with all
the resources that are created in the background so you might want to keep a track of it or if you want to edit it
you can edit it to suit your needs because if you're working on multiple projects at times you need to keep a
track of all these things so once you do that guys you've actually gone ahead and selected a template
you've put in the details which repository do you want to use so again guys I've mentioned that with AWS you
have code commit which acts as a version control device uh for open source projects people normally use GitHub
which is very popular but with AWS we normally prefer using AWS code commit it is an in-house version control device so
let us stick to that for now in case if you wish to know more about GitHub again you can take a look at my previous
sessions um I've talked about these things so repository name this is what it is and let us just go ahead and say
next so Guys these are the processes that are going to happen you have your source which is code commit you are going to
use services not V but this would happen in the background but for build we would be using Code build it would be tested
it would be deployed using Code deploy and you can monitor it with um code star
itself which basically uses various parameters from different services to do that for you once you are done with it
AWS codes are would like permission to administer the resources in short these
Services which I just mentioned so we can just say create a project guys this is important uh we are mounting our
application on top of AWS ec2 and to log into ec2 you basically need a key pair
so you can actually go ahead and visit the AWS ec2 service and on the left hand
side you have an option of creating a keypad it is fairly simple you have to enter a name and download it and keep it
with you somewhere safe you can do that we won't be needing this key to a greater extent today but let us just go
ahead and select one so you can create one or you can select the one that you have so I'm gonna select this sample SSH
which is already there I acknowledge the fact and then I say create project
so guys the project is gonna go ahead and work now I mentioned that there are
some extensions and stuff you can actually go ahead and pick up certain tools that help you work with the code say for example I wish to commit a
change to a particular code I need to change that code so how do I do I can use a particular editor like Clips you
can use visual studio if you want or you can directly use command line interface as well you have a choice to do that and
configure this later as well so if you want you can just go ahead and say skip or not the fact that we won't be working
on a very big project we do not need to do that so let's just skip it for now so as you can see it says that no ID
connected because I haven't selected one and guys this is going to go ahead and select up set up a particular project
for me so setting up a project takes a longer while the reason for that is a
lot of things happen you have a particular piece of code and a number of quotes actually I'm gonna show you that
piece of code as well once we go back because um a template application is being generated which would be available for
display so that is why it takes a longer while and for now the configuration part
is also happening I mean it would actually go ahead and configure some of the resources because there would be a
particular server or a virtual machine on top of which your application would be hosted certain IM rules or policies
would be created that would give particular access to particular services subnets and vpcs are virtual private
clouds or networks basically would be created a network under which these instances would run and can communicate
with each other so all these things happen in the background and that is why these things take longer than normal so
let us just hope that this application gets created quicker I've just refreshed it but no it is
still getting created and this is a dashboard guys once this gets created probably you'd be having application
endpoints and it would give you complete detail as in what got built what got deployed and what all changes can you
make so meanwhile will have to bear with it and guys while this happens let us just go ahead and take a look at some
other stuff maybe we can talk about certain things or discuss certain other things so that our time does not get
wasted so guys I talked about creating a key pair you can always visit this particular service ec2
and once you do that you would be taken to the ec2 Management console
um there are the instances and stuff all the details that concern this particular service so there should be something
yeah this is where the key pair option is you can just click on it and just go ahead and create a new key pair all you
have to do is put in your name name of the keypad basically and you're good to go guys so this is what you can do
create a key pair uh the way it suits your needs and once you do that you
would be good to go guys if I just go back yeah
there are quite a few other services I talked about AWS devops in particular right and to do that there are quite a
few Services say for example code pipeline code commit something that I've used recently that is why you can see
all these services in the History part as well so let us just go ahead and see whether our application is up and
running my sample project the project has been successfully
created guys you can see that there is status update initial commit made by AWS code start
during project creation and committed two minutes ago so it was committed you can see that this is the source code
commit it is getting built and once it is built it would be ready to deploy guys so once this happens there are
quite a few other things that we need to take care of say for example this is how the if you click here you would be taken to the dashboard which we are on already
you have your IDE if you are using any code commit where you have all the
details about the code that you're going to use and stuff like that basically your code built
um something that builds your code if you want to take a look at it you can do that as well deployment you can study that pipeline where you can actually go
ahead and add more stages to your project once we are done with the deployment part probably we would talk about it a little as in what you can do
with it so yeah these are the things that are going to happen guys uh you can see the code is still getting built so
let us refresh it for now and see what is happening and where
there you go so it was built already and now it is getting deployed so half a minute or so and guys our application
would be up and running now again guys if you are looking for maybe a structured approach when it comes to
getting trained in this particular technology if you are interested probably you can take a look at this website by edureka where you have this
particular course AWS certified devops engineer and in this training probably you'd be covered with all the details
that concern devops and all the services that are there see these are the topics that are covered here and it talks about
everything that is related to devops on AWS like the services that are there and
the operations that you can perform talking about AWS code formation this is
something that has been used in our project as well that we've just created I would talk about it as well uh getting
back to this discussion for now you can probably visit this place and take a look at the details that are there so
that is if you are interested and probably you can get in touch with any of our executors and probably they would
guide you on how to proceed with it I mean what is the fees whether there are any discounts and stuff like that and
how you would be attended in case if there is any problem related to customer service or any project details or any
class details or stuff like that so you can actually go ahead and visit this website and take a look at all the stuff that is here I believe I've stalled
enough time to so let us just actually go ahead and see whether our code has been deployed or not so let me just
refresh it once more Now You See It refresh
it is still getting deployed guys uh will happen soon do not worry about it and guys then you have jeera as well you
can integrate with jeera what it does is it lets you track your items and issues with AWS code star
and you can actually integrate with jira as well so there are any commitments or any changes that you have to do using
jira integration you can do that as well it is fairly easy just connect to it and enter in few details and you would be
more than good to go guys and once this application is up and running probably your Cloud watch I
mentioned the fact that AWS connects with other services to give you monitoring details cloudwatch is one
such service which gives you all the details so once this application is deployed the changes would be visible
here as in what happened how it got deployed and all those things
okay this is taking longer than normal guys normally it does not take this long but yeah it is taking this long for now
so the fact that our code has been committed already so guys let's just go back to the code part
so guys in case if you wish to take a look at the code that you've used you can actually come to this particular place and all the details as in what
does the sample project have see these are the files it has all the files that concern your particular piece of code or
particular piece of application that you would be using so if you click on this read MD file and if you say edit it so
probably all the details if you click on say view source so this is the code that is there in
this readme file guys you can actually go ahead and make changes to it you can see edit and you can change this file as
well you can do that by using basically your CLI as well which is your command line interface where you can actually go
ahead and commit all the changes that are concerned to this particular service and you can see that all the pull
request commits and branches are taken care of when you talk about this particular service coming back to this particular project
that we've launched which is taking very long than normal let us see what has happened
there you go guys so our project has been successfully deployed as well so all the operations that concern this
particular service if you click on this particular endpoint and open it you can see that an
application has been deployed we just created a node.js web application using this particular service and it is a
response you guys you can go to home that is it will take you to AWS home and tell you about the services and you can
contact them as well if you click on this thing you would be redirected to Twitter basically there you go you have
the Twitter details as well so guys yeah we've pretty much actually gone ahead and we've launched an application uh
again I've talked about the pipeline and stuff so this is the pipeline you can actually go ahead and make changes to
this pipeline as well so if I click on this pipeline icon probably I can actually go ahead and add more stages to
it guys so this is how the pipeline would look like if I just say edit it
gives me an option of editing so I can scroll down guys and probably I can decide what to do say for example this
is the deploy stage this is the build stage after build stage and before the deployment stage say I wish to basically
okay have it here say I wish to approve particular pieces of quotes or Builds on
my own I can just say approval say manual approval maybe so that I can
manually approve these things so I say at this particular stage the stage name is invalid let's just stick to approval
for now there you go and I say at this stage um I can just scroll down and I can add
action groups to it I can decide what is to be added I mean you can add the action name say approve deployment
and I say manual approval basically it would ask me for the details do you want
SNS topic Arn that is do you want to be notified each time manually you approve something and you want it want a
notification to go to other team members as well so you can do that you can add those here you can save the changes if
you wish and you'd be good to go guys let's not do that for now uh let us just stick to the basic stuff so you can
actually go ahead and add those details and that particular thing would be added here in the pipeline as well guys once you are done with it there quite a few
other things that you can do with this service like you have this dashboard you can decide how do you wish to play with it do you wish to push and pull some
other components so that you can decide what is the frequently used application that you're using and if you commit the
changes those changes would be reflected here and here as well so this is it guys and apart from that you have the teams
option here as well which actually lets you decide if you wish to add more team members what access to give to them and
stuff like that so which member gets what access to which service and how much is something that you can decide
and you can have all these details on your dashboard here and here again guys once you are done using all the services
and if you are practicing make sure you delete all the resources that are there with you and you won't be charged to any
particular level in case if a particular service charges you so even before we leave we are going to go ahead and
delete all the stuff that we've created but as far as this session goes guys we've talked about quite a few things
and we've discussed what AWS code star is I believe you have some basic understanding to this particular topic
so let us just go ahead and delete our project first and then probably we can
better due to this particular session so when you say delete guys you'd have to enter your ID so I'm just gonna copy it
from here and I'm gonna paste it here and say delete and then it would delete this particular service for me takes
maybe a minute or two to do that once the service is deleted you are good to go
[Music] why do you need AWS Ops works before we
talk about this particular Point let us just quickly take a look at the definition of this particular topic so
that probably we can relate to the topic in hand much better that is once we understand what is database devops there
might be quite a few people here who might be completely new to these terminologies so it is important we
understand what Ops works so that we can probably relate to the bigger picture as in why do we actually use it so let's
just do that let us take a look at the basic definition first when you talk about the definition this is what it has
to say it says that it is a configuration Management Service that helps you build and operate highly
Dynamic applications and propagate changes instantly so too many words here let me just simplify it for you now
these days we built so many applications which are built consistently and constantly so if you think about it from
an infrastructure perspective you might be required to maintain hundreds of servers different applications on top of
it configure those applications and do n number of activities that you actually would not want to do because as a
developer or as a business owner your Prime focuses on focusing on the business part and on the development
part so what if you could give these laborious activities to someone else to do it wouldn't that be nice well Ops
works it exactly does that I mean it takes care of the configuration and the management part and when you talk about
this service it specifically helps you in various devops related activities like it lets you deploy these codes as
well or these applications as well you can do that manually automatically and in number of other ways it lets you load
balance your applications so all in all it serves quite a few purposes when you talk about devops and when you talk
about devops these things can be done by using chef and puppet so what this thing
does is that is AWS Ops works it basically lets you use configurations that support these tools as well and
puppet and Chef that is which these individual tools are they also let you use Ops Works along with them so that is
what AWS Ops works is basically it lets you configure and basically manage
various deployment processes and the infrastructure that we are talking about as I get into the demo part I would
actually give you a deeper explanation as to what you can do with Chef what you can do with puppet and any number of
other things so this is what the basic definition is now let us go back and try to understand why AWS Ops works
now when I talk about AWS Ops Works let's consider this example suppose you
have like thousands of servers that are running at a time something that I talked about a while logarison you might be required to work with a number of
servers so in this case suppose that you have somewhere around 1000 servers and probably a supposed or you are made to
deal with changing the properties of certain files on these servers if you do it manually actually there would be a
lot of laborious activities involved in this task but if you talk about Ops Works in particular what it does is it
lets you automate this process and it lets you do this task badly on N number of services so thus it helps you save a
lot of time and apart from that it gives you following features it is highly productive it is highly powerful it is
easy to use and it starts easily it is highly flexible and it is secure as well the plus point of it is you can run any
kind of application on top of it so that is not your concern I mean you do not have to worry about the kind of
applications that you can run on top of it since you can run any kind of application all you have to do is focus
on the configuration part then let go of the worry where you focus on what kind of application should I deploy so guys
this was the reason why you should actually go ahead and learn AWS Ops Works we've already looked into the
details of what is AWS Ops works let us take a look at certain components that are important to this particular service
so following are the components that this service focuses on you have stacks and layers then you have instances
applications and cookbooks let me throw in some light on each of these terms
individually stacks and layers now when I talk about building applications basically you're
talking about so many things I mean you're talking about code you're talking about configuration files you're talking about installations and stuff like that
so all these things have to lie inside a container right inside a box where these
things can be managed so that stack of these resources is called as a stack I
mean the name Remains the Same but that is what it is and then you have something called as layers now again
these operations that I just talked about say for example you are supposed to install certain softwares so you need
to have a stack or maybe a sub stack that basically holds in these installation files right so that sub
stack is called as a layer it basically lets you have more classified approach
towards the resources that you have so a sub classification inside a stack is
called as a layer and then you have instances I've talked about instances already as in what it is again if you
talk about computation I mean you would be carrying out the operations that I just mentioned right so you need some
source that lets you perform all these computations and a platform to perform these computations on top of these
applications right so that platform is something that is provided to you by the instances that are there now there are
various types of instances let us not get into the details of those but yeah these instances think of them as
platforms on top of which you can put in your applications and Carry Out computation then you need certain
applications that you can put on top of it so these applications again you can build those on your own you can use
custom applications depending upon your need AWS or most of the cloud platforms
that you would be using they would give you these options where you can have your customizations or you can actually go ahead and use the default
applications that are there on these Cloud platforms specifically with AWS it is definitely true
cookbooks now what are cookbooks again you have various configuration files or
basically files that tell your service what to do I mean what kind of operation do you want your service to perform so
this piece of information is a configuration file and when you talk about a cookbook basically it is a
collection of recipes recipe again is nothing but a script basically or your
configuration file so if you have n number of such configuration files which
basically tell your service what to do you can Club them together so that you form a cookbook just like our normal
routine cooking practice right if you talk about a cookbook it would have a number of recipes and each recipe would
tell you what different dish to cook right so if you apply this logic into Ops works or basically if you apply this
logic into configuration process that is what recipes and cookbooks would be like so yeah guys we've talked about all
these things we've talked about stacks and layers we've talked about instances we've talked about applications and
cookbooks now let us just go ahead and switch into the console part and try to see what AWS Ops Works have to offer to
us or has to offer to us so guys what I've done is I've gone ahead and I've switched into the AWS
console this is where we would be getting a walk through to AWS Ops works
now before we get into the details of AWS Ops works for people who are completely new to Amazon web services
AWS what they can do is they can actually go ahead and create a free tier account here what AWS does is it gives
you some of the or most of the services for free so that you can actually go ahead and practice these Services now
when I say for free these services are available to you for one complete year and there is certain limitation cap on
it where you can use these services for certain limits only so what I would
suggest is you actually go ahead and log into the AWS website this particular website or you can just go ahead and
search for AWS free tier in any browser and you would be redirected to this page
there you'd have to go ahead and sign in or probably sign up and once you create an account you'd be having all these
Services made available to you what is the process for signing up you have to put in certain details like why do you
want to use this account who are you by profession and what is your intention of using this account plus you'd have to
enter in your credit card or debit card details because there is a certain cap on these services and if you exceed
those limits you would be charged post that but again the limits are more than
enough if you wish to get started so I would suggest that you put in your details for your credit card and debit card and you won't be charged you can
stay rest assured about that as well so guys once you do have an account you can actually go ahead and use the services
that AWS has to offer to you in this case we would be going through Ops works now when you actually move into AWS
console you have this find Services option where you can put in the details of the service that you want to use in
this case it is Ops works right so it would be there in the suggestion year for your use if you've recently used it
it would be shown in that option as well or in the recently visited Services as well so you can visit it either ways let
us just click on this icon today for some reason the internet is a little slow so few things are taking
longer than normal now if you've opened or used this service for the first time
or you're using this service for the first time this is what would appear to you a fresh screen for a beginner I am
purposefully using this account because I want you all to see how it looks from a fresher's perspective so guys once you
do that this is where you have all these stuff that you can do now in the previous part of this session I've
discussed about stuff like Stacks I've talked about recipes and cookbooks right so that is what we are going to do we
are going to go ahead and create a stack we are actually gonna go ahead and deploy an application and see how it
works using AWS Ops works so as you can see if you are a fresher you can
actually go ahead and start fresh with this what it does is you create your first stack and if you already have
certain instances running and you wish to use those you can register those instances as well but for now let's just
go ahead and start fresh so we get to see what all can be done so let's say at your first stack so guys you're given
these options as I've already mentioned when you talk about AWS Ops works it
works with two services or two tools rather that is your chef and puppet in a very good manner initially it started on
a great scale when you talk about Chef in particular I mean it was very easy to integrate with Chef that is integrating
AWS with Chef rather and now the fact that puppet is trending these days you
can actually go ahead and use puppet as well if you talk about adding a stack you can actually go ahead and add a
sample stack where a node.js application would be deployed to the discussed
instance or to the instance that you would be creating and if you talk about the other options you can actually go
ahead and bring in your existing cookbooks and use certain Community cookbooks as well that are there for
offerings when you talk about AWS in particular to do that you'd have to use Chef 12 stack and in case if you wish to
use built-in cookbooks for the applications and deployments you can switch to Chef 11 stack as well so guys
we're gonna stick with the basic application for now once you click or select this option A node.js app will be
set up to help you explore the features and configuration options of AWS Ops Works stacks for example layers
lifecycle events and Etc so let's just go ahead and create one what operating system do you want let's
stick to any I mean we are not going to go ahead and explore these instances or these operating systems so let it be
Linux for now it does not matter to any great extent and say create a stack so Guys these are the five processes that
would happen it would create a stack the name would be my sample stack a cookbook repository would be created basically
that is using Chef a layer would be created node.js app server would be its
name then probably a recipe would be assigned to basically using which you can actually go ahead and deploy your
application and finally you'd add an instance layer as well or you'd add an instance to the layer rather so once you
do that let's just go ahead and say explore the sample stack so all these things have happened guys now as you can
see you have a sample stack basically so if you wish to explore you can actually
go ahead and go through these options as well the important thing that you need to do is actually need to go ahead and start an instance so let's just say
start the instance so guys this is the process which takes a long time meanwhile let us just go
through certain Basics if we are able to bite the time so that the instance gets launched in that duration I mean so
basically there is a server that is getting installed here so it's obviously gonna take a longer while so meanwhile
let's talk about certain terms and try to understand those if we still have enough time then I would probably pause
this session meanwhile the application gets or the instance and the server gets up and running so let us try to take a
look at certain pointers when we talk about these Stacks probably this is the layer that is there which we have we've
talked about layers already so again if you come to this particular location it would give you the information about the
stuff that we are talking about let me just click on this thing see there's the information here as in what is a stack
what does it mean layers as in what are the layers that you're using what a layer is and stuff like that so if you
wish you can add more layers as well instances this is where you would have information about air instance which is
getting booted right now or booting up right now this is the IP address that we can use then there are two types of
instances guys you have your time-based instance and you have your load based instance where you can customize the way
your application needs to be handled if you wish to spawn in more instances or if you wish to give in certain
instructions for instance to react to you can certain certain time variable parameters and if you wish to load
balance better you can actually go ahead and load balance at instances as well if you click on this icon no load balance
based instances is what it is saying you can actually go ahead and add those you can edit the configuration for the same
as well now this is the information as in what happens with a load based instance then you have your time based
instance as well same for it as well the information as and how it works deployments this is something that we're
gonna do once instance is up and running so we should wait for that monitoring this is where you get all the
information about what has happened once the application server is up and down let us just go ahead and see whether the
instance is up and whether it's running it is still booting up guys so I'm just
gonna go ahead and um kind of wait for this particular activity to happen and once it does
happen I would recontenue the discussion okay booting process is done now it is
running the setup so I'm hoping that it's gonna happen quicker so guys there you go the node.js server
is up and running and you can see the status says online the instance that we
are using right now is T2 medium now there are different families of instances that you use which serve
different purposes certain instances are memory specific certain are General
instances if I'm not wrong T2 family instances are burstable instances which actually work at some reserved capacity
in the initial phase December 120 percent and if you require more computation they burst into that mode
where you can actually go ahead and utilize that instance 100 now this is for cost saving purposes but that is not
the discussion for today let us just go ahead and see what we have here in store now our instance is up and running so
let us move ahead and get into the deployment part let's just say deploy an application
deploy this particular application yep command deploy
comment sample for today there you go and I say deploy
so yeah this application would be deployed to our instance basically so again guys the reason we use Ops to work
is what it does is it kind of simplifies all these processes now this is where we've actually gone ahead and created a
very simple application and we've deployed it again as I've already mentioned you have the freedom of choosing the instances that you use
choosing the application that you want to bring in and you can actually go ahead and use all the recipes and
cookbooks that you have I mean you can use configurations and instructions that you want to use that suit your need
better and your applications can be large they can run on any kind of platform so all these restrictions won't
be there once you actually go ahead and deploy your application or once you actually do good and decide to use AWS
Ops works so yes it is a complete service once this thing is up and ready
we can see what has happened so the application is deployed and if you wish to take a look at what has happened the
reason it took this long was these were the number of operations that were performed C you can take a look at the
lock this is all that has happened and this is what it has gone through these
many processes so you can understand the amount of time it has taken and when we say automate certain processes it helps
you take care of these many processes guys so yep again so if you wish to actually go ahead and see what has
happened you can kind of move to your instances this is where instance lies right so you
can come here and kind of move to your IP now I've clicked on that IP and this
is what I've got you've successfully just deployed your first app with AWS
Ops works so this is it guys I mean we've understood how apps work Works
what all it does and at least the basic part one more important thing that you need to notice guys when you do actually
go ahead and create these applications when you actually go ahead and do all these things you should understand one
thing that these things might cost you so it is very important that you actually go ahead and clear all the
stuff that is there with you so it is very important you stop your instances it is very important you delete your
applications that are there with you so first and foremost it is important you delete your application once you've
deleted the application guys your next process is to actually go ahead and stop
and terminate the instance that you're using and and you would be good to go guys so once you've deleted the instance
once you've deleted the application then go ahead and delete your stack from here
because you might be charged otherwise so ensure that you carry out these practices [Music]
so let us move further and talk about ECS a little more it is a highly
scalable fast container Management Service which makes it easy to run stop and manage Docker containers on a
cluster which actually lets you host your cluster on a serverless infrastructure so let us talk about
these terms as I always do let us discuss them decode them and break them into smaller chunks and try to
understand them a little more it is a highly scalable fast container Management Service so basically it lets
you manage your containers as I've already mentioned when you talk about Docker you're talking about containers so this service basically lets you
manage those containers those Docker images and lets you deal with them a lot better and the fact that it is highly
scalable and it is fast it meets in the changing needs of the applications that are there I mean if you wish to scale up
scale down have more RAM have more space and stuff like that it lets you deal with those because AWS is highly Adept
in providing you with the needs or actually providing you with the requirements that you actually have so
yep it is highly scalable it is very fast and it lets you manage your containers what it does is it makes it
easier for you to run manage and stop your containers whenever you wish to and
the fact that these containers they run on top of a cluster so you have a cluster which is nothing but basically
think of it as again a bigger container that lets you manage your containers better what a cluster is what do these
things are or what do these things mean I would be talking about those in detail as we get into the demo part but
meanwhile that is what the definition says and the important part here is and the important part in this definition is
it basically lets you a host a clusters on top of serverless infrastructures
what is a serverless infrastructure basically it is a place where you never
get to know that would be a wrong term to use never get to know I would say you are never required to worry about your
servers your upscaling down scaling maintaining and monitoring your servers somebody else does it for you so that is
not your concern and that is why we call these infrastructures as serverless
infrastructures your AWS ec2 is a very common example for that so guys let us
move further and try to understand some of the other terms or let us just actually go ahead and move into the demo
part so probably we can understand these terminologies and the topics that I've been talking about in a better way
so guys what I've done is I've actually gone ahead and I've switched into my AWS console and this is how it looks like
this is what I tell in all my sessions for people who are completely new to AWS they can actually go ahead and create a
free tier account and they can have access to this portal for free for one complete year where most of the services
are free under certain limits I mean if you basically cross those limits you would be charged but if you stay in
those limits you would not be charged guys so if you are interested in practicing AWS definitely those limits
are enough and I would suggest that you create a free tier account how do you do that go to AWS free tier or search for
AWS free tier on your browser and then probably try to sign up or create an
account they would ask you to enter in details like why do you want to use this account who are you and some other details and then they would ask you to
given your credit card or debit card details yes these are mandatory guys because if you exceed those limits you
would be charged but that happens very rarely that you would exceed those limits plus you can have certain alarm
set that tell you that okay um you're crossing a particular limit if you wish to know more about that you can take a
look at our video called as AWS cloudwatch which talks about these things in a lot more detail so once you
do have an account you can join me back here and we can get started with this session and the fact that I was suggesting you a video one thing that
also reminded me that I should suggest you is the fact that you can actually go ahead and take a look at one more video
that we have or two more videos that we have one is I think it is called as virtualization or virtual machines
tutorial by edureka which basically talks about virtualization in general and you can also take a look at one more
video called as Docker on AWS which talks about quite a few terms that is containers virtualizations in a better
depth or in more depth so that you can probably understand those Concepts in little more detail in case if you've not
understood those in this session having said that guys now let us just go ahead and try to understand how you can
actually go ahead and run your Docker images on on top of ECS which is elastic container service so how do we go there
we searched for it we type ECS and it would take you to your ECS
service again guys if you are completely new this is how the console or your
first page would look like so if you've created more applications here those would be visible so the fact that this
is a fresh account let's just say get started so guys this is what we are going to do here we are going to go
ahead and create a container definition task definition service and cluster and
then we are going to run our container on top of our cluster the when I say a
container you can think of it as your Docker image basically so guys what am I doing here I'm basically actually gonna
go ahead and choose an image for my container there are quite a few options you can use your engines you can
actually use a tomcat web server you can actually go ahead and configure as well if you click on this configure you would
be entering quite a few details manually if you wish to skip that you can actually go ahead and use this image in
general now let's just go ahead and do that it is a sample app it says HTTP 2.4
um memory is half a GB and that is the CPU details so I select this and I
scroll down what is the task definition it is a blueprint for your application it describes one or more containers
through attributes some of the attributes are configured at the task level but majority of them actually get
configured as per the container so these are some of the parameters that are at concern that is your definition name
Network Mode your execution role compatibility is memory and the CPU so I
say next the number of desired tasks I would say one and Security Group Well let that be
created automatically and load balancer guys if I'm experiencing more traffic I
would want to balance it better so your load balancer does it for you listener Port is 80 and HTTP now these are
networking details I hope you are aware about it and then I say next so guys now by now we have our container
definition and task definition we now actually need to go ahead and configure our cluster as well so let us just go
ahead and give it a name say sample for today forgive me guys I'm bad at naming
do I need to review the details I'm just gonna go ahead and say create so now guys it would prepare somewhere
around 10 services or 10 operations rather so these are the operations that are happening here
and it might take little longer than normal so let us just hope that it
happens quicker if not I'm gonna pause this video meanwhile and once it gets up and running I would get back to you
so guys as you can see we've actually gone ahead and run all the services that I've mentioned 10 out of 10 and this is
what it has done additional service Integrations log group you have cloud formation stack then you have basically
your VPC subnet one subnet to security groups load balances so guys a security group is created that takes care of
security your load balancer ensures that all the traffic gets distributed evenly in case if there is a lot of traffic you
have your VPC that helps you connect all the instances that are there inside a particular Network you have your cluster
you have a task definition you have your service and quite a few other things so guys let us just go ahead and view the
service that we've launched if you come here you would actually get a lot of details like what task is running you
have your even details here as well Auto scaling groups if any and what are the deployments that have happened
other metrics if any what was the CPU utilization and stuff like that so there
would be a lot of things that would be under concerned for now let us just go ahead and see how the application looks
like and stuff like that before that you have your cluster details here you have your task definitions eks that is for
some other session you have your clusters you have your repositories and stuff like that so if I just go ahead
and click on this particular icon not the icon the link rather there you go I would be redirected here if I just
scroll down so guys basically this is your load balancing group that is here and these
are some of its basic configurations that are there this is the name this is the Arn that is there which concerns
this particular service the ports and all that we've already assigned VPC and your load balancer these are some of its
attributes that are there we can actually go back or scroll up and
probably we can not scroll up let's just come down here somewhere yeah
description you're actually gonna go ahead and take a look at some of the targets that are there these are your
availability zones that you can do you have your health checks that are constantly shown as an how healthy your
services some monitoring details again here if you wish to cloudwatch gives you the details I've already mentioned that
it would also help you actually go ahead and set alarms that if you wish to and
you can just go back here okay let's just go ahead and search for
ECS again there you go so your cluster is here basically Guys these are the other
details it is sample for today bad name and we've launched using fargate it
works in tandem with your ECS and these are the running tasks it says and the
the number of services that are there so you can just open sample for today so yeah this is the details about all
the services that are there and and then you have your load balancer which would let you actually go ahead and confirm
whether your application or your container is up and running or not so if you kind of scroll down here there would
be a DNS here which you can copy and you can put it here and there you go your
application is up and running so guys this is how you actually go ahead and create a container and basically you
deploy that using ECS and you can deploy Docker images you can actually go ahead
and deploy quite a few other types of applications and containers using ECS one more thing that you need to know
guys is you actually need to go ahead and delete your applications when you are actually using ECS service or you're
actually going ahead and using AWS it is very important you understand the importance of deleting your services because if not you might be charged
without actually gone ahead and configured a load balancer as well so we need to delete quite a few things here
so this is our application right so if we come here my application what are the
options that I have I can say update and number of tasks is one set it to zero so
you did not miss out on anything important say next next so the service is updated let's
just go to sample app service the problem for me is navigating between these Services guys that is I'm actually
not used to using this interface a lot and it kind of makes it irritating for me to actually go ahead and do all these
things so I'm just gonna go ahead and now select this and set delete say delete me
so this application would be deleted once you do that you go back to your cluster guys and you can actually go
ahead and delete your cluster as well whereas the option to delete your cluster go to this sample for today and
delete the cluster so deleting the cluster takes a longer time than
quite a few other things once you've deleted this guys you can go to your services go to ec2 probably and in the
left side panel you'll be given an option as in something called as load balancers you can delete the load
balancer there and probably all the required tasks are done
[Music] so what exactly is Docker now first let
us take a look at the basic definition as in what does the definition has to say now as far as the definition is
concerned it stands for a tool that basically lets you create deploy and
test applications in different environments now the second line here is very interesting it says that it is
nothing but containers which are lightweight alternatives to Virtual machines and they let you do that on one
single OS yes now for people who are completely new to Docker they might wonder what virtual machines are what
containers are and what am I talking about creating deploying and running applications in different environments
well to understand this particular term or these terms let us first understand what virtualization is and probably all
these terms would make a lot more sense to you people so let us first understand what virtualization is
now guys if you talk about virtualization this is the scenario that existed before uh virtualization
actually came into picture now to consider Computing or to consider software usage or development we all
know that we need a hardware on top of which we have a software and on top of which we run our applications right as
simple as that we all use laptops computers systems so what we do is we probably have a system which is your
Hardware on top of which you have an operating system which is a software or basically an operating system which
actually lets you go ahead and use your applications whether it's your Java application playing some games playing
music whatever it is for that you need a basic operating system which is a software so when you talk about software
development this is what you need you need a basic infrastructure a hardware on top of which you have an operating
system and on which you basically run applications but when you talk about software development it is not that
simple I mean you're talking about n number of applications in number of processes let us consider Amazon now
when you talk about Amazon whether it's the e-commerce website where they're talking about Amazon web services or the
various applications they host on top of their platforms now all these applications they basically have so many
so many parallel applications or small small applications running I mean e-commerce website for example there you
have your billing section you have your different products section you have uh customer service and a number of other
sections right so having a monolithic application actually
makes our architecture very difficult to use and also for the people who are monitoring it maintaining it it is a
tedious task for them the other disadvantage of of this particular architecture is that the fact
that there are multiple applications or multiple processes running on a single architecture what it does is it kind of
makes all those applications dependent on that particular infrastructure
suppose if I wanted to use like multiple operating systems or different operating systems to run different kind of
softwares so if I was bound to a particular infrastructure I wouldn't be able to do that
so what virtualization does is it basically lets you use one infrastructure on which you can run
virtual softwares or have virtual environments running parallely so that you can run different operating systems
to give you a very simple example we have our laptops right in which we have operating systems what if I wanted to
run multiple operating systems probably if I if I can install a particular software something like a virtualization
software what that would do is that would basically let me run multiple applications on that particular Hardware
so this is what virtualization does it lets me run multiple operating systems
multiple kind of applications in a single environment but the best part about it is when I'm using it as an
individual I do not realize that I'm working on a virtual environment instead it feels like I'm working on a single
system that means if I'm using an operating system say windows and running a particular software on it parallely if
I have have one more instance on which I'm running maybe a Mac OS what I would feel is my windows OS is running on a
different system and my Mac OS is running on a different system but that is not the case with virtualization it
is actually giving you that feel but the underlying architecture is one on top of
which we are running multiple operating systems and multiple softwares on top of that now if you actually go ahead and
take a look at the architecture this is what it looks like virtualization in particular now if you talk about a
particular virtual machine which runs or is based on virtualization you would be
having a server host that is your Basic Hardware on top of which you'd be having
a host operating system and then a hypervisor now what is a
hypervisor your hypervisor is a middleware or a middle layer now this
hypervisor basically it acts as a middleware that communicates with your host applications not host applications
your host infrastructure basically and it also communicates with the operating
systems or the applications that you want to run on top of it so what a virtual machine does is it gives you the
basic infrastructure which is must I mean no matter what kind of application you are running you need a basic
infrastructure that is your server where you can actually go ahead and host your applications and on top of it you have
your host operating system so what virtual machines do is using this hypervisor they let you run n number of
different virtual machines which would have their own guest OS which would have
their own binaries libraries and applications that run on it so what this
approach does is this basically helps you in to certain extent where you can actually save in some money and some time as well
I mean instead of having n number of machines running parallely you can have one machine and on top of that machine
you can have various virtual machines so you're actually saving up costs for
running various Hardwares or various servers instead on one server you can
have various virtual machines so yes you are saving costs as well and the fact that you can switch between different
OSS and different instances or environments that also helps you save time but again is this the most
efficient way for certain applications definitely yes if you need privatization you need uh
more private approach towards your applications towards your operating systems towards your binaries libraries
then yes virtual machine is a good approach but in case if time is something that is very important to you
probably you wish to save more space in that case you have a better option that
option is your containerization now take a look at this diagram and probably I would again get back to Virtual machines
and have a discussion on these topics now when we talk about containers it is
similar to your virtual machines yes so here too you have basically a server and
a host OS I mean that is understandable as I've already mentioned you need an underlying architecture so that is what
you have but on top of it you do not have a middleware instead what you have is you have a Docker engine
and the best part here is you do not see a guest OS directly what you see is you
see containers instead of your virtual machines and even in those containers what you see is you see your
applications you see your binaries you see your libraries and those things so how is containerization better well
what containerization does is it actually saves you from certain factors like you do not need a hypervisor you do
not need a guest OS here so that means the process that is your virtualization process in which virtual machines spawn
a completely different guest OS that takes time but in case of your containers that time is saved how
because you do not have guest OSS here so if a virtual machine takes say
certain minutes to launch uh your so-called container would take seconds to launch so yes you're saving time and
the fact that you are not using extra resources I mean if you are spawning a guest OS some space extra would be
consumed and would be occupied that is not the case with containers so if you talk about more efficiency yes
containers in certain aspects are more efficient but again guys do not mistake me for the fact that I'm saying that
containers are better than virtual machines no that is not the case it depends upon the applications what kind
of applications are you using for certain applications virtual machines are better for certain applications
containerization is better so containerization or containers basically are good enough when you talk about
um basically when you talk about speed or you need faster deployment faster creation and testing of applications so
in that case containers serve much much better so again what are the exact differences
where should you use Virtual machines and where should you use containers well that is not the discussion for today for
some other time probably we would have discussion on those topics for now our main focus is containers and Docker so
this is what a container is and I believe by now you have a clearer picture as to what a container is and if
you go back you would be able to relate to this definition um in a much better manner so back then
I said that Docker is a tool which is designed to make it easier for you to create deploy and run your applications
now using containers that actually becomes true and the fact that containers are lightweight alternatives
to Virtual machines probably this sentence is making a lot more sense to you people now so yes this is what
Docker is it basically lets you spawn these containers using which you can actually create deploy and run your
applications much faster by using containers so again what is the other benefit are
what is the anomaly that Docker containers overcome now if you all know what devops is which is an approach that
brings in developers and operations team what devops styles that as basically
bringing in these two teams why because developers normally develop softwares and operations team probably maintains
those softwares so when you talk about maintaining that particular software what happens is uh the software that is
developed in your development environment might not work in the production environment and one of the
reasons for that is probably the change in environment in which you're running the software so this is not the case
when you use containers why because your container is nothing but it is a limited
object or think of it as a limited box in which you only have your binaries in
which you only have your application details or code so basically when you pick up this container and if you run it
on any machine that supports Docker probably your image would run the way it
ran so testing your images or rather your applications becomes much easier
when you use a container so this is what Docker is and this is what it lets you
do so by now we've already discussed what Docker is we've understood what virtualization is and we've gone through
what containerization is the fact that we are talking about Docker and we are
talking about using it on AWS let us try to understand AWS a little more so that
we can probably relate Docker with AWS a little better
so let us not waste any time and quickly just switch into the demo part shall we
so guys this is what the AWS console looks like now again for people who are
completely new to Amazon web services let me tell you one thing that Amazon web services gives you a free tier
account using which you can actually go ahead and do quite a few things you can understand how cloud computing works you
can actually host your applications you can load balance those scale those scale
down those applications and a number of other things you can set up networks you can actually create security groups make
sure that all these services are secure and quite a few other things so yes for
people who are completely new I would suggest that you actually go ahead and create a free tier account on Amazon web services now these services are made
available to you for free for one complete Year yes what Amazon web services does is it probably gives you
um certain services for free for certain duration and if you exceed certain limits only then you are charged
otherwise if you stay in the limit it you would not be charged for one complete year and which I believe is
more than enough if you just have to go ahead and practice these services or get sufficient Hands-On on Amazon web
services or cloud computing so again if you do not have an account just go ahead and create an account do you need any
details to create this account probably you'd have to visit the console just type Amazon web services console or
Amazon web services free tier and you can click on the first link that is available and you will be redirected to
this page this is not what it would look like instead what would happen is basically a signup page would come in
where you would have to enter your email ID maybe what is the purpose that you're visiting this particular website and
probably they would ask you to enter in your credit card or debit card details do not worry AWS won't charge you
without your consent and probably you can set in your alarms as well so that you do not exceed the limits that you
use so having said that I would want you all to have an account on AWS because it is a very fun and nice platform to work
on so once you do have an account this is what you have at your disposal let's
have a quick introduction to what are the services that are here and what all can be done with it once we are done
with it we'll probably jump into the docker part and we'll see what we can do with Docker on Amazon web services so
guys as you can see there are quite a few Services here there are compute Services you have your ec2 light cell ECR ECS we would be using ECS today and
to some part I would be talking about ec2 as well which is your elastic Cloud compute I talked about virtualization
right so what virtualization does is um basically we create virtual machines so cloud computing or AWS in particular
lets you create virtual machines as well and instead of calling them virtual
machines we call them instances here so you can launch multiple instances or basically environments on top of which
you can run applications so one way to actually go ahead and host your Docker applications or containers is you can
actually launch an ec2 instance and on top top of it you can actually go ahead and probably install docker and then
host containers or have containers which you can run to run your applications so
that is the old school method where your ec2 acts as your system on top of it
you're gonna go ahead and play with your containers the other way is your ECS and AWS forget now these Services what they
do is they completely cut off the process where you have to go ahead and spawn an instance on top of which you
have to create security groups and do a number of other things so in this session we would be doing both the
things first I would show you certain things as in how do you get started with
your Docker in ec2 and then I would switch to forget so that you understand
forget you know better way and you understand why do we actually need to use forget so let us do that let us just go ahead
and launch a simple virtual machine not a virtual machine rather an instance using ec2
so if you click on ec2 this is what you'd be redirected to you can launch an instance by clicking on this icon
so as you can see in the marketplace you have quite a few options you can choose Amis Amis are nothing but images
basically when you create a template or create an operating system it has a by default template which AWS saves as in
the form of images so these images are nothing but templates for your respective environments or operating
systems which you might want to use now when you talk about Docker it has to be on Linux I mean yes there is a way you
can do it on Windows as well but preferably it is Linux so let us just go ahead and launch a Linux instance first
so this is a free tier eligible that means I won't be charged if I launch this instance so I do that
and then there are certain details what is the family of your instance um there are quite a few families you
have T2 micro you have um various other uh actually families which you can actually go ahead and
explore now again if we keep discussing those families this session would go too long so let us just stick to the basic
or the free tier resources that we have you can actually go ahead and explore these resources if you want
if I go to the next button and I click on it it would show me some other settings for now my main concern is this
part yes the security group so when I launch
an instance I need to log into my instance to maintain security so you have certain security groups which
ensure that these are the rights users have when they try to access these kind of instances so we need to actually go
ahead and set up a security group to some extent so there is one rule which says that I allow traffic from SSH
um protocol here is TCP and this is the port it is using I can actually go ahead and add one more
rule I can say http
there you go and allow traffic from anywhere basically
now I can say review and launch in order to sign into my instance I need a key pair which is a security measure so I
need to create a security or sorry a keeper rather and make sure one thing is that if you
do create a key pair you save it properly so says let us name it says um
new I'm very bad at naming conventions guys so please forgive me for that
again guys you have to take care that this key pair is well maintained you do not lose this key pair so if you do
create one you just need to save it somewhere and then I say launch an instance
so guys what is happening here is my instance is getting launched so once this instance gets launched I can
actually go ahead and install Docker on top of it and do quite a few things with it now the reason I'm going through this
drill is because I want you to understand what is happening here so this is like uh basically going ahead
and just creating an instance on top of it running your Docker and doing all those things which was very relevant say
three four years back now there are better ways to do this but first let us see how do we do it in this particular
way so if I go back to Services I go back to ec2 you can see that an instance would be
here saying that there is a running instance which I just created let us see whether that instance is launched or it
has it is still getting initialized guys but that is not a concern I can connect it to or connect to this instance so
guys this being a Linux instance I need to actually go ahead and um
use a key to log into this instance and this being a Linux instance I cannot use this
pem file it needs to be in PPK format and to do that we have a software if you
do not have this software you can install this it is putty so you have putty software which
lets you use this key or convert this key and to generate a PPK key you need party gen as well this is the other
software so what you do is you just click on load and you select the file that we just
actually downloaded so it is not here because it is in different format you can see pem I say open
and always save a copy of this key guys do you want to save it yes um
and what should be the name let's say sample one two
now it says save so my the key is saved here now I can actually go ahead and launch my instance
first let us confirm whether our instances up and running whether it has initialized or not
okay it is still initializing let it get initialized and once it is done probably we can actually go ahead and connect to
this instance
at times this can be Troublesome I mean you have to wait for certain things so guys again as you can see I've launched
an instance the launching process is taking time when you talk about virtual machines
which I mentioned it takes while why because it boots in a guest operating system and hence this time I mean the
checks the quality checks the initialization process all these things they can take a little longer while than
normal
okay
so meanwhile just um let me tell you what we have to do the
fact that we have a key now probably we can actually go ahead and connect to this instance by using this putty
configuration you have to enter in your host name and you have to enter in that
PPK file as well so let us see whether these status checks are complete or not
yeah as you can see the status checks are complete so when I say connect I get
certain details now these are the important things this is the stuff I need to copy
there you go and I paste it here in this hostname okay and then I go back to this place I
click on data and here if I come back the first part before
I can copy that and I can just come here and paste it here now I need to actually go ahead and
select the PPK file which I um saved so to do that I
click on SSH and in that I click on authorization I browse and here is the file I say open
I say open
yes so guys what it has done is it has launched a virtual machine on top of
which I'm gonna install Docker now how do we do that first let us check whether
I am the right person or I have signed in as a right person yes ec2 user this is me
so I should say sudo TM
update why it would get in the necessary details
that are needed to install Docker on top of it
thank you okay it is almost done it should be done
soon
there you go so next what we do is I say sudo him
install
okay I made a spelling mistake here
so when I go ahead guys what I can do now is I can just say sudo service
docker start thank you
okay I'm not doing it fairly well today um
photo service docker
start and there you go guys uh it has configured your Docker service here so
guys this is about configuration I mean uh back then when containers when you
this was actually still very fast and very effective but as I've already mentioned there are faster ways to do it
now by now what we've done is we've just installed an instance and on top of that instance I've
actually installed Docker so that is the only thing I've done but if I do it using certain services this process can
be speeded up like anything I mean you not have to worry about anything here we haven't created a cluster we haven't almost anything so let us just go ahead
and use one of the services as in your ECS or your forget let us see what we can do using those services
so let's just close this for now yes
again guys one more suggestion if you do have your instances or Services make sure you terminate those because that
actually helps you um basically save any costs that you might incur there's being
a free instance that wasn't the case but still let me just shut it down I've done that now I'm gonna go to ECS
this is a fresh account so you do not have any applications running here so let's just say get started so guys this
is what we are going to do we are going to go ahead and create an image that is basically your container definition as
in your basic code your application whatever you want to run in this case we would be using a sample application then
we would be having a task definition where the star's definition what it does is it
has some other attributes which are not there in my container definition or in my image to manage my so-called
containers or to run my so-called containers and then you have your service which tells your cluster or your
Docker demon rather to run a particular container image and use a particular
task to do that so we would be doing all these things and we'll be creating a cluster and let us see how long it takes
now so guys there is a sample application you can actually go ahead and edit you
can use any kind of um container definition you want you can customize it to meet your needs as well
you see there's an option here which says customize you can do that as well for now let us use the basic application
the sample application so that is selected by default task definition guys again this would be basic or by default
you can make changes here you can change the name you can decide what network mode to use uh you can decide
what execution role you want if you have any role you can actually assign that particular role where you can assign
particular properties to it and what are the compatibilities in this case it has to be Target because I would be using
fargate and this is the task memory that is allocated by default so let us stick
to those configurations because we do not want to get charged unnecessarily
so Yep this is what it is I'm gonna say next
now it says Define a service a service allows you to run and maintain a specified number of simultaneous
instances of a task definition in an ECS cluster something that I told when we did get started with this particular
topic number of desired tasks you can change it you can edit it you can have multiple
tasks just to be secure so that if any of these tasks it goes down if its
health is low you can actually go ahead and use the other task as well but in this case again let us stick to one task
only load balancer if your website experiences high amount of traffic you can balance that by using a load
balancer let us create one for sample so it would be using this port as The
Listener port and protocol would be http so I say next
there you go and then it says configure your cluster should I name it something let us say
today's cluster again guys I'm very bad at naming conventions please forgive me for that
you have a VPC ID and subnets which it automatically creates you can um let them do it for now because again
this is a basic application so I say next and it asks me to review all these
things for now I'm gonna say create because I know what all details I have entered
so as you can see guys it is spawning a cluster and a container for me and now let us see how long it takes to spawn a
container now guys there are 10 processes that are happening here three of which are done
and let us see how quickly the other processes also take place again guys in this case we are not
worried about anything I did not I wasn't concerned about uh probably creating security groups I wasn't
concerned about creating um a virtual machine I wasn't concerned about using ec2 I wasn't concerned about
anything my only concern was what is the cluster I want what is the application I want to run on top of it uh what is the
image I'm using and all those things the basic information and once you enter that basic information your cluster would be created again
creating your images won't take that longer time yes creating clusters might take now your cluster can hold multiple
containers as well so yes certain processes might take a while so in this case we can see that seven services are
complete out of 10 probably we are left with three more to go
which should be done very quickly I'm sure maybe say half a minute more
foreign
guys once we are done with all these applications creation of these applications ensure that you actually go
ahead and delete all the resources that you're using and all the services that you've used and meanwhile this gets done
let us just quickly see something else if you people are interested in a structured training now again on our
YouTube channel we have quite a few videos that talk about these things in detail even if you go through those
details probably you would be in a good state to actually go ahead and start with any course that we have at edureka
but if you're looking for a structured training probably a director would be a very good option guys um the reason for that is the prices
here are very affordable and those are in real time I mean if you do participate in some of our competitions
you might get more discounts and this price might seem very less it is in Indian rupees for dollar price you can
actually call Our Help Center number and probably they would assist you in a much better way so this is before we call
course and you can see that these are the models that are covered here which you are free to actually go ahead and
take a look at so if you're interested in AWS and devops kind of a role probably this is a very good course to
take apart from that guys um AWS or it did require that provides you with the certification as well which
holds a lot of value in the market and then they have this 24 7 support they have their LMS where you can access all
these resources for Lifetime and a very helpful support team that actually answers all your queries right away
having said that guys this was about your personal choice you are free to decide on this let us just go back and
see that whether it's completed and you can see that our process is complete and it is up and
running so guys let us view this service what it has to offer to us
there you go uh these are certain details you have your task definitions
um again Amazon eks is something that you can use for kubernetes which is again a container service this is not
the topic of discussion today so I did not get into details of this particular topic but when you talk about this
particular topic are creating your cluster is concerned we have one with us and there are the details or here are
the details about that particular topic you have your tasks the tasks that are running
there you go it is still pending is what it says then you have your events
these are the events your auto scaling groups your deployments and all the details that you need
so if I go back here and I click on this particular icon or the link
it would redirect me to this page
and if I scroll down you can see that I have a load balancer if I click on it
I have a DNS name here I copied and I paste it here
there you go it says that I have a sample application and it has been deployed by using a container so it has
created a container on top of it it has run this simple application now there is nothing much here basic code so you
don't see anything too um real time or too interactive but there is enough of
an Evidence which says that an application has been launched and it is running on my container so there you
guys this is what it is probably you have all the details you can just go back and take a look at other
things as well if you wish to explore these Services a little more you can always do that
so let us just say ECS and now it would show my cluster here
today's cluster it was a very bad name but forgive me for that yep so guys now that
we have all these things again if you are doing this for experimental basis I would suggest that you delete all these
things how do you do that you click on this icon first thing that you do is you say update
this is just a security measure number of tasks set it to zero so that even if you miss out on deleting these
you won't incur too much damage and then say update now that I've updated this service
I can go back and I can just say delete
how do you delete it you say delete me
and then you say delete there you go
and it has been deleted again what do you need to delete
you need to delete a cluster as well so say delete and the cluster would be deleted
it might take a while to delete your cluster before that okay let's just delete the
cluster first
so once the cluster is deleted guys your next step is to actually go ahead and delete the load balancer as well which
again is not that difficult of a task
so guys we are almost nearing the end of today's session once we are done with the deletion process probably we would
be bidding due to each other please let me know in the comment section how did you feel this session
was if you did like this session please hit the like button and stay tuned with
our edureka Channel because on ideurika YouTube channel we have quite a few sessions quite a few videos that go live
almost every day and if you are somebody who likes to learn who likes to explore
new technologies understand what is up to date in the market probably you would have a lot of things to digest a lot of
things to learn when you talk about this YouTube channel
beautiful
so the previous Tech probably took a lot of time as you can see that from four to we are just on five right now probably
that was the process which took a long a while to delete now we are online probably we should soon reach 15.
there you go
foreign and the cluster is deleted guys now we
go back to ec2 console and let us hope that our
load balancer is also deleted because I deleted the application so with that the
load balancer should also go if not you delete it manually
yes it has been deleted so all our resources have been deleted guys we've actually gone ahead and understood what
containerization is we've understood how Docker works and how you can actually go ahead and create a container a cluster
and launch an application using AWS and all this happened quickly because of
Target and we saw that ec2 took a longer while to do that so I believe I've talked about quite a few things and I
hope that I've cleared quite a few myths that you people had about Docker and virtualization [Music]
what are containers containers are an executable unit of software in which application code is
packaged along with its libraries and dependencies so it can be run anywhere whether it is a desktop traditional ID
or the cloud containers are small fast and portable because unlike a virtual
machine containers do not include a guest OS in every instance now as I've
mentioned about virtual machine let us take a look at container and virtual machines architecture here virtual
machine has a host operating system its infrastructure and a layer called the hypervisor which is a software that
creates and runs the virtual machine then it contains the guest operating system which is the virtual copy of the
hardware that the OS requests to run along with that application and its Associated libraries and dependencies
now when you look at containers instead of virtualizing the underlying Hardware continuous virtualize the operating
system so each individual container contains only the application and its libraries and dependencies the absence
of the guest OS is why containers are so lightweight and thus fast and portable and also container runtime is a software
that executes the containers now that you have an understanding about what are containers let us move on to
the next topic and see containers on ews continuous on AWS can be divided into
three categories registry to store container images orchestration that
manages when and where the containers are run and the third category is flexible compute engines to power the
containers now let us discuss about these one by one in details now first in orchestration we have Amazon
ECS Amazon elastic container service is a highly scalable container Management
Service that makes it easier for you to run stop and manage containers on a cluster
the containers in ECS are defined in a task definition that you use to run individual tasks of various tasks within
a service the task definition can be thought as a blueprint for your application now to run your application on Amazon
ECS you must create a task definition the task definition is nothing but a
text file in Json format that describes one or more containers that form your application and here you can have only
up to 10 containers now let us take a look at the working of Amazon ECS
now you can see Amazon ECS schedules your task which is usually done by Amazon ECS task scheduler they are
responsible for scheduling or placing your task within the cluster now you can see all the containers run
within the ECS cluster and ECS cluster is nothing but a logical grouping of tasks or services
and when you first start using ECS a default cluster is created for you now you can see the container agent or the
ECS agent runs on each container in sensors within an Amazon ECS cluster the
agent sends information about the resources current running task and resource utilization to Amazon ECS now
the container images are then downloaded and stored in the container registry the container registry could be Amazon
ECR Docker Hub or a self-hosted registry now let us move on and take a look at
Amazon eks Amazon elastic kubernetes service is a managed service that can be used to run
kubernetes on AWS and you can do this without needing to install operate or maintain your own
kubernetes control plane or nodes now let us first understand what is kubernetes then we can see what are the
control plane or nodes kubernetes is an open source system for automating the deployment scaling and
management of containerized applications kubernetes works by managing a cluster
of compute instances and scheduling containers to run on the cluster so in kubernetes containers run in logical
grouping called pods so you can run and scale one or many containers together as a port now kubernetes control pin
software is a software which decides when and where to run your pods they also manage your traffic routing and
scale your ports based on utilization or other metrics that you define kubernetes will automatically start ports on your
cluster based on the resource requirements and automatically restart ports if there are any instance that are
running on fail now the advantages of using kubernetes is because it is an open source project
you can use it to run your containerized applications anywhere without needing to change your operational tooling these
were the orchestration management tools now let us have a look at Amazon elastic container registry
now this is a fully managed container registry that makes it easier for you to store manage share and deploy your
container images now if you're wondering what is a container image it is a immutable file
that contains the source code libraries dependencies tools and other files needed for an application to run now due
to his read-only quality these images are also sometimes referred to as snapshots
now the advantages of using Amazon ECR is you do not have to worry about operating your own container repository
or scaling the underlying infrastructure Amazon ECR will host your images in a
highly available and high performing architecture which will allow you to reliably deploy images for your
containerized applications you can also share your container software privately within your organization a publicly
worldwide for anyone to discover and download now let us move on to the next management tools which are Amazon ec2
instances and Amazon fargate Amazon elastic compute cloud or ec2 is a web
service that provides secure resizable compute capacity in the cloud
the advantages of running your containers on ec2 instances is you have complete control on the compute platform
you can select the operating system the memory acquired the CPU and many more
the billing is based on the cost of underlying ec2 instances it allows you to optimize the process by taking
advantages of building models like spot instances or reserved instances
and you can also increase or decrease your Computing capacities within minutes
now on the other hand we have AWS far gate fog it makes it easier for you to
focus on building your application it removes the need to provision and manage servers it will let you specify and pay
for resources per application and it also improves security through application isolation
the advantage of using fargate is it will allocate the right amount of compute eliminating the need for you to
choose instances and scale cluster capacity in Far gate the billing is based on CPU or memory requirements per
second these were the container management tools on AWS now let us move on to the
next topic and see some of the advantages of running containers on AWS the First Advantage is security
AWS offers 210 security compliance services and key feature which is 40
times more than the next largest cloud provider it also provides strong security isolation between your
containers which will ensure you're running on the latest security update and give you the ability to set access
permission for every container the second Advantage is it is reliable AWS
container services one of the best Global infrastructure with 77 availability zones across 24 regions
they also have service level agreements for all their container services the
next advantages would be it gives you a variety of choices it offers choices of services to run
your containers you can choose AWS fargate if you want serverless compute for containers or Amazon ec2 if you need
complete control over a compute environment you can also choose from ECS or eks for container orchestration
AWS container services are deeply integrated with AWS by Design so you can
use various AWS services for example for load balancing you can use Amazon elastic load balancing for monitoring
you can use cloud watch for networking you can use VPC and so on these were just some of the advantages
of running containers on AWS now let us move on to the next topic and see some
of the use cases for containers on AWS the first use case is microservices
containers provides process isolation that makes it easier for you to break apart and run application as individual
components called microservices the next use case is batch processing
you can pack batch processing and ETL jobs into containers now if you're wondering what is ETL ETL stands for
three different terms which is extract transform and load this will allow you to start your job quickly and scale them
dynamically in response to demand the next use case is machine learning
you can use containers to quickly scale machine learning models for training and run them close to your data source on
any platform moving on to a fourth use case which is hybrid applications
container lets you standardize how code is deployed making it easier for you to build workflows for application that run
between on-premises and Cloud environments the final use case for today is
application migration to the cloud now Container makes it easier for you to package the entire application and move
them to the cloud without needing to make any code changes these were some of the use cases for running containers on
AWS now let us move on to the demo part and see how can you deploy a container on AWS
now for the demo let me switch to the AWS console now we'll start by searching ECS
here it is now this is what the Amazon elastic container service page looks like so
when we scroll down we can see some of the advantages of ECS it says you can run containers at scale
it provides flexible container placement and it is integrated and extensible oh
let's get started here we can see a diagram of ECS object which contains the container definition
task definition service and cluster we will look into this one by one so first
we have container definition so container definition is choosing an image for your container so you can
start quickly and Define your container images so these are some of the sample container images you can edit it from
here you can change your container name and also the container image next we have private repository
authentication it is an authentication for a private repository so if there is a container image you want to pull from
it you should mention your secret manager Arn or the name of it next we have the memory limit which is
set to 512 MB and after that we have Port mapping Port mapping is nothing but
which allows your container to access port on the host container let us not make any changes in that and move on to
the task definition as I've mentioned before task definition is a blueprint for your application
so here we have the task definition name and the network mode the network mode is the docker
networking mode used for the container next we have the task execution rule which is the IAM role used by your task
the compatibility is the launch type you use in this case it is fargate you can also use ec2 instances
next we have the task memory which is 0.5 GB and the task CPU which is 0.25
virtual CPUs now if you want to make any changes in this you can just click on edit and you
can make some changes but now I do not want to make any changes so let me click on next
the next step is you define your service a service allows you to run and maintain
specified number of instances of task definition for example you can run and maintain three instances of task
definition in an ECS cluster here you have a service name which is sample app service and the number of desired task
is one application load balancer distributes the incoming traffic across the various
tasks running in a service now if you do not want to use a load balancer a security group is created to
allow all the public traffic to your service only on the container Port specified in the previous step but if
you want to use the application load balancer two security groups are created to secure your service
in today's demo let us not create any load balancer and we'll click on next now
the last step is to configure your cluster as I've mentioned before clusters are
logical grouping of tasks or services first we have the cluster name now let
us change it into edureka ECS
next we have the VPC ID and subnet which will automatically be created
after we set this now let us click on the next button we will have to review our details now
everything looks good now let us move forward and create our own container we click on the create button here usually
creating a service might take up to 10 minutes here you can see your cluster name your task definition name and in
some time even your service name would be visible over here and you can also add additional features after your
service is created now you can see a 7 out of 9 is complete we're just waiting for two more
now the loading is complete let us view our service we click on view service
and here we have the detail of our service here we have the cluster name the status the task definition we also
have the launch tab which is fargate the service role and is created by whom as
we have not used load balancing you can see you have no load balancer here are the network access details
now when we click on task it will show you that there is one running task
and when we click on events the first event is the task is sorted the second is deployment is completed
and the third is it has reached a steady state now under Auto scaling you can see there
is no Auto scaling resources configured for the service so moving on we'll see what is
deployments deployment consists of the task placement and the service deployment options
and in metrics we have the CPU utilization and the memory utilization as I have not assigned any tags so even
this would be empty you can click on logs and see at what time what task was running
so now let us go to task and click on our task now here we just have to copy our public
ID and paste it in a new window here now your application is running on
a container in Amazon ECS using fargate so this was our demo application
now let us go back and see how you can delete the service and the cluster now just click on cluster over here
so here you have your cluster name and it is running on fargate and you can see the running task is one over here
so now let us click on the cluster name and we have option here to delete our cluster
so we click on this and type in delete me
and then click delete this process might also take a few minutes
now we get a confirmation message saying deleted cluster your cluster name successfully
now this is how you delete your cluster [Music]
pops versus azure devops so let's talk about the services that it
provides the Amazon web services core pipeline is a continuous delivery
service for fast and reliable application updates code pipeline builds tests and deploys your code every time
there is a code change based on the release process models you define on the other hand Azure devops services for
teams to share code track work and ship software Azure devops provides unlimited
private git hosting Cloud bill for continuous integration agile planning and release management for continuous
delivery to the cloud and on-premise it also includes a broad integrated
development environment support in the next comparison we will talk about what category of tech stack do
they belong to as you can see Amazon web services code pipeline belongs to continuous deployment category of the
tech stack while Azure devops can be primarily classified under integrated development environment tools
now let's talk about the features of Amazon web services and Azure as you can
see Amazon web services provides workflow modeling AWS integration and it
also has some pre-built plugins in it whereas Azure on the other hand provides agile tools like kanban boards backlogs
Chrome boards and it also provides reporting tools like dashboards widgets power bi Etc
and it also allows you to integrate with Git which provides free private repositories and pull requests now why
do people like AWS is because it is extremely simple to set up whereas Azure
on the other hand is a complete package it's full and it's very powerful
now let's move on and check out the uses of AWS in azure as I've already
mentioned AWS is extremely simple to set up it also has several managed Services
it can be easily integrated with GitHub and it also has parallel execution
deployment is completely automatic and there are manual steps available so you're never lost Azure on the other
hand supports open source it also has several Integrations that is it can be
easily integrated with GitHub and Jenkins it also has several project management features and the most
important plus point of azure is that it is free for stakeholders I hope the users between AWS and Azure are
extremely clear now let's move on and talk about AWS and Azure clients so
Amazon web services is used by companies like play HQ affirm curro education up
ready for and bitbank incorporation whereas Azure has clients like Microsoft
Kingsman software evodeck digital news energy to Market and QR point
now the next important difference is with what tools that these cloud
services can be integrated with Amazon web services as you can see on the screen can be easily integrated with
GitHub Jenkins Amazon ec2 Amazon S3 AWS elastic Beanstalk run scope and Cloud
base Azure on the other hand can be easily integrated with kit and GitHub Docker slack Jenkins Trello and visual
studio now let's move on and compare the salary of an AWS devops engineer and Azure
devops engineer as you can see on the screen already the median salary for an AWS devops engineer is rupees 4 lakh 67
000 rupees whereas the median salary for an Azure devops engineer is about rupees
6 lakhs 4 000 rupees a year [Music]
now let us see the basic interview question we'll start with the first question now the question is describe
the three major categories of cloud service and the AWS products that are
based on them so if we understand the question they have asked the three major categories of the cloud and their
products as well right now here as you know we have three types of clouds right public Cloud private cloud and hybrid
Cloud now public cloud is nothing but it is a cloud computing environment that is
built using the end users not owned it infrastructures now for example Alibaba
Cloud Amazon web services Google Cloud IBM cloud and Microsoft Azure are the
few of the biggest public cloud service providers the cloud computing environments that is built using the end
users not owned it infrastructure these days public Cloud don't always have
restructures because some providers select users utilize their services for
free now we will look into the private clouds now private clouds are also a cloud computing platform that are only
used by one individual or a group of individuals currently businesses are
constructing private clouds on rented off-site data centers controlled by vendors then comes the hybrid clouds
hybrid clouds is a combination of public cloud and a private cloud and it is an
IT environment made up of the several environments that appear to be a connected by Lan or a virtual private
Network depending on who you ask the criteria of hybrid clouds might vary
from their characteristics and those can be complicated too however when apps can
be moving in or out of many distinct yet connected environments or every it
systems turn into a hybrid cloud now let us see the cloud services here we have
the major services like infrastructure as a service platform as a service and
finally we have software has a service now we will see each of it in a detailed
form now we will come to infrastructure as a service what does it mean it means
a cloud service provider that manages the infrastructure for you the user has
the access through API or a dashboard and especially runs the infrastructure the provider takes care of any hardware
networking hard devices and data storage and service now if we look at the
platform as a service it is a hardware and an application software platform
these are provided and managed by the outside cloud service providers but the
users can handle the apps running on the top of the platform and the data that
app relies on primarily for developers and programmers the platform as a
service gives user a shared Cloud platform for application development and
management without having to build and maintain the infrastructure usually associated with the process now if we
look into the software as a service software as a service is a service that
delivers a software application which the cloud service providers manages to
its users typically the software as a service apps are web applications or
mobile applications that the user can access via a web browser software
updates bug fixes and other General software maintenance are taken care of
the users and they connect to the cloud applications via the dashboard or API
software as a service also eliminates the need to have an app installed
locally on each individual user's computer which allowing the greater
methods of group or team access to the software so I hope you understood the
answer here I have just given you the full explanation where you could answer
just the public Cloud private cloud and hybrid cloud and their services to the
interviewer now let's move on to the next question how do the availability
Zone and the region related to one another here you need to understand the
availability Zone and the regions difference so each region has multiple
isolated location known as availability Zone the code for availability zone is
its region code followed by the letter identifier for example you Us East 1A so
when you launch an instance you select a region and a virtual private cloud and
then you can either select a subnet from one of the availability zone or let the
AWS choose one for you if you distribute your instance across the multiple
availability Zone you can also use the elastic IP addresses to mask the failure
of an instance in another availability Zone by rapidly mapping the address to
an instance in another availability zone now when it comes to region each region
is designed to be isolated from one another region this achieves the
greatest possibility for tolerance and stability when you view your resources
you see only the resources that are tied to the region that you specified this is
because the region are isolated from one another and we don't automate automatically replicate the resources
across the region when you launch an instance you might select an Ami that's
in the same region if the Ami is in the another region you can copy the Ami to
the other region you are using it now we will look into the next question that is what is auto scaling AWS Auto scaling
monitors your applications and it automatically adjusts the capacity to
maintain the study and it also predicts the performance at the lowest possible cost the service provides a simple
powerful user interfaces that lets you build the scaling plan for resource
including Amazon ec2 instance Amazon ECS tasks and indexes
let's move on to the next question what is Geo targeting in Cloud font now if we
look at the explanation in Amazon Cloud front we can detect the country from
where the end users are requesting our content this information can be passed
to our origin server by the Amazon cloudfront it is sent to a new HTTP
header based on the different countries we can generate the different content from different versions of the same
content so these versions can be cached at different Edge locations that are
closer to the end user of that country in this way we are able to Target our
end users based on their Geographic locations with that we'll move to the
next question what services can be used to create a centralized logging solution
the essential services that you can use are Amazon Cloud watch logs which can
store them in the Amazon S3 and then you can use the Amazon elastic search to
visualize them and also you can use Amazon Kinesis fire hose to move the
data from Amazon S3 to Amazon elasticsearch now let's move on to the
next question what are the different types of virtualization in AWS and what
are the difference between them now if we take the virtualization it is divided
into three types that is Hardware virtual machine para virtualization para
virtualization on hvm now what is Hardware virtual machine Hardware
virtual machine in short hvm it is a fully virtualized Hardware where all the
virtual machine acts separate from each other this virtual machine boot by
executing a master boot record in the root block device of your image now let
us see the para virtualization and para virtualization is the bootloader that
puts the para virtualization Amis now the power virtualization chain loads the
kernel specified in the menu now let's see what is para virtualization on hvm
now para virtualization on hvm helps the operating system takes advantage of
storage and network input output that available through the host
now the next question is explain what T2 instance are T2 instance are bustable
performance instances that provides base level of CPU performances with the
ability to burst above the Baseline now the Baseline performances and the
ability to burst are governed by the CPU credits here the t2 instance accumulate
the CPU credits when they are idle and they can consume the CPU credits when
they are active now that we are done with the basic level of AWS interview questions now let's jump into some of
the AWS ec2 questions in that we will see what is Amazon ec2 here Amazon ec2
is a short form of elastic compute cloud and it provides the scalable Computing
capacity using Amazon ec2 which eliminates the need to invest in
hardware and also it leads to the faster development and the deployment of the
applications also you can use Amazon ec2 to launch as many or has few virtual
servers as you need and you can also configure the security and networking
and also manage the storage it can also scale up or scale down to handle the
changes in requirements and it can reduce the need to forecast traffic ec2
provides the virtual Computing environments coil instances with that we
will move on to the next question what are Solaris and AIX operating systems
are they available with AWS now let us see about the Solaris and AIX operating
system Solaris is an operating system that uses The Spar processor
architecture which is not supported by the public Cloud currently where AIX is
an operating system that runs only on power CPU and not on Intel which means
that you cannot create AIX instance in ec2 since the both operating system have
their limitations they are not currently available in AWS now let us see the next
question that is what are key pair in AWS now the key pair are password
protected login credentials for the virtual machines that are used to prove
our identity while connecting to the AWS ec2 instances the key pair are made up
of the private key and a public key which lets us connect to the instances
let's move on to the AWS S3 interview questions now let us see what is Amazon
S3 is Amazon S3 is a short form of simple storage service the Amazon S3 is
the most supported storage platform available in the Amazon where S3 is
object storage that can store and retrieve any amount of data from anywhere despite that it is versatility
and it is practically unlimited as well as a cost effective because its storage
is available on demand in addition to its benefits it offers unprecedented
levels of durability and availability Amazon S3 helps to manage data for cost
optimization access control and compliances let's move on to the next
question how do you allow a user to gain access to a specific bucket here you
need to follow few steps that is you need to start categorize your instances
after that you need to Define how authorized users can manage specific
servers then you can lock down your tags and you can attach your policies to IAM
users with that you can be able to allow the users to gain access to a specific
bucket let's move on to the next question what are the storage classes
available in Amazon S3 here the storage classes that are available in the Amazon
S3 are Amazon SC glaciers instant retrieval storage class followed by
Amazon S3 Glacier flexible retrieval storage class Amazon S3 Glacier deep
Archer also we have S3 Outpost storage class followed by Amazon F3 standard
infrequent access also we have Amazon S3 one zone infrequent access Amazon S3
standard Amazon S3 reduced redundancy store storage finally we have Amazon S3
intelligent tiring now we will move on to VPC AWS interview questions let's see
what is Amazon virtual private Cloud that is VPC and why it is used virtual
private cloud is a best way of connecting to your Cloud resources from
your own data center once you connect your data centers to the VPC that is
what your private cloud in which your instances are present each instances is
assigned to a private IP address that can be accessed from your data center
that way you can access your public Cloud resources as if they were on your
private Network now let us see the next question how do you connect multiple
site to a VPC that is virtual private Cloud The Hub features of VPN Gateway
allows the large scale Enterprise is to connect multiple offices to each other
and it connects to multiple offices to Virtual private Cloud 2. now we will see
how to connect multiple site to a VPC now we have different steps over here
we'll start with the first step by creating a VPN gateway then after you
create a VPN Gateway you need to create a customer Gateway for each offices in
the next step we need to create an IP section for VPN connection for each
office then in the fourth step you need to configure the routes for the VPN
Gateway and in the next step we need to configure the on-premises Gateway
devices and finally you need to test the network connectivity by following these
steps you can easily connect multiple site to a VPN let's move on to the next question name
and explain some security products and features available in VPC here is a
selection of security products and features let's see each in details we
have security groups network access control list and flow logs here the
security groups acts like a firewall for ec2 instances by controlling in bond and
outbound traffics at the instant level now we will see the network access control list it acts like a firewall for
the subnets controlling the inbound and outbound traffics at the subnet level
then finally the flow logs these capture the in-bonds and outbound traffic from
the network interfaces in your VPC let's move on to the next concept that is
cloud formation let's see the question what are the steps involved in our cloud
formation solution so here are the few steps involved in cloud formation here
in the first step in the cloud formation is to create or use an existing cloudformation template using Json or
yaml format in The Next Step you can save the code in an S3 bucket which
serves as a repository for this code and in The Next Step you can use AWS cloud
formation to call the bucket and create a stack on your template and finally
this cloud formation reads the file and it understands the services that are
called and their orders and the relationship between the services and it
Provisions the services one after the other now let us see the next question
how is AWS cloud formation different from AWS elastic Beanstalk now let us
see the difference between these both AWS cloud formation and AWS elastic bean
stack here in AWS cloud formation it helps you to provision and describe all
the infrastructures resources that are present in your Cloud environment on the
other hand AWS elastic beam stack provides an environment that makes it
easy to deploy and run the application in the cloud AWS cloud formation
supports the infrastructure needs of various type of applications like Legacy
applications and existing Enterprise application on the other hand AWS elastic bean stack
is combined with the developer tools to help you to manage the life cycle of
your application with that we'll move into the next question what are elements
of AWS cloud formation template here the AWS cloud formation templates are yaml
or Json formatted text files that are compressed of five Essential Elements
they are template parameters output values and data tables resources and
finally file format versions now let us see the questions from elastic block
storage the first question here in elastic block storage is how can you
automate easy to backup using EBS now here are the following steps in order to
automate easy to backup using EVS first let's get the list of instances and
let's connect to the AWS through API to list the Amazon EBS volumes that are
attached locally to the instances Now list the snapshot of each volume and
assign a retention period of the snapshot later on create a snapshot of
each volume now make sure to remove the snapshot if it is older than the
retention period by doing this you can automate easy to backup using EBS now
let's move on to the next question what is the difference between EBS and
instant storage here the EBS is a kind of permanent storage in which the data
can be restored at a later point when you save the data in the EBS it
stays even after the lifetime of the ec2 instance on the other hand instant
storage is a temporary storage that it is physically attached to the host
machine with an instant storage you cannot detach one instance and attach it
to the another unlike in EBS the data in an instant storage is lost if any
instance is stopped or terminated let's see the next question can you take a
backup of EFS like EBS if s can you please State how the answer for this is
obviously yes you can use the EFS to EFS backup solution to recover from unintended
changes or deletion in Amazon EFS now if you follow those steps given below you
can easily take the backup if you follow these steps the step one is to sign into
the AWS Management console and you need to click the launch EFS to EFS restore
button then in the next step you can use the region selector in the console
navigation bar to select the region after that you need to verify if you
have chosen the right template on the select template page then you need to assign a name to your solution stack
finally you need to review the parameters for the template and modify
them if necessary let's move on to the next question on elastic load balancing
what are the different types of load balancer in AWS here are the three types
of load balancer that are supported by the elastic load balancing those are
application load balancer Network load balancer and finally classic load
balancer and the next question is what are the different uses of the various
load balancer in AWS load balancing so as I said in the previous question we
had three types of load balancer that is application load balancer Network load
balancer and classic load balancer now let us see each of it in a detail Now
application load balancer used if you need flexible application management and
TLS termination now Network load balancer are used if you require Extreme
Performance and static IPS for your applications now what does the classic
load balancer do it is used if your application is built within the ec2
classic Network now in the next question we will see the difference between the cluster and load
balancing here the cluster is a method of turning the multiple computer servers
into a cluster which is a group of servers that act like a single system
and also you can have a active to active cluster but it will require multiple
instances of SQL servers running on each node now what is load balancing here the
load balancing is about the distribution of workloads across multiple Computing
resources such as computers server cluster Network
links Etc now let's see the questions from AWS security Now what is identity
and access management and how does it work now the identity and the access
management that is IAM ensures that the right people and the job roles in your
organizations can access the tools they need to do their jobs now here the
identity management and the access system enables your organization to
manage the employees app without logging into each app as an administrator
identity and the access management systems enables your organization to
manage a range of identities including people software and Hardware like
Robotics and iot devices let's see the next question here what are the
different AWS IAM categories that you can control by using AWS IAM you can
create and manage the identity and access management users you can also
create and manage the IAM groups you can manage those security credentials of the
users too also you can create and manage the policies to Grant access to AWS
services and resources now let us see the difference between IAM role and IAM users the two key
difference between the IAM role and IAM uses are identity access management role
is an IAM entity that defines a set of permissions for making AWS Services
request while an IAM user has a permanent long term credentials it is
used to interact with the AWS Services directly in terms of in the IAM role it
is trusted entity like IAM users applications or an AWS Services assuming
the role whereas the IAM users has full access to all the AWS iam
functionalities let us see the question from AWS Route
53 now let us see what is Amazon Route 53 here the Amazon Route 53 is a highly
available and scalable domain name systems web service here the Route 53
connects the user request to the internet application that are running on
the AWS or on premises the name refers to the TCP or the UDP Port 53 where the
DNS server requests are addressed the name of our services that is Route 53
comes from the fact that DNS servers respond to the queries on Port 53 and it
provides the answers that route end users to the applications on the
internet let's move on to the next question what is the difference between the latency based routing and Geo DNS
here the geo-based DNS routing takes the decisions based on the geographic
location of the request whereas the latency based routing utilizes the
latency measurements between the network and the AWS database centers here the
latency based routing is used when you want to give your customers the lowest
latency possible on the other hand geo-based routing is used when you want
to direct the customer to different websites based on the country or the
region they are browsing from with that I hope you understood let's move on to
the next question how does Amazon Route 53 provides High availability and low
latency here we have around three resources that is globally distributed
servers dependency and optimal locations here about the global distributed server
the Amazon is a global Service and consequently has a DNS Services globally
any customer creating a question from any part of the world gets to reach a
DNS server local to them and that provides the low latency now let us see
the dependency Route 53 provides a high level of dependability required by the
critical applications now if we see the optimal locations here the Route 53 uses
a Global anycast Network to answer those queries from the optimal position
automatically let's see the next question from configure AWS how does AWS
config work with the AWS cloudtrail here the AWS cloud trail records the user API
activity on your account and allows you to access information about the activity
by using the cloud shell you can get the full detail about the API actions such
as the identity of the caller time of the call request parameters and response
elements on the other hand AWS config records point in time configuration
details for your AWS resources on configuration items you can use the
configuration item to ascertain what your AWS resources looks like at the
given point of time whereas by using the cloud trail you can quickly answer who
made an API call to modify the resources also you can cloud trail to detect if a
security group was incorrectly configured let's move on to the next question the question is like can AWS
config aggregate data across the different AWS accounts the answer is yes
you can set up the AWS config to deliver the configure station updates from the
different accounts to one S3 bucket once the appropriate IAM policies are applied
to the S3 bucket now let us see the database AWS questions how are reversed
instance different from on-demand database instances here the reverse
instance and on-demand instance are the same when it comes to the functions but
they only differ in how they are built the reverse instance are purchased as
one year or three year reservations and in return you get very low Harley based
pricing when compared to the on-demand cases that are built on the hardly basis
so these are the difference between the reserved instances and on-demand
database instances with that we will move on to the next question here which
type of scaling would you recommend for RDS and why here there are two types of
scaling we have vertical scaling and horizontal scaling here in the vertical
scaling it lets you to vertically scale up your master database with the
pressure of the button a database can only be scaled vertically and there are
18 different instances in which you can resize the RDS on the other hand
horizontal scaling is good for replicas so these are read-only replicas that can
only be done through a Amazon Aurora we'll move on to the final question in
the database what is a maintenance window in Amazon RDS and will your database instance be
available during the maintenance events here in the database maintenance window
it lets you to decide when the database instance should be modified and the
database engine versions update and the software that patching have to occur the
automatic scheduling is done only for the patches that are related to security
and the durability by default there is a 30 minutes value assigned has a
maintenance window and the database instance will still be available during
the event though you might not observe a minimal effects on the performances
now let us see the advanced AWS interview questions here the first question in advance level
is Define the snapshot in AWS light sale now the point in time backups of easy to
instance it blocks the storage devices and the database are known as the
snapshot and they can be produced manually or automatically at any moment
your resources can always be restored using these snapshots even after they
have been created these resources will also perform the same task as the
original ones from which the snapshots were made let's move on to the next question explain Amazon ec2 root device
volume here the image that will be used to boot an easy to instance is stored on
the root device also this occurs when an Amazon Ami runs a new ec2 instance and
this root device volume is supported by EBS or are an instant store in general
the root device data on AWS EBS is not
affected by the lifespan of an ec2 instance with this we will see the next
question mention the different types of instances in Amazon ec2 and explain its
feature here we have general purpose instance compute
optimize instance then we have memory optimized then we have accelerated
Computing then finally we have the storage optimized so let us see these
all in detail here the general purpose instance these are used to compute a
range of workloads and it is Aid in the allocation of processing memory and
network resources now let's see the compute optimize instance these are
ideal for compute intensive applications they can handle batch processing
workloads high performance web services machine learning interference and
various other tasks then comes the memory optimized they process the
workload that handle massive data set in memory and deliver them quickly and the
next one is accelerated Computing it aids in the execution of floating Point
number calculations data pattern matching and graphic processing so these
functions are carried out using the hardware accelerators then finally we have the storage
optimized they handle tasks that require sequential read and write access to the
big data sets on local storage let's see the next question what do you understand
by changing in Amazon ec2 here to make limit Administration easier for the
customers the Amazon ec2 now offers the option to switch from the current
instance count based limitations to the new CPU based restrictions as a result
when launching a combination of instance type based on the demand utilizing it
measured in terms of the number of CPUs now let us understand the difference
between a spot instance an on-demand instance and a reserved instance
here spot instants are unused ec2 instance that users can use utter
reduced cost when you use on-demand instances you must pay for the Computing
resources without making a long term obligations when you use the reserved
instance preserved instances on the other hand allow you to specify the
attributes such as the instant type platform Tendencies region and
availability Zone now let us see some of the scenario based questions here now
try answering this scenario based question I have some private servers on my
premises also I have distributed some of my workloads on the private Cloud what
is this architecture called I will give you some of the options here
such as the virtual private Network private Cloud the virtual private cloud
and the hybrid cloud and if you know the answer please pause the video and just
go down below and comment there the answer is the hybrid Cloud now let us
see the explanation here so the answer for this is hybrid Cloud
because we are using both the private cloud and your on-premises services that
is the private Cloud to make this hybrid architecture easy to use wouldn't it be
better if your private and public Cloud where all on the same network this is
established by including your public Cloud servers in a virtual private cloud
and connecting this virtual cloud with your on-premises services using a
virtual private Network I hope you understood this question let's move on to the next scenario based question
assume your business prefers to use its email address and the domain to send and
receive the compliances of emails so what service do you recommend to
implement it easily and budget friendly now the answer for this would be the
Amazon simple email Services call Amazon SES it is a cloud-based email sending
services so this could easily solve the problem of sending the email and
receiving the email at a instance with this let's move on to the next
question on an easy to instance an application of yours is active once the
CPU usage on your instance hits 80 then you must reduce the load on it what
strategy do you use to complete this task if you have any idea on this
question please comment below now the answer for this would be now it can be
accomplished by setting up an auto scaling group to deploy the additional
instances when an ec2 instances CPU use suppresses 80 percent and by allocating
the traffic across the instances via the creation of an application load balancer
and the designation of the ec2 instances hash Target instances let's move on to
the next question here we have an another scenario based question now you
need to configure an Amazon S3 bucket to the server static assets for your public
facing web application which method will ensure that all the objects uploaded to
the bucket are set to public red here are the options set permissions on the
object to public rent during upload the next option is configure the bucket
policy to set all the objects to public red the next option will be to use AWS
identity and access management role to set the bucket to public red finally the
Amazon S3 objects default to public rate so no action is needed now comment the
answer below in the comment section here the option is configure the bucket
policy to set all the objects to public red here I will give you the explanation
for this option here that rather than making changes to every object it is
better to set the policy for the whole bucket here the identity and access
management is used to give more granular permissions since this is a website all
the objects would be public by default now let us see the final question in the
scenario paste in AWS there are three different storage services are available
such as EFS S3 and EBS when should I use
Amazon EFS versus Amazon S3 versus Amazon elastic Block store here the
Amazon web services that is AWS offers the cloud storage services to support
wide range of storage workloads Amaze on EFS is a file storage service for use
with Amazon compute such as ec2 containers and serverless and
on-premises servers too and Amazon EFS provides a file system interfaces file
system access semantic such as strong consistency and file locking and
concurrently accessible storage for up to thousand of Amazon ec2 instances
here the Amazon EBS is a block level storage services for use with Amazon ec2
the Amazon EBS can deliver performance for workloads that require the low
latency access to data from a single ec2 instances
here comes the AWS S3 here this AWS S3
is an object store Services a AWS S3 makes the data available through an
internet the Amazon S3 makes the data available through an internet API that
can be accessed anywhere if you have any doubts about this topic then quickly
head over to the comment section and post your questions over there we will
get back to you as soon as possible until then Happy learning
[Music] now let us not waste any time and go
ahead and happy first question who is the devops engineer now this machine may look simple but what the
interviewer does not want you to know is that this question is a trick question he wants to understand the in-depth
knowledge of how prepared are you for the interview and depending on your answer he can understand or identify if
that person truly has a skill and knowledgeable about the role that you've applied for now a fresher might answer
this question like a devops engineer is a person who works with both software developers and the IT staff to ensure
smooth code releases or they might have answered something like this a devops engineer is somewhat knowledgeable about
the sdlc or we can say a software development cycle and automation tools for creating the CI CD pipeline yes both
answer is an accurate description of the role of a devops engineer but I suggest you answer this question something like
this a devops engineer work at the intersection of development operation and they are responsible for helping to
bridge the gap between these two teams now they work with developers to understand the codes and its
dependencies and they work with the ID staff to ensure that the codes can be effectively deployed and maintained in
the production environment now devops engineer often have a strong background in both development and in operation and
they use this knowledge to help streamline the software development and releases processes now they may be responsible for
automating various parts of the process such as testing deployment and monitoring to improve efficiency and
reduce the risk of errors now moving on to the next question we have why has devops become famous or why
do you think devops has become the trend in today's generation or error the main point in asking you this question is
they want to understand from their point of view if you understand the job rules and responsibility in the organization
as a devops engineer or an architect or an associate Etc or whatever the job
role that you have applied for now you might answer this question by saying devops has become famous due to these
following reason frequent features deployments then reduce time between paths fixes reduce the failure rate of
releases and quicker recovery time in case of release failures now what if I
tell you there's a better way to answer this question so you can say something like a devops devops is a software
development and delivery approach that emphasizes collaboration and communication between software
developers and the ID operation professionals now devops has become famous because it allows organization to
deliver software updates and application faster and more reliable by automating the build test and deployment process
devops helps teams to reduce the time time it takes to develop and release software this is essentially important
in today fast-paced business environment where companies needs to be able to quickly response to changing Market
condition and customer needs now you can also add more information by saying additionally devops helps organization
to improve the reliability and stability of the software as well as the security
of the system by adopting devops practices organization can more easily adopt new technologies and deliver value
to the customer faster which has become a motto for all organization now coming to the third question we have
what is the use of SSH now you might answer this question like SSH which stands for security is an
administrative protocols that allows users to access and control remote servers over the internet via the
command line you might have also added this path to the answer by saying SSH also includes mechanism for remote user
authentication input communication between a client and the host and output
delivery back to decline okay that is correct but let us explain to the interviewer your answer to that question
in such a way that they understood you have the knowledge and the basics are clear and you know what you are saying
so you can start the answer by saying something like SSH is a secure way to remotely connect to a computer or server
and perform tasks as if you were physically present at the location it is
commonly used to remotely administer server configure and manage network devices and automate tasks then you can
add some more inputs to the answer by saying SSH uses encryption to secure the
connection between the client that is your computer and the server ensuring that the data transfer over the
connection is protected from eavesdropping this makes it a useful tool for remote access and
administration as it allows users to securely connect to and perform tasks or
remote server without worrying about the confidentiality of the data being transmitted in addition to providing
secure remote access SSH can also be used to transfer files between computers redirect input and output and tunnel are
the protocols such as HTTP FTP and telnet through an encrypted connection
now coming to the next question we have what is configuration management now you
can start answering this question by saying it is the systematic handling of changes
in such a way that the system does not loses its Integrity over time this includes policies techniques procedures
and tools for evaluating managing and tracking changes proposal as well as
maintaining appropriate documentation that is correct but what if I tell you guys that there is a better way to
phrase the answer to this question so you'll be that candidate that stand up amongst other candidates
now you can start it by phrasing the answer like this now configuration management is a practice
that helps organization maintain the integrity and functionality of the system by systematically handling
changes to their configuration it involves identifying organizing and
controlling the configuration of Hardware software and related infrastructure to ensure that the system
operates as intended and to make it easier to identify and fix problems when they occur now configuration we can also
add configuration management is an important aspect of it infrastructure management asset Health Organization
keep track of the various components that make up their system such as servers storage device and networking
equipments it also help organization maintain consistency and control over
the configuration of their system which is critical for ensuring that they function properly and meet the needs of
the organization now to implement the configuration management organization typically uses a combination of tool
processes and policies some common tools used in the configuration management includes the Version Control System
configuration management databases and asset management system these tools helps organization track and manage
changes to the system as well as keep tracks of the various components that make up their system
till here they are already impressed by your answer but you can go further and add more details to that answer by
saying in addition to tools organizations also needs to establish processes and policies for managing
changes to the system thus processes and policies outline these steps that needs
to be taken when making changes to a system as well as who is responsible for making those changes this helps ensure
that the changes are made in a control and consistent manner which is critical for maintaining the integrity and
functionality of the system overall configuration management is a key practice that helps organization
maintain control over the system and ensure that they function properly it
involves the use of tools processes and policies to identify organize and
control the configuration of Hardware software and related infrastructure in a
way that helps maintain the system integrity and functionality
now the next question they might have asked you is what is the importance of having configuration Management in
devops so basically this question is a part of the previous question now what
they want to see is if you have any knowledge about the tools so you might answer this question
something like configuration management is important as it assess the team in automating time
consuming and tedious thoughts thereby improving the organization performance and Agility but at this point we have
understood that this is not how we go about with the answer those who are a part of our Enrico family know this
since they have already had a mock interview preparation test which helps them deliver the answer in a better way
and is the interview so let me help you answer this version so that the interviewer will remember you and put
your site from the other candidate you can answer this question in the following manner configuration
management is an important aspect of devops because it helps organization maintain control over the system and
ensure that they function properly it allows organization to identify organize
and control the configuration of Hardware software and related structure in a way that helps maintain the system
integrity and functionality guys always remember to add key points
to your answer in every interview because it is always a good impression so you can say something like there are
several key benefits to using configuration Management in a devops environment first is it has improved
efficiency now configuration management tools and processes help automate the management of system and infrastructure
which can improve the efficiency of the development and operation teams now the
second benefit is configuration management and hence disability now what I mean by that is configuration
management tools provide a central repository for tracking and managing changes to system which can improve
visibility into the configuration of the system now the third benefit is configuration
management is a better collaboration now configuration management tools and processes help facilitate collaboration
between the development and the operation teams by providing a common platform for managing changes to the
system now the fourth key benefits of configuration management is an increase in the speed
what I mean by this is configuration management can help speed up the delivery of new features and updates by
streamlining the process of managing and deploying changes to your system now the fifth benefit and the last
benefit is its enhanced security Now what I mean by enhanced security is that they can help improve the security of
system by helping the organization identify and fix vulnerabilities more
quickly and by providing clear record of changes made to the system now we can say overall configuration
management is an essential component of devops that helps organization maintain control over the system and improve
their efficiency speed and security of their operation the next question that we have is very
important especially from a fresher point of view since there is a higher chance that this question will be asked
a majority of the time now the question is what is continuous integration I have
already told you guys that this is an important question so I will straight away tell you guys how you should deliver this answer to the interviewer
so you can answer this question by saying continuous integration is a software development practice in which
developers regularly merge their code changes into a central Repository the
central representory is then built and tested automatically ensuring that the code changes do not introduce any new
defects one of the main goals of CI is to catch errors as early as possible in the
development process by integrating code changes frequently developers can detect and fix defects more quickly rather than
waiting until the end of the development cycle to find and then fix a large number of effects all at once so this
can benefit by saving time and resources which leads to a higher quality final product
now you can also explain the workflow of continuous integration as an example to the interviewer so you can say a
developer makes a code changes and pushes it to the central repository then the CI system automatically detects the
code change and start a build process as you can see in the diagram on the screen after that the CI system compiles the
code and runs automated tests to ensure that the code changes has not introduced any new defects now if the bill and
tests are successful then only the code is automatically deployed to a staging or production environment but in case if
the bill or test fails the developers is notified and can fix the issue and push the code changes again
so there are many tools available to support the CI including Jenkins Travis CI and circle CI now intervals Jenkins
is the most common tool used for continuous integration and you can share your experience of your project when you're working and
building the pipeline for the devops project hence you can say these tools can be configured to automatically Bill
and test score change and to perform other tasks such as deploying code to different environment or triggering
other automated processes in addition to catching defects early in the development process TI can also help to
improve collaboration amongst Developers by integrating code change frequently developers can see how the changes
affect the overall code base and make necessary adjustment now this can help to prevent conflicts
and reduce the risk of Court breaks hence CI can also improve the overall quality of a software project
that is by regularly testing and deploying code developers can ensure that the code base is always in stable
and Deployable State now this can make it easier to release new features and updates to customer and can improve the
reliability and performance of the software overall continuous integration is a valuable software development
practice that helps to improve the quality the speed and collaboration of a project it is an important tool for any
software development teams looking to improve its process and deliver high quality product to its Customs you can
explain it to them even furthermore by giving them a live example of how working with A continuous integration is
a hazardous process and how developer phase an issue after working weeks or months on a court and later on they
found out that there were lots of bugs and error in dark so let us start with an example
say developers are creating a software model and have worked for three weeks straight now that is a lot of code
really now after three weeks you think that your job is done for now now that is what you thought right every
developers suppose they sit there and type for three weeks for a single project and then after three weeks they
expected like yeah there we have completed the project and it's good to go as per the process all this code will
be fetched by the build server and this code is Bill tested and later on they found out that there were lots of Errors
bugs conflicts and build failures now developers have to fix all these defects
now they have to rewrite the code at several places which is a lot of free work really
so what will this happen is now this could have been much much easier if the problem was detected very very early in
the process but then code was collected with defects for three weeks and now they have to fix all of that so the code
is getting merged into the repository but not really getting integrated the solution to this is very simple if they
had used continuous process now after every single comment from the developers the code should be built and tested so
there is no need for waiting and collecting all these code with bugs but then the developers commit several times
in a day so it is not humanly possible to do the build and release several times in a day now what I mean is the
manual process so that is simple we will just use an automation process
so when the developers commit any code and automated process it will fetch the code fill it and test it and send a
notification if there are any failures now as soon as the developers receive a fail notification they will fix the code
and commit it again so again Bill enters new changes if it is good then it can be version and
stored in a software repository and it is all automated like this any defects
can be caught as soon as it's merged with the code this whole automated process is called continuous integration
or CI for short now the goal of CI sbtech defects at a very early stage so
it does not multiply now after this question we have a very similar question like earlier where it
is a part of the previous question so let us go ahead and see the next question why is continuous integration
needed so there are two ways to answer this question either you can go with the first one or you can go with the second
but my suggestion is going to be the second one so it is up to you to decide which answer best suits for you now you
can start by saying it has been discovered that incorporating continuous integration for both development and
testing has improved software quality and drastically reduce the time required to deliver software features because
each commit to the shear repository is automatically built and run against the unit and integration test cases the
development teams can detect and fix errors earlier so this is one answer or
you can answer this question in this manner condis integration is a software
development practice that aims to minimize the time between writing the code and deploying it to the production
it is designed to improve the quality speed and collaboration of a software
project by catching defects early in the development process and integrating code changes frequently now I have already
told you guys and stress out and putting your answer in the key point is a very important and beneficial from an
interview point of view so go ahead and impress the interviewers by saying these following points now
there are several reason why CI is an important practice for software development teams first CI can help to
catch defects early in the development process by building and testing code changes automatically CI can help to
identify defects and issues before they become more difficult and costly to fits
this can save time and resources and lead to a higher quality final product secondly CI can improve the speed of the
development process by integrating code changes frequently now developers can deploy new features and update the
customer more quickly this can be especially important in fast-paced environment where the ability to quickly
release new features can be a competitive Advantage thirdly is CI can improve collaboration amongst developers
Now by integrating code changes frequently developers can see how the changes affect the overall code base and
make the necessary adjustment now this can also help to prevent conflicts and reduce the risk of code break which can
save times and improve the overall efficiency of the development process thirdly CI can improve the overall
quality of a software project now this can be done by regularly testing and deploying codes developers can ensure
that the code base is always in a stable and Deployable State now this can make it easier to release new features and
updates to customers and can improve the reliability and performance of the software finally CI can also help reduce
the risk of costly failures by catching defects early in the development process CI can help to prevent major failures or
rth that can be costly and damaging to a company's reputation now you can add the
conclusion by saying overall CI is an important practice for software development teams because it can help to
improve the quality speed and collaboration of a project it is an effective way to catch defects early in
the development process improve the speed of the development process improve collaboration among developers improve
the overall quality of a project and reduce the risk of costly failures
now another important question that devops pressure encounter in an interview is what is continuous testing
now like I said before some of you out there might just answer and try to get it done with the answer but a very
important Point here to highlight is that you should always try to engage as much as possible with the interviewer in
that manner he knows you not only have the skill but also the determination and the perseverance
even if there are some questions that you're not able to answer instead of being silent it is always better to engage and ask the doubts of the
question from the interviewer or say something like can I get back to you on this topic later now you have to make
sure that you collect all the information at the end of the interview so that later you can answer those questions
so let us go back to the question which is what is continuous testing now the first way to answer this question is
continuous testing is a devops phase that entails the process of running automated test cases as part of an
automated software delivery Pipeline with the sole goal of receiving immediate feedback on the quality and
validation of the business risk associated with the automated bill of code developed by Developers
the best way to answer this question is you say something like this continuous testing is a software testing practice
in which tests are continuously run typically in an automated fashion as part of the software development process
now the goal of continuous testing is to provide rapid feedbacks to developers on the quality and correctness of their
code enabling them to identify and fix issue as soon as they are introduced not
only it helps to reduce the time and cost of debugging and fixing issue later in the development cycle but it can also
improve the overall quality and reliability of the software one key aspect of continuous testing is
automation now automated tests can be run much faster and more frequently than the manual test and can be run at any
stage of the development process from code check-in to deployment this enables
developers to get feedback on their code changes almost immediately rather than having to wait for a separate testing
phase to be completed now you can also add another important aspect of continuous testing is the use of test
framework and test infrastructures that make it easy to run a wide variety of tests including unit test integration
test and end-to-end tests now these Frameworks and infrastructure can be
integrated with the development process allowing tests to be run automatically as code is check-in bill or deployed now
after this you can add some example to your answer by saying one example of continuous testing inaction is the use
of countries integration system in a CI workflow developers check the code changes into a shared repository and a
CI system automatically build and test the code now if the test is passed the code is deployed to a staging
environment where it can be further tested now if the test fail the CI system can alert the developers and
provide them with the information on what went wrong and in the enable them to fix the issue and rerun the test
another example of continuous testing is the use of countries delivery system in
a CD workflow code changes are automatically built tested and deployed to a production as soon as they are
ready now this require a high level of Automation and testing as well as robust
deployment process to ensure that code changes are released smoothly and without interrupting service so cornless
testing can be applied to a wide variety of software development projects by saying continuous testing can be
applied to a wide variety of software development projects including web application mobile apps and back-end
Services it can be especially useful in Agile development environment where code changes are made and released on a
frequent basis by providing rapid feedback on code changes continuous testing helps to
ensure that software is delivered quickly and with a high level of quality
the last question for the devops interview question for pressure is what are the three important devops kpi or
the key performance indicator now to answer this question you can go about in this field you can say there
are many possible devops key performance indicator that an organization may choose to track
and the specific kpi that are the most important will depend on the goals and priorities of the organization however
some commonly track devops kpis include deployment frequency now this kpi
measure how often new codes is deployed to the production so high deployment frequency can be assigned of a
well-functioning devops process as indicates that a team is able to quickly and reliably bring new features and
pixels to users second is lead time for changes risk API measures the amount of time it takes or
change to be made to the system now from the time the change is requested until it is deployed to the
production so the short lead time can be assigned of a streamline and efficient devops process as it indicates that the
team is able to quickly respond to changing business needs now the third key performance indicator
is the mean time to recovery now this kpi measures the amount of time it takes to restore service after a failure or an
outage now a low mttr or a mean time to recovery is important for maintaining a
high level of availability and ensuring that the system can quickly recover from any disruptions so you can see these
were the three important devops kpis since they have specifically asked for just three devops kpis or key
performance indicating so answering it till here will be enough but if you want to know more about the devops kpi or in
case the interviewer asks for more than three devops CPI then you can add the following as well now at the possible
devops kpi includes the change failure rate now this kpi measures the percentage of changes that result in the
failure or regression now a low change failure rate is the importance for maintaining the stability and
reliability of the system then we have the mean time between failures now this
kpi measures the average time between the failures or outage now a high mtbf or a mean time between failures is
important for maintaining the reliability and the availability of the system and another important kpi is
service level agreement compliance now disk apis measure the percentage of the
time that the system is able to meet the service level Target specified in the SLA so a high SLA compliance is
important for maintaining the reliability and the quality of the services and the last keypad that we're going to
talk about here now is the customer satisfaction this kpi measures the level of satisfaction of the users or customer
of the system so a high customer satisfaction is important for maintaining a positive reputation and
for retaining the customers so it is important to know that these are just a few examples of the devops kpi and the
specific kpis that are most important for a given organization will depend on its specific goals and priorities it is
also important to carefully consider which kpi are most relevant and meaningful for the organization and to
try and analyze them over time to gain insight and drive continuous Improvement let us jump into the top important
interview questions for an advanced or an experienced level now let us start with our first question
now the first question is can you explain the shift left to reduce failure Concept in devops
now to understand what this means we must first understand how the traditional sdlc Cycle Works the
traditional cycle has two major sites a planning designing and development phases comprises the Cycles left side
and stress testing production staging and user acceptance are all part of the
Cycle's right side so shifting left in devops simply means moving as many tasks
that normally occur at the end of the application development process as possible into the early stages of
application development as you can see from the graph if the shift left operations are followed the
chances of error occurring during the later stage of application development are greatly reduced because they will
have been identified and solved during the early stages now one of the most common way to
achieve shift left in devops is to have a collaboration with the development team to create deployment and test case
automation so this is the first and the most obvious steps towards shifting left
this is done due to the well-known fact that failures in the production environment are frequently not seen
earlier these failures can be traced back to the development teams use of various deployment procedure while
developing these features then a second common ways is production deployment procedia are not always the
same as development procedures there may be difference in tooling and process may also be manual at times now both the
development and the operation teams are expected to take ownership of developing and maintaining standard deployment
procedures that make use of the cloud and patent capabilities so this contributes to the assurance that the
production deployments will be successful patent capabilities are used to avoid configuration inconsistencies
in the various environments being used so this will necessary collaboration between the development and the
operation teams to create a standard process that guides developers in testing their application in the
development environment in the same way that they test in the production environment then we can further say to implement a
shift lab strategy in a devops environment it is important to have the right tools and processes in place
so this may include countries integration and continuous delivery tools that allow developers to build
test and deploy code quickly and efficiently it may also involve adopting
Agile development methodologies which emphasizes rapid iteration and the early and continuous delivery of valuable
software in summary shift left is a practice in the software development and testing
process that aims to detect and fix issues as early as possible in the process by shifting activities that are
typically done later in the process to earlier stages companies can improve the quality of their product reduce risk and
increase efficiency now coming to the next question we have do you know about postmortem meeting in
devops so let's see how we can answer this following question in the context of devops a post-mortem meeting also
known as a post instant review or a post incident debrief is a meeting that takes place after an instant has occurred in
order to review what happened and identify what can be done to prevent similar incidents from occurring in the
future now the goal of a post-mortem meeting is to improve the system or process that has involved in the
incident rather than assigning blame to individuals so postmodern meeting can be helpful for
a number of reasons we provide an opportunity for teams members to come together and discuss the incident in a
structure way and to identify any problems or issues that may have contributed to the incident
they also allow teams members to share their insight and perspective on what happened which can help to identify the
root causes and the potential Solutions there are a few key steps that should be followed in a post-mortem meeting
number one is review the incident now this involved discuss what happened including the symptoms causes and the
impact of the incident now the second is identifying the root causes now the team should work together to identify the
underlying cause of the instant rather than just the symptoms next is determine corrective actions
based on the root cause identify the team should brainstorm potential solution or corrective action that could
prevent similar incidents from occurring in the future coming to the next point we have Implement corrective actions so
the team should work together to implement the agreed upon corrective actions then we have the follow-up so it
is important to follow up on the corrective actions to ensure that they have been implemented effectively and
that the issue has been resolved post-modern meetings can be effective way to improve processes and systems but
they are only as useful as the actions that are taken as a result of them it is important to follow through on a
corrective actions that are identified and to keep track of their effectiveness over time
coming to the next question we have what is the concept behind sudo in Linux operating system
so let us see how to go about with this answer the sudo command in Linux allows the
user to execute command answer not the users usually this super user or the roots this is useful in cases where a
user needs to perform actions that require elevated privileges such as installing software or modifying system
file when the user tries to run a command using sudo the system check for the following to see if the user is
allowed to run the command as another user if the user is allowed and the system prompt for the user's password
and if it is entered correctly then it runs the command if the user is not allowed the command
is not run and the user is given an error message one of the benefits of using Sudo is that it allows the system
administrator to Grant specific users the ability to run specific commands as another user rather than giving all user
pull root access now this can improve the security by limiting the actions that user are able to perform on the
system however there are a few potential downside to using sudo one is that it
can be confusing for user who are not familiar with it as they may not understand why they are being prompted
for a password when they're trying to run certain commands and notice that it requires an additional step for users to
run commands that require elevated privileges which can be inconvenient overall Sudo is an important tools for
managing user privileges on a Linux system it allows the system administrator to Grant specific users
the ability to perform specific tasks that require elevated privileges while
also providing a means of logging and auditing the command that are run on the system
so that was about the concept behind sudo in a Linux operating system now moving on to the next question we have
can you explain the architecture of Jenkins as you all know Jenkins is the most
important tools used in devops so let us see the explanation of the following question Jenkins is a popular open
source continuous integration and continuous delivery tools that is used to automate the build test and
deployment of software projects now you can see here is an overview of the architecture of gen kids
now we have the Master Slave architecture now Jenkins uses a Master Slave architecture to distribute the
bill and test loads to multiple machines the master node is responsible for scheduling and dispatching build jobs to
the slave nodes as well as recording the build results and Publishing the artifacts the slave nodes are
responsible for executing the build jobs assigned to them by the master this allows Jenkins to scale to support large
and complex projects with many builds and tests now the next is we have is the
plugins now Jenkins has a rich plug-in ecosystem that allow it to integrate
with a wide variety of tools and services for example you can use the plugin to
integrate Jenkins with a version control system like get test tool like junit and
deployment tools like and spill now this allows Jenkins to customize to fit the needs of your projects next we have is
the web UI or the user interface Jenkins provide a web-based user interface that allows you to configure
and manage your builds as well as view the build results the web UI is accessible from any web browser which
make it easy to use Jenkins from anywhere so here's an example of how Jenkins could be used in a project
a developer pushes code changes into the project in repository Jenkins now
receives a notification of the code change and trigger a bill of the project now the Jenkins masternode dispatches
the build job to a slave node the slave node checks out the code from the get repository and run the build script
example like amazing build now if the bill is successful Jenkins publishes the bill artifact in a jar file to a
repository that is your Nexus Now Jenkins sends or notification to the development teams about the build status
now if the bill is successful Jenkins can also trigger automated tasks and deployment I hope this helps let me know
if you have any questions about Jenkins or would like to know more information this architecture allows Jenkins to
scale horizontally by adding additional save nodes as needed to handle the workload it also allows the master to
delegate tasks to save that half the necessary resources and tools to execute them which can improve the efficiency of
the build process now coming on to the next question we have can you explain the infrastructure
as code concept let us see how we shall answer this quality question
infrastructure as code is a server development practice in which infrastructure is treated as a version
entity that can manage using Version Control System just like the application code the goal
of an infrastructure as code is to allow users to create manage and deploy infrastructure in a repeatable and
automated way one key benefit of IEC is that it allows infrastructure changes to
be tracked and audited in the same way the code changes are it also make it easier to roll back changes if something
goes wrong this can be particularly useful when managing large and complex infrastructure environment that include
many different components infrastructure as code refers to the practice of managing infrastructure in a Version
Control file rather than manually configuring resources through a user interface
the imperative approach is similar to writing a script or a set of instruction that tells the system exactly what to do
in order to achieve the desired outcome it is similar to how you will manually set up and configure your infrastructure
the declarative approach on the other hand involves defining the desired state of the infrastructure and letting the
system figure out how to reach that state this approach is more like writing a configuration file let's specify what
the end result should look like rather than specifying the steps that it needs to get there now both approaches have
their own benefits and drawback and it is up to the users to decide which one is more suitable for their needs
now there are several tools and technologies that can be used to implement the IAC including
configuration management tools such as ansible chef and puppet which allow
users to describe and manage the desired state of the infrastructure cloud formation and terraforms which are tools
for creating and managing infrastructure on cloud platforms such as AWS and Resort
custom scripts and tools that allow users to automate infrastructure tasks using languages such as python or Bash
now IC can be applied to a wide range of infrastructure components including servers databases networking
configuration and many more so by using IEC organization can more easily manage
and maintain the infrastructure in a consistent and reliable way now moving on to the next question we
have what is pair programming a pair programming is a software
development technique in which two programmers work together at one Workstation now one programmer called
the driver write score while the other called The Observer or Navigator preview each lines of the code as it is typing
so the two programmers switched both frequently peer programming can be an effective way
to reduce defects in code improve code readability and share knowledge among team members it can also be an effective
way to train new programmers or to familiarize programmers with a new code base
now coming to the next question we have what is a blue or green deployment pattern now let us see how to answer
this following question blue green deployment is a strategy for rolling out updates to software systems that allows
for minimal downtime and disruptions to users now the basic idea is to maintain two
identical environment one called blue and the other called cream the blue environment is the current production
environment where the green environment is a new version of the software that has been deployed but it's not yet live
to perform a blue or green deployment the new version of the software that is the Green version is first deployed into
the green environment now this can be done by building the software and deploying it into the staging
environment or by using a continuous delivery pipeline to automatically build and deploy the software
now once the green environment is up and running traffic is redirected from the blue environment to the green
environment now this can be done using a load balancer or by changing the DNS records to point to the green
environment once the traffic has been redirected the blue environment can be taken offline for maintenance or
decommission this allow the teams to roll back to the blue environment if there are any issues with the green
environment without any downtime or disruption to the users for example let's say you have a web application
that you want to update now the current version of the application is running in the blue environment and you have to
deploy a new version of that application to the green environment now to perform the blue or green deployment you will
first test the green environment and to ensure that it's working properly right so once you are confident that the green
environment is stable you will redirect traffic from the blue environment to the green environment using a node balancer
or DNS records now once the topic has been redirected you can take the blue environment offline or you can
decommission it there are several benefits to using a blue green deployment first it allows you to
minimize downtime and disruptions to users hence traffic is redirected from
the old version of the software to the new version with minimal downtime second it allows you to easily roll back
to the previous version if there are any issues with the new version since the blue environment is still available and
then finally it allows you to test the new version of the software in a production-like environment before it
goes live which can help identify any issues before they are impacted by the users in summary you can see a blue
green deployment is a useful strategy for rolling out updates to software systems that allows for minimal downtime
and disruptions to users while also providing a way to easily roll back to the previous version and accessory
now moving on to the next question we have what is dog pile effect and how can it be prevented so let us see how to
explain the following question so the stockpile effect is a phenomenon that can occur when a website becomes
temporarily unavailable or slow down due to high influx of traffic this can
happen for a variety of Reason such as when a popular website links to the Target site the way the target site is
featured in the news or when the target site experience the research in traffic due to a viral event so to prevent the
dog pile effect it is important to ensure that a website has sufficient capacity to handle large amount of
traffic this can be achieved to a variety of Pleasures including scaling up the website infrastructure this can
involve adding more servers increasing the size of existing server or implementing a Content delivery Network
to distribute traffic across multiple servers the second is optimizing the website for performance this can involve
minimizing the size of website codes and assets implementing catching mechanism and optimizing the website database
and the third is implementing rate limiting this involves limiting the number of requests that a website will
accept from a single user or an IP address within a certain time frame but this can help prevent the website from
being overwhelmed by access traffic from a single source now the last point is by monitoring the
website for me now regular monitoring of the website performance can also help to identify potential issues before they
become a major problems now this can involve monitoring the website uptime the spawn times and error rates
now moving on to the next question we have what are the steps to be undertaken to configure get repository so that it
turns that the code suddenly checking task before any comments and how do you prevent it from happening again if the
sanity testing fails so let us explain the answer to this following question
to configure your git repository to run course sanity checking tools before any commits you can use the GitHub now
GitHub are scripts that run automatically before or after certain kids events such as committing pushing
and receiving there are a few steps to set up a GitHub to run quotes on rechecking the tools before the comments
now first step is by navigating to the GitHub directory in your local repository and the second step is create
a file called pre-commit in the GitHub directory and then you can also add the
following pre-commit files which you have can see here on the screen so I'll explain each and every line what
does it mean now the first line in the code is bin bash now bin bash is the sheet Bank line used in the strip file
to set bash that are present in the bin directory as the default shelf for institute man's present file it defines
an absolute path to deduction so to put it in simple term it is used for executing the script using the Tasha now
the second line of code is branch code therapy checking tools now the line
hashtag run code is a command that provides a brief description of the following code
so the housing balls indicates that everything followed on the same line is a command and it's not executed as a
part of the script now the line dot slash run code sanitycheck.sh runs the
runcodesantly check.sh script now the dot slash at the beginning of the line specify the current directory so the
script is executed from the current directory now this code will typically be used in the GitHub script that runs
code sanity checking tools before coming now will run the runcodes and the check.sh script to perform these entry
checks and then check the exact code of the script to determine whether the check pass or fail
if the check sphere the hook will abort the command and display an error message now we're coming to the last line of
this code let's go check the exit code of the previous command which is the Run codes and recheck.sh script in this case
now as you can see on the screen the dollar and question mark variables holds the exit code of the previous command
the if n statement then check whether the exam code is not equal to zero if
the exit code is not zero it means that the Run code scientific check dot asset trip has failed in this case the F
statement will execute the code inside the dance statement which display an error message and exit
the script with an exit code 1. see it doesn't mean a scale committed
aboard and then it will accept it now the exit 1 command causes the script to exit with an exit code of 1. now which
indicates that the script has failed the F statement is then closed with the FI keyword which standards for a finish
this code will typically be used in the GitHub strip that run code sanity checking tools before comments if the
sanity checks fail the hook will abort the commit and display an error message the exit code of the script will then be
checked by the git system and the commit will be rejected if the exit code is not zero
now we can make the pre-commit file executable by running the following command now the chmod command stands for change
mode and is used to change the permission on a file then as you can see here the plus X flag
tells the CH mod to add execute permission for the file D plus X flag
tells the CH mod to the user who owns the file in this case the command is
adding execution permission for the pre-commit files to the user who owns the file now this is typically done to
make a script file executable now this command will typically be used in a git repository to make a hook script
executable GitHub script that run automatically before or after certain
git events such as committing pushing and receiving now in order for the hook
script to be executed it must have an execute permission so that's why we have the CH mod plus X
the CH mod plus X command allows the work strip to be executed now every time you try to commit changes
in your repository the pre-commit hook will run the code sanity checking tools defined in the Run code
sandycheck.shcript if the Sunday checks fail the hook will abort the commit and display an error message now to prevent
this from happening again you can either enter fix the issue that caused the Sunday checks to fail or modify the Run
codes and to check that asset strip to ignore the failing checks now let us move to the last question of
this devops interview question for the advanced level the question is how can you ensure a script runs every time the repository
gets new commit through the get push now to ensure that the script runs every
time the repository gets a new comment through a git push you can use the GitHub call host receive
the post receipt Fork runs on the server side after all the revs have been updated to set up a post receive hook
you have a certain steps so the first step is you navigate to The Hook directory in the bare repository on the
server now where repository is the git repository that does not have a working directory and is used to push and pull
form second step is you have to create a new file called post receive in the Hope directory
then the third is you have add the following command line onto the post receipt file now I've already explained
you what bin slash bash is so let us move on to the second line of the code which is dot slash runscript.sh
as you can see the line hash diagram the strip is the command that provides a brief description of the following code
the Hashem bolt indicates that everything follows in the same line hashtag run the strip is a command that
provides a brief description of the following code now the hash symbol indicates that everything follow it on
the same line is the command and is not executed as a part of the script now the line dot slash runscript.sh runs the
following script now the dot slash at the beginning of the line specifies that the current directory so the script is
executed from the current directory this code will typically be used in a script or command that run other scripts
the dot slash transcript.sh command will execute the Run script dot sh script now
the script will be in the script file with executable permissions such as script a python script or a patch file
now the slip could perform any desired action such as running testing or building a project or even deploying the
code the script will run every time the containing strip or command is executed
so the next step is make the post receive file executable by running the following command
now every time a command are pushed to the repository the post receiver will run the Run script.sh script
now if you want the hook to run on a local repository you can use the post commit hook instead of the post receipt
book the post committee hooks run on the client side after the commit is completed with this we have come to the
end of today's session I hope at this point you are confident enough to attend your devops interview if in case you
have any doubts or queries then make sure to leave it in the comment below thank you and happy learning
I hope you have enjoyed listening to this video please be kind enough to like it